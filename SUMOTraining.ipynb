{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXNVZxRqRDmn"
   },
   "source": [
    "# Trying to install SUMO (TRACI) [ONLY FOR COLAB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1750191951055,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "s3YqaycTUawD",
    "outputId": "65ada99e-9606-456e-ffe5-a257256d24fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1750191952820,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "x0aIqWEJVVp8",
    "outputId": "104e5962-de73-4188-9ebf-cec1c15440da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Cityflow training\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/Cityflow\\ training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10382,
     "status": "ok",
     "timestamp": 1750191963370,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "_sUeRBYwgAYc",
    "outputId": "27f69f7c-460d-4cea-a017-b6840ff11842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: 'deb https://ppa.launchpadcontent.net/sumo/stable/ubuntu/ jammy main'\n",
      "Description:\n",
      "SUMO is a highly portable, microscopic traffic simulation package designed to handle large road networks. SUMO is open source, licensed under the EPLv2.\n",
      "More info: https://launchpad.net/~sumo/+archive/ubuntu/stable\n",
      "Adding repository.\n",
      "Adding deb entry to /etc/apt/sources.list.d/sumo-ubuntu-stable-jammy.list\n",
      "Adding disabled deb-src entry to /etc/apt/sources.list.d/sumo-ubuntu-stable-jammy.list\n",
      "Adding key to /etc/apt/trusted.gpg.d/sumo-ubuntu-stable.gpg with fingerprint 7604B28616B7E70EC0E6D840C32412BB1ADB414B\n",
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [79.8 kB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,798 kB]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,040 kB]\n",
      "Get:12 https://ppa.launchpadcontent.net/sumo/stable/ubuntu jammy InRelease [18.3 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,250 kB]\n",
      "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,296 kB]\n",
      "Get:17 https://ppa.launchpadcontent.net/sumo/stable/ubuntu jammy/main amd64 Packages [915 B]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,986 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,557 kB]\n",
      "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,747 kB]\n",
      "Fetched 23.2 MB in 4s (6,317 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    }
   ],
   "source": [
    "# installing SUMO\n",
    "!add-apt-repository ppa:sumo/stable -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3839,
     "status": "ok",
     "timestamp": 1750191967212,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "PWPrkH6-jRWB",
    "outputId": "b8807f31-d689-4fcf-edc7-cce61765fa06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/sumo/stable/ubuntu jammy InRelease\n",
      "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    }
   ],
   "source": [
    "!apt-get update -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31445,
     "status": "ok",
     "timestamp": 1750191998659,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "sy9w9IqqjULW",
    "outputId": "88582ded-7491-476f-be93-ef83ce37f586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  binfmt-support fastjar jarwrapper javascript-common libfox-1.6-0 libgdal30\n",
      "  libglu1-mesa libjs-openlayers libproj22 libspatialindex-c6\n",
      "  libspatialindex-dev libspatialindex6 libspatialite7 proj-bin python3-certifi\n",
      "  python3-numpy python3-pyproj python3-rtree\n",
      "Suggested packages:\n",
      "  apache2 | lighttpd | httpd python-numpy-doc python3-pytest\n",
      "The following NEW packages will be installed:\n",
      "  binfmt-support fastjar jarwrapper javascript-common libfox-1.6-0 libgdal30\n",
      "  libglu1-mesa libjs-openlayers libproj22 libspatialindex-c6\n",
      "  libspatialindex-dev libspatialindex6 libspatialite7 proj-bin python3-certifi\n",
      "  python3-numpy python3-pyproj python3-rtree sumo sumo-doc sumo-tools\n",
      "0 upgraded, 21 newly installed, 0 to remove and 48 not upgraded.\n",
      "Need to get 152 MB of archives.\n",
      "After this operation, 703 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 binfmt-support amd64 2.2.1-2 [55.8 kB]\n",
      "Get:2 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 libspatialite7 amd64 5.0.1-3~jammy0 [2,091 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fastjar amd64 2:0.98-7 [67.1 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 jarwrapper all 0.78 [10.8 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 javascript-common all 11+nmu1 [5,936 B]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfox-1.6-0 amd64 1.6.57-1build1 [890 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libproj22 amd64 8.2.1-1 [1,257 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspatialindex6 amd64 1.9.3-2 [247 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspatialindex-c6 amd64 1.9.3-2 [55.8 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-certifi all 2020.6.20-1 [150 kB]\n",
      "Get:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 libgdal30 amd64 3.4.3+dfsg-1~jammy0 [7,953 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-numpy amd64 1:1.21.5-1ubuntu22.04.1 [3,467 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspatialindex-dev amd64 1.9.3-2 [16.0 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-rtree all 0.9.7-1 [46.4 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-openlayers all 2.13.1+ds2-8 [709 kB]\n",
      "Get:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 proj-bin amd64 9.3.1-1~jammy0 [205 kB]\n",
      "Get:18 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-pyproj amd64 3.6.1-2~jammy0 [399 kB]\n",
      "Get:19 https://ppa.launchpadcontent.net/sumo/stable/ubuntu jammy/main amd64 sumo amd64 1.23.1-1~jammy [37.3 MB]\n",
      "Get:20 https://ppa.launchpadcontent.net/sumo/stable/ubuntu jammy/main amd64 sumo-doc all 1.23.1-1~jammy [71.7 MB]\n",
      "Get:21 https://ppa.launchpadcontent.net/sumo/stable/ubuntu jammy/main amd64 sumo-tools all 1.23.1-1~jammy [25.2 MB]\n",
      "Fetched 152 MB in 11s (14.2 MB/s)\n",
      "Selecting previously unselected package binfmt-support.\n",
      "(Reading database ... 126319 files and directories currently installed.)\n",
      "Preparing to unpack .../00-binfmt-support_2.2.1-2_amd64.deb ...\n",
      "Unpacking binfmt-support (2.2.1-2) ...\n",
      "Selecting previously unselected package fastjar.\n",
      "Preparing to unpack .../01-fastjar_2%3a0.98-7_amd64.deb ...\n",
      "Unpacking fastjar (2:0.98-7) ...\n",
      "Selecting previously unselected package jarwrapper.\n",
      "Preparing to unpack .../02-jarwrapper_0.78_all.deb ...\n",
      "Unpacking jarwrapper (0.78) ...\n",
      "Selecting previously unselected package javascript-common.\n",
      "Preparing to unpack .../03-javascript-common_11+nmu1_all.deb ...\n",
      "Unpacking javascript-common (11+nmu1) ...\n",
      "Selecting previously unselected package libglu1-mesa:amd64.\n",
      "Preparing to unpack .../04-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
      "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
      "Selecting previously unselected package libfox-1.6-0:amd64.\n",
      "Preparing to unpack .../05-libfox-1.6-0_1.6.57-1build1_amd64.deb ...\n",
      "Unpacking libfox-1.6-0:amd64 (1.6.57-1build1) ...\n",
      "Selecting previously unselected package libproj22:amd64.\n",
      "Preparing to unpack .../06-libproj22_8.2.1-1_amd64.deb ...\n",
      "Unpacking libproj22:amd64 (8.2.1-1) ...\n",
      "Selecting previously unselected package libspatialite7:amd64.\n",
      "Preparing to unpack .../07-libspatialite7_5.0.1-3~jammy0_amd64.deb ...\n",
      "Unpacking libspatialite7:amd64 (5.0.1-3~jammy0) ...\n",
      "Selecting previously unselected package libgdal30.\n",
      "Preparing to unpack .../08-libgdal30_3.4.3+dfsg-1~jammy0_amd64.deb ...\n",
      "Unpacking libgdal30 (3.4.3+dfsg-1~jammy0) ...\n",
      "Selecting previously unselected package libspatialindex6:amd64.\n",
      "Preparing to unpack .../09-libspatialindex6_1.9.3-2_amd64.deb ...\n",
      "Unpacking libspatialindex6:amd64 (1.9.3-2) ...\n",
      "Selecting previously unselected package libspatialindex-c6:amd64.\n",
      "Preparing to unpack .../10-libspatialindex-c6_1.9.3-2_amd64.deb ...\n",
      "Unpacking libspatialindex-c6:amd64 (1.9.3-2) ...\n",
      "Selecting previously unselected package proj-bin.\n",
      "Preparing to unpack .../11-proj-bin_9.3.1-1~jammy0_amd64.deb ...\n",
      "Unpacking proj-bin (9.3.1-1~jammy0) ...\n",
      "Selecting previously unselected package python3-certifi.\n",
      "Preparing to unpack .../12-python3-certifi_2020.6.20-1_all.deb ...\n",
      "Unpacking python3-certifi (2020.6.20-1) ...\n",
      "Selecting previously unselected package python3-numpy.\n",
      "Preparing to unpack .../13-python3-numpy_1%3a1.21.5-1ubuntu22.04.1_amd64.deb ...\n",
      "Unpacking python3-numpy (1:1.21.5-1ubuntu22.04.1) ...\n",
      "Selecting previously unselected package python3-pyproj.\n",
      "Preparing to unpack .../14-python3-pyproj_3.6.1-2~jammy0_amd64.deb ...\n",
      "Unpacking python3-pyproj (3.6.1-2~jammy0) ...\n",
      "Selecting previously unselected package libspatialindex-dev:amd64.\n",
      "Preparing to unpack .../15-libspatialindex-dev_1.9.3-2_amd64.deb ...\n",
      "Unpacking libspatialindex-dev:amd64 (1.9.3-2) ...\n",
      "Selecting previously unselected package python3-rtree.\n",
      "Preparing to unpack .../16-python3-rtree_0.9.7-1_all.deb ...\n",
      "Unpacking python3-rtree (0.9.7-1) ...\n",
      "Selecting previously unselected package sumo.\n",
      "Preparing to unpack .../17-sumo_1.23.1-1~jammy_amd64.deb ...\n",
      "Unpacking sumo (1.23.1-1~jammy) ...\n",
      "Selecting previously unselected package sumo-doc.\n",
      "Preparing to unpack .../18-sumo-doc_1.23.1-1~jammy_all.deb ...\n",
      "Unpacking sumo-doc (1.23.1-1~jammy) ...\n",
      "Selecting previously unselected package libjs-openlayers.\n",
      "Preparing to unpack .../19-libjs-openlayers_2.13.1+ds2-8_all.deb ...\n",
      "Unpacking libjs-openlayers (2.13.1+ds2-8) ...\n",
      "Selecting previously unselected package sumo-tools.\n",
      "Preparing to unpack .../20-sumo-tools_1.23.1-1~jammy_all.deb ...\n",
      "Unpacking sumo-tools (1.23.1-1~jammy) ...\n",
      "Setting up libspatialite7:amd64 (5.0.1-3~jammy0) ...\n",
      "Setting up fastjar (2:0.98-7) ...\n",
      "Setting up javascript-common (11+nmu1) ...\n",
      "Setting up proj-bin (9.3.1-1~jammy0) ...\n",
      "Setting up libspatialindex6:amd64 (1.9.3-2) ...\n",
      "Setting up libproj22:amd64 (8.2.1-1) ...\n",
      "Setting up sumo-doc (1.23.1-1~jammy) ...\n",
      "Setting up libgdal30 (3.4.3+dfsg-1~jammy0) ...\n",
      "Setting up python3-certifi (2020.6.20-1) ...\n",
      "Setting up binfmt-support (2.2.1-2) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of restart.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service → /lib/systemd/system/binfmt-support.service.\n",
      "Setting up python3-numpy (1:1.21.5-1ubuntu22.04.1) ...\n",
      "Setting up libjs-openlayers (2.13.1+ds2-8) ...\n",
      "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
      "Setting up libfox-1.6-0:amd64 (1.6.57-1build1) ...\n",
      "Setting up sumo (1.23.1-1~jammy) ...\n",
      "Setting up libspatialindex-c6:amd64 (1.9.3-2) ...\n",
      "Setting up jarwrapper (0.78) ...\n",
      "Setting up libspatialindex-dev:amd64 (1.9.3-2) ...\n",
      "Setting up python3-pyproj (3.6.1-2~jammy0) ...\n",
      "Setting up python3-rtree (0.9.7-1) ...\n",
      "Setting up sumo-tools (1.23.1-1~jammy) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!apt-get install sumo sumo-tools sumo-doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1750191998777,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "wzACfk22gXCd",
    "outputId": "d11e9822-2218-427d-c011-04072dcba467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/.\n",
      "/etc\n",
      "/etc/profile.d\n",
      "/etc/profile.d/sumo.csh\n",
      "/etc/profile.d/sumo.sh\n",
      "/usr\n",
      "/usr/bin\n",
      "/usr/bin/activitygen\n",
      "/usr/bin/dfrouter\n",
      "/usr/bin/duarouter\n",
      "/usr/bin/emissionsDrivingCycle\n",
      "/usr/bin/emissionsMap\n",
      "/usr/bin/jtrrouter\n",
      "/usr/bin/marouter\n",
      "/usr/bin/netconvert\n",
      "/usr/bin/netedit\n",
      "/usr/bin/netgenerate\n",
      "/usr/bin/od2trips\n",
      "/usr/bin/polyconvert\n",
      "/usr/bin/sumo\n",
      "/usr/bin/sumo-gui\n",
      "/usr/include\n",
      "/usr/include/libsumo\n",
      "/usr/include/libsumo/BusStop.h\n",
      "/usr/include/libsumo/Calibrator.h\n",
      "/usr/include/libsumo/ChargingStation.h\n",
      "/usr/include/libsumo/Edge.h\n",
      "/usr/include/libsumo/GUI.h\n",
      "/usr/include/libsumo/InductionLoop.h\n",
      "/usr/include/libsumo/Junction.h\n",
      "/usr/include/libsumo/Lane.h\n",
      "/usr/include/libsumo/LaneArea.h\n",
      "/usr/include/libsumo/MeanData.h\n",
      "/usr/include/libsumo/MultiEntryExit.h\n",
      "/usr/include/libsumo/OverheadWire.h\n",
      "/usr/include/libsumo/POI.h\n",
      "/usr/include/libsumo/ParkingArea.h\n",
      "/usr/include/libsumo/Person.h\n",
      "/usr/include/libsumo/Polygon.h\n",
      "/usr/include/libsumo/Rerouter.h\n",
      "/usr/include/libsumo/Route.h\n",
      "/usr/include/libsumo/RouteProbe.h\n",
      "/usr/include/libsumo/Simulation.h\n",
      "/usr/include/libsumo/TraCIConstants.h\n",
      "/usr/include/libsumo/TraCIDefs.h\n",
      "/usr/include/libsumo/TrafficLight.h\n",
      "/usr/include/libsumo/VariableSpeedSign.h\n",
      "/usr/include/libsumo/Vehicle.h\n",
      "/usr/include/libsumo/VehicleType.h\n",
      "/usr/include/libsumo/libsumo.h\n",
      "/usr/include/libsumo/libtraci.h\n",
      "/usr/lib\n",
      "/usr/lib/x86_64-linux-gnu\n",
      "/usr/lib/x86_64-linux-gnu/liblibsumojni.so\n",
      "/usr/lib/x86_64-linux-gnu/liblibtracijni.so\n",
      "/usr/lib/x86_64-linux-gnu/libsumocpp.so\n",
      "/usr/lib/x86_64-linux-gnu/libsumocs.so\n",
      "/usr/lib/x86_64-linux-gnu/libtracicpp.so\n",
      "/usr/lib/x86_64-linux-gnu/libtracics.so\n",
      "/usr/share\n",
      "/usr/share/applications\n",
      "/usr/share/applications/netedit.desktop\n",
      "/usr/share/applications/osmWebWizard.desktop\n",
      "/usr/share/applications/sumo.desktop\n",
      "/usr/share/doc\n",
      "/usr/share/doc/sumo\n",
      "/usr/share/doc/sumo/changelog.Debian.gz\n",
      "/usr/share/doc/sumo/copyright\n",
      "/usr/share/man\n",
      "/usr/share/man/man1\n",
      "/usr/share/man/man1/TraCITestClient.1.gz\n",
      "/usr/share/man/man1/activitygen.1.gz\n",
      "/usr/share/man/man1/dfrouter.1.gz\n",
      "/usr/share/man/man1/duarouter.1.gz\n",
      "/usr/share/man/man1/emissionsDrivingCycle.1.gz\n",
      "/usr/share/man/man1/emissionsMap.1.gz\n",
      "/usr/share/man/man1/jtrrouter.1.gz\n",
      "/usr/share/man/man1/marouter.1.gz\n",
      "/usr/share/man/man1/netconvert.1.gz\n",
      "/usr/share/man/man1/netedit.1.gz\n",
      "/usr/share/man/man1/netgenerate.1.gz\n",
      "/usr/share/man/man1/od2trips.1.gz\n",
      "/usr/share/man/man1/polyconvert.1.gz\n",
      "/usr/share/man/man1/sumo-gui.1.gz\n",
      "/usr/share/man/man1/sumo.1.gz\n",
      "/usr/share/mime\n",
      "/usr/share/mime/application\n",
      "/usr/share/mime/application/sumo.xml\n",
      "/usr/share/pixmaps\n",
      "/usr/share/pixmaps/netedit.png\n",
      "/usr/share/pixmaps/osmWebWizard.png\n",
      "/usr/share/pixmaps/sumo.png\n",
      "/usr/share/sumo\n",
      "/usr/share/sumo/data\n",
      "/usr/share/sumo/data/3D\n",
      "/usr/share/sumo/data/3D/car-microcargo-citrus.mtl\n",
      "/usr/share/sumo/data/3D/car-microcargo-citrus.obj\n",
      "/usr/share/sumo/data/3D/car-minibus-citrus.mtl\n",
      "/usr/share/sumo/data/3D/car-minibus-citrus.obj\n",
      "/usr/share/sumo/data/3D/car-normal-citrus.mtl\n",
      "/usr/share/sumo/data/3D/car-normal-citrus.obj\n",
      "/usr/share/sumo/data/3D/humanResting.mtl\n",
      "/usr/share/sumo/data/3D/humanResting.obj\n",
      "/usr/share/sumo/data/3D/poleBase.obj\n",
      "/usr/share/sumo/data/3D/tl.obj.mtl\n",
      "/usr/share/sumo/data/3D/tlg.obj\n",
      "/usr/share/sumo/data/3D/tlr.obj\n",
      "/usr/share/sumo/data/3D/tlu.obj\n",
      "/usr/share/sumo/data/3D/tly.obj\n",
      "/usr/share/sumo/data/3D/tram.mtl\n",
      "/usr/share/sumo/data/3D/tram.obj\n",
      "/usr/share/sumo/data/emissions\n",
      "/usr/share/sumo/data/emissions/MMPEVEM\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/BMW_i3.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/Citroen_e-C4.rou.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/Hyundai_Ioniq_5.rou.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/Opel_Corsa_Electric.rou.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/Opel_Mokka-e.rou.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/Peugeot_e-2008.rou.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/Peugeot_e-208.rou.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/SUV.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/VW_ID3.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/VW_ID4.xml\n",
      "/usr/share/sumo/data/emissions/MMPEVEM/VW_eUp.xml\n",
      "/usr/share/sumo/data/emissions/PHEMlight\n",
      "/usr/share/sumo/data/emissions/PHEMlight/PC_D_EU4.PHEMLight.veh\n",
      "/usr/share/sumo/data/emissions/PHEMlight/PC_D_EU4.csv\n",
      "/usr/share/sumo/data/emissions/PHEMlight/PC_D_EU4_FC.csv\n",
      "/usr/share/sumo/data/emissions/PHEMlight/PC_G_EU4.PHEMLight.veh\n",
      "/usr/share/sumo/data/emissions/PHEMlight/PC_G_EU4.csv\n",
      "/usr/share/sumo/data/emissions/PHEMlight/PC_G_EU4_FC.csv\n",
      "/usr/share/sumo/data/emissions/PHEMlight5\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/Deterioration.det\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/Mileage.vma\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/NOxCor.tno\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/PC_EU4_D_MW.PHEMLight.veh\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/PC_EU4_D_MW.csv\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/PC_EU4_D_MW_FC.csv\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/PC_EU4_G.PHEMLight.veh\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/PC_EU4_G.csv\n",
      "/usr/share/sumo/data/emissions/PHEMlight5/PC_EU4_G_FC.csv\n",
      "/usr/share/sumo/data/font\n",
      "/usr/share/sumo/data/font/Roboto-Medium.ttf\n",
      "/usr/share/sumo/data/lang\n",
      "/usr/share/sumo/data/lang/visumEN.txt\n",
      "/usr/share/sumo/data/lang/visumFR.txt\n",
      "/usr/share/sumo/data/lang/visumIT.txt\n",
      "/usr/share/sumo/data/locale\n",
      "/usr/share/sumo/data/locale/de\n",
      "/usr/share/sumo/data/locale/de/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/de/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/locale/es\n",
      "/usr/share/sumo/data/locale/es/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/es/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/locale/fr\n",
      "/usr/share/sumo/data/locale/fr/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/fr/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/locale/hu\n",
      "/usr/share/sumo/data/locale/hu/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/hu/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/locale/it\n",
      "/usr/share/sumo/data/locale/it/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/it/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/locale/ja\n",
      "/usr/share/sumo/data/locale/ja/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/ja/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/locale/tr\n",
      "/usr/share/sumo/data/locale/tr/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/tr/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/locale/zh\n",
      "/usr/share/sumo/data/locale/zh/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/zh/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/locale/zh-Hant\n",
      "/usr/share/sumo/data/locale/zh-Hant/LC_MESSAGES\n",
      "/usr/share/sumo/data/locale/zh-Hant/LC_MESSAGES/sumo.mo\n",
      "/usr/share/sumo/data/logo\n",
      "/usr/share/sumo/data/logo/netedit-application-icon.ico\n",
      "/usr/share/sumo/data/logo/sumo-128x138.png\n",
      "/usr/share/sumo/data/logo/sumo-145x50.png\n",
      "/usr/share/sumo/data/logo/sumo-application-icon.ico\n",
      "/usr/share/sumo/data/po\n",
      "/usr/share/sumo/data/po/de_gui.po\n",
      "/usr/share/sumo/data/po/de_py.po\n",
      "/usr/share/sumo/data/po/de_sumo.po\n",
      "/usr/share/sumo/data/po/es_gui.po\n",
      "/usr/share/sumo/data/po/es_py.po\n",
      "/usr/share/sumo/data/po/es_sumo.po\n",
      "/usr/share/sumo/data/po/fr_gui.po\n",
      "/usr/share/sumo/data/po/fr_py.po\n",
      "/usr/share/sumo/data/po/fr_sumo.po\n",
      "/usr/share/sumo/data/po/hu_gui.po\n",
      "/usr/share/sumo/data/po/hu_py.po\n",
      "/usr/share/sumo/data/po/hu_sumo.po\n",
      "/usr/share/sumo/data/po/it_gui.po\n",
      "/usr/share/sumo/data/po/it_py.po\n",
      "/usr/share/sumo/data/po/it_sumo.po\n",
      "/usr/share/sumo/data/po/ja_gui.po\n",
      "/usr/share/sumo/data/po/ja_py.po\n",
      "/usr/share/sumo/data/po/ja_sumo.po\n",
      "/usr/share/sumo/data/po/tr_gui.po\n",
      "/usr/share/sumo/data/po/tr_py.po\n",
      "/usr/share/sumo/data/po/tr_sumo.po\n",
      "/usr/share/sumo/data/po/zh-Hant_gui.po\n",
      "/usr/share/sumo/data/po/zh-Hant_py.po\n",
      "/usr/share/sumo/data/po/zh-Hant_sumo.po\n",
      "/usr/share/sumo/data/po/zh_gui.po\n",
      "/usr/share/sumo/data/po/zh_py.po\n",
      "/usr/share/sumo/data/po/zh_sumo.po\n",
      "/usr/share/sumo/data/typemap\n",
      "/usr/share/sumo/data/typemap/navteqPolyconvert.typ.xml\n",
      "/usr/share/sumo/data/typemap/opendriveNetconvert.typ.xml\n",
      "/usr/share/sumo/data/typemap/opendriveNetconvertBicycle.typ.xml\n",
      "/usr/share/sumo/data/typemap/opendriveNetconvertPedestrians.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvert.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertAerialway.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertAirport.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertBicycle.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertBidiRail.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertExtraRail.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertPedestrians.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertPedestriansNES.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertRailUsage.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertShips.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmNetconvertUrbanDe.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmPolyconvert.typ.xml\n",
      "/usr/share/sumo/data/typemap/osmPolyconvertRail.typ.xml\n",
      "/usr/share/sumo/data/typemap/visumPolyconvert.typ.xml\n",
      "/usr/share/sumo/data/xsd\n",
      "/usr/share/sumo/data/xsd/activitygenConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/additional_file.xsd\n",
      "/usr/share/sumo/data/xsd/amitran\n",
      "/usr/share/sumo/data/xsd/amitran/linkdata.xsd\n",
      "/usr/share/sumo/data/xsd/amitran/network.xsd\n",
      "/usr/share/sumo/data/xsd/amitran/od.xsd\n",
      "/usr/share/sumo/data/xsd/amitran/routes.xsd\n",
      "/usr/share/sumo/data/xsd/amitran/trajectories.xsd\n",
      "/usr/share/sumo/data/xsd/baseTypes.xsd\n",
      "/usr/share/sumo/data/xsd/battery_file.xsd\n",
      "/usr/share/sumo/data/xsd/calibratorstats_file.xsd\n",
      "/usr/share/sumo/data/xsd/collision_file.xsd\n",
      "/usr/share/sumo/data/xsd/connections_file.xsd\n",
      "/usr/share/sumo/data/xsd/datamode_file.xsd\n",
      "/usr/share/sumo/data/xsd/det_e1_file.xsd\n",
      "/usr/share/sumo/data/xsd/det_e1meso_file.xsd\n",
      "/usr/share/sumo/data/xsd/det_e2_file.xsd\n",
      "/usr/share/sumo/data/xsd/det_e3_file.xsd\n",
      "/usr/share/sumo/data/xsd/detectors_file.xsd\n",
      "/usr/share/sumo/data/xsd/dfrouterConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/duarouterConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/edgediff_file.xsd\n",
      "/usr/share/sumo/data/xsd/edges_file.xsd\n",
      "/usr/share/sumo/data/xsd/emission_file.xsd\n",
      "/usr/share/sumo/data/xsd/fcd_file.xsd\n",
      "/usr/share/sumo/data/xsd/full_file.xsd\n",
      "/usr/share/sumo/data/xsd/genericparameter_file.xsd\n",
      "/usr/share/sumo/data/xsd/instant_e1_file.xsd\n",
      "/usr/share/sumo/data/xsd/jtrrouterConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/marouterConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/meandataTypes.xsd\n",
      "/usr/share/sumo/data/xsd/meandata_file.xsd\n",
      "/usr/share/sumo/data/xsd/net_file.xsd\n",
      "/usr/share/sumo/data/xsd/netconvertConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/neteditConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/netgenerateConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/netstate_file.xsd\n",
      "/usr/share/sumo/data/xsd/nodes_file.xsd\n",
      "/usr/share/sumo/data/xsd/od2tripsConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/person_summary_file.xsd\n",
      "/usr/share/sumo/data/xsd/polyconvertConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/ptlines_file.xsd\n",
      "/usr/share/sumo/data/xsd/queue_file.xsd\n",
      "/usr/share/sumo/data/xsd/routeTypes.xsd\n",
      "/usr/share/sumo/data/xsd/routes_file.xsd\n",
      "/usr/share/sumo/data/xsd/state_file.xsd\n",
      "/usr/share/sumo/data/xsd/statistic_file.xsd\n",
      "/usr/share/sumo/data/xsd/stopinfo_file.xsd\n",
      "/usr/share/sumo/data/xsd/summary_file.xsd\n",
      "/usr/share/sumo/data/xsd/sumoConfiguration.xsd\n",
      "/usr/share/sumo/data/xsd/tazTypes.xsd\n",
      "/usr/share/sumo/data/xsd/taz_file.xsd\n",
      "/usr/share/sumo/data/xsd/tllogic_file.xsd\n",
      "/usr/share/sumo/data/xsd/tlsstates_file.xsd\n",
      "/usr/share/sumo/data/xsd/tlsswitches_file.xsd\n",
      "/usr/share/sumo/data/xsd/tripinfo_file.xsd\n",
      "/usr/share/sumo/data/xsd/turns_file.xsd\n",
      "/usr/share/sumo/data/xsd/types_file.xsd\n",
      "/usr/share/sumo/data/xsd/viewsettings_file.xsd\n",
      "/usr/share/sumo/data/xsd/vtypeprobe_file.xsd\n",
      "/usr/share/sumo/bin\n"
     ]
    }
   ],
   "source": [
    "!dpkg -L sumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6951,
     "status": "ok",
     "timestamp": 1750192005730,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "6FVDbNFtQ-hn",
    "outputId": "646f5b63-563a-4e83-f8fb-c0300651ab45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting traci\n",
      "  Downloading traci-1.23.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting sumolib>=1.23.1 (from traci)\n",
      "  Downloading sumolib-1.23.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading traci-1.23.1-py3-none-any.whl (277 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/277.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.2/277.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sumolib-1.23.1-py3-none-any.whl (276 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/277.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.0/277.0 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sumolib, traci\n",
      "Successfully installed sumolib-1.23.1 traci-1.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install traci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750192005747,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "cq_c3Ablj4-D"
   },
   "outputs": [],
   "source": [
    "# adding sumo to path\n",
    "import os\n",
    "os.environ['SUMO_HOME'] = \"/usr/share/sumo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1750192005800,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "_4-Mj1UikCFc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8blq6fXauRJ"
   },
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5367,
     "status": "ok",
     "timestamp": 1750192122119,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "2k4eiWqjVZhC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import traci\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import math\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUPlWt2Iaw-s"
   },
   "source": [
    "# Creating the Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1750192122169,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "mSeqoDc9a1X9"
   },
   "outputs": [],
   "source": [
    "class SUMOEnvironment:\n",
    "    '''\n",
    "        This class is the environment implemented using SUMO and TRACI for a single intersection.\n",
    "    '''\n",
    "    def __init__(self, sumoCfgPath, sumoMode='sumo', maxTime=3600.0, enableLogging=True, logPath='logs/RLLog.txt'):\n",
    "        self.sumoMode = sumoMode\n",
    "        self.sumoCfgPath = sumoCfgPath # these two are made into class variables because they will also be used in the reset function\n",
    "        self.maxTime = maxTime\n",
    "        self.currTime = 0.0\n",
    "        self.directions = ( # movement directions for calculating the reward function\n",
    "            ('E1_2', '-E3_2'), # left\n",
    "            ('E1_1', 'E2_1'), # straight\n",
    "            ('E1_0', 'E4_0'), # right\n",
    "\n",
    "            ('-E4_2', '-E1_2'), # left\n",
    "            ('-E4_1', '-E3_1'), # straight\n",
    "            ('-E4_0', 'E2_0'), # right\n",
    "\n",
    "            ('-E2_2', 'E4_2'), # left\n",
    "            ('-E2_1', '-E1_1'), # straight\n",
    "            ('-E2_0', '-E3_0'), # right\n",
    "\n",
    "            ('E3_2', 'E2_2'), # left\n",
    "            ('E3_1', 'E4_1'), # straight\n",
    "            ('E3_0', '-E1_0') # right\n",
    "        )\n",
    "        self.incoming = [t[0] for t in self.directions]\n",
    "        self.capacity = 40\n",
    "\n",
    "        # starting the simulation\n",
    "        if enableLogging:\n",
    "            traci.start([sumoMode, '-c', sumoCfgPath, \"--log\", logPath, \"--duration-log.statistics\", \"true\"])\n",
    "        else:\n",
    "            traci.start([sumoMode, '-c', sumoCfgPath])\n",
    "\n",
    "    def _getState(self, intersectionId='Inter'):\n",
    "        '''\n",
    "            This function returns the state at the current time step.\n",
    "        '''\n",
    "        stArray = []\n",
    "        for l in self.incoming:\n",
    "            # getting the number of waiting vehicles in the lane\n",
    "            vc = traci.lane.getLastStepHaltingNumber(l)\n",
    "            stArray.append(vc)\n",
    "\n",
    "        # at the end, appending the current state of the intersection\n",
    "        cs = traci.trafficlight.getPhase(intersectionId)\n",
    "        stArray.append(cs)\n",
    "        return stArray\n",
    "\n",
    "    def _getReward(self):\n",
    "        '''\n",
    "            This function returns the reward as of the current state of the intersection.\n",
    "        '''\n",
    "        r = 0\n",
    "\n",
    "        # looping through the directions and calculating individual rewards\n",
    "        for d in self.directions:\n",
    "            # waiting in the incoming lane\n",
    "            vIn = traci.lane.getLastStepHaltingNumber(d[0])\n",
    "            vOut = traci.lane.getLastStepHaltingNumber(d[1])\n",
    "            r_i = -1 * vIn * (1 - (vOut/self.capacity))\n",
    "            r += r_i\n",
    "        return r\n",
    "\n",
    "    def _step(self, t=10):\n",
    "        '''\n",
    "            This function moves the simulation t timesteps ahead. And if the total number of steps reaches the max allowed steps, it stop and returns if the iteration is done.\n",
    "        '''\n",
    "        finished = False\n",
    "        for i in range(t):\n",
    "            if self.currTime==self.maxTime:\n",
    "                finished = True\n",
    "                break\n",
    "            # if not, the continue\n",
    "            self.currTime  = self.currTime + 1.0\n",
    "            traci.simulationStep()\n",
    "        return finished\n",
    "\n",
    "    def takeAction(self, action, intersectionId='Inter', t=10):\n",
    "        '''\n",
    "            This function performs the given action, steps the environment ahead for next t seconds/steps, and then returns the next state, reward and whether the simulation has finished or not.\n",
    "        '''\n",
    "        # take action: set the tl phase to the action value\n",
    "        traci.trafficlight.setPhase(intersectionId, action)\n",
    "        # simulate next t time steps and get the next state\n",
    "        finished = self._step(t)\n",
    "        # get the next state\n",
    "        next_state = self._getState()\n",
    "        # get the reward\n",
    "        reward = self._getReward()\n",
    "\n",
    "        return next_state, reward, finished\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "            This function resets the environment to the start and returns the starting state.\n",
    "        '''\n",
    "        # reseting the sumo engine\n",
    "        traci.load([\"-c\", self.sumoCfgPath])\n",
    "        self.currTime = 0.0\n",
    "        return self._getState()\n",
    "\n",
    "    def close(self):\n",
    "        '''\n",
    "            This function closes the connection of traci with the sumo environment.\n",
    "            NOTE: After calling this function, you will need to reinitialize the object, as now the connection to SUMO has been closed for this. NEED TO FIND A BETTER WAY TO DO THIS.\n",
    "        '''\n",
    "        traci.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ihpGS1QbCmn"
   },
   "source": [
    "# Defining the Hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1750192122199,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "J28OyBLybE8V"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "PARAM_learning_rate = 0.001\n",
    "PARAM_gamma = 0.8\n",
    "PARAM_epsilon = 1.0\n",
    "PARAM_epsilon_min = 0.01\n",
    "PARAM_epsilon_decay = 35000\n",
    "PARAM_batch_size = 64\n",
    "PARAM_target_update_freq = 10\n",
    "PARAM_memory_size = 10000\n",
    "PARAM_episodes = 500\n",
    "PARAM_TAU = 0.005\n",
    "PARAM_device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSob1rZ7by5U"
   },
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1750192122235,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "wN1dyloibz3Y"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''\n",
    "        This class defines the neural network used for the model.\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        # creating the layers\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEimAY_Cb4sv"
   },
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1750192122245,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "iEeRN8ddb18V"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    '''\n",
    "        This class defines the replay memory\n",
    "    '''\n",
    "    def __init__(self, bufferSize):\n",
    "        self.bufferSize = bufferSize\n",
    "        self.memory = deque(maxlen=self.bufferSize)\n",
    "\n",
    "    def insert(self, state, action, reward, nextState, done):\n",
    "        self.memory.append((state, action, reward, nextState, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        sampleBatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, nextStates, done = zip(*sampleBatch)\n",
    "        return np.array(states), np.array(actions, dtype=int), np.array(rewards), np.array(nextStates), np.array(done)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgzOcK4ub9xB"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1750192122273,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "D2qh-vW6b2_h"
   },
   "outputs": [],
   "source": [
    "def selectAction(state, epsilon):\n",
    "    '''\n",
    "        This function selects and returns an action using the epsilon greedy method.\n",
    "    '''\n",
    "    # select a random number between 0 and 1\n",
    "    rNum = random.random()\n",
    "    # print('Rnum: ', rNum)\n",
    "    if rNum<epsilon:\n",
    "        # explore, select a random action\n",
    "        return random.choice([0,1,2,3])\n",
    "    else: # exploit, get the best action\n",
    "        state = torch.tensor(state, device=PARAM_device, dtype=torch.float32).unsqueeze(0)\n",
    "        qVals = policyNet(state)\n",
    "        return torch.argmax(qVals).item() # the action that has the highest Q value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-_1hYzWc6H8"
   },
   "source": [
    "# Starting the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1750191421657,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "tunyTfS5gxYv",
    "outputId": "64a7a2d3-3ad9-4f18-d1ad-90cb01d84696"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\SUMO\\\\tools',\n",
       " 'c:\\\\Users\\\\danie\\\\anaconda3\\\\envs\\\\sumo\\\\python39.zip',\n",
       " 'c:\\\\Users\\\\danie\\\\anaconda3\\\\envs\\\\sumo\\\\DLLs',\n",
       " 'c:\\\\Users\\\\danie\\\\anaconda3\\\\envs\\\\sumo\\\\lib',\n",
       " 'c:\\\\Users\\\\danie\\\\anaconda3\\\\envs\\\\sumo',\n",
       " '',\n",
       " 'c:\\\\Users\\\\danie\\\\anaconda3\\\\envs\\\\sumo\\\\lib\\\\site-packages',\n",
       " 'c:\\\\Users\\\\danie\\\\anaconda3\\\\envs\\\\sumo\\\\lib\\\\site-packages\\\\win32',\n",
       " 'c:\\\\Users\\\\danie\\\\anaconda3\\\\envs\\\\sumo\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'c:\\\\Users\\\\danie\\\\anaconda3\\\\envs\\\\sumo\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'D:\\\\Traffic Intersection Throguhput Optimization\\\\Cityflow-expt\\\\sumo-win64-1.22.0\\\\sumo-1.22.0\\\\tools']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2939,
     "status": "ok",
     "timestamp": 1750192140334,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "XM2eSIZGesIP",
    "outputId": "9ea49171-495f-4ed4-969f-c95e8d8e3733"
   },
   "outputs": [],
   "source": [
    "# declaring the environment\n",
    "env = SUMOEnvironment(\"data/SingleIntersection.sumocfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1750192144654,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "Wwg6v8GMc76w"
   },
   "outputs": [],
   "source": [
    "# defining the input and output dimensions\n",
    "input_dim, output_dim = 13, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1750192145363,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "9SCUOi6Udcdg"
   },
   "outputs": [],
   "source": [
    "# Declaring the networks\n",
    "policyNet = DQN(input_dim, output_dim)\n",
    "targetNet = DQN(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1750192146470,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "aTy2Z9dBdd7t",
    "outputId": "1f9bbc21-ffae-4a92-dc26-e7bc97508eca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the state dict from policy to target so that they have the same starting weights\n",
    "targetNet.load_state_dict(policyNet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 8682,
     "status": "ok",
     "timestamp": 1750192302244,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "4eRl6TLBlMbD"
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.Adam(policyNet.parameters(), lr=PARAM_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1750192308795,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "EcVDG0HKlwVn",
    "outputId": "bea9f487-3cfc-4b94-e623-9b3412533770"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=13, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving the models to gpu\n",
    "policyNet.to(PARAM_device)\n",
    "targetNet.to(PARAM_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1750192324536,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "Yl0JjRktlz-J"
   },
   "outputs": [],
   "source": [
    "# copying the epsilon value to use\n",
    "INUSE_epsilon = copy.deepcopy(PARAM_epsilon)\n",
    "# initializing the memory\n",
    "memory = ReplayMemory(PARAM_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4326180,
     "status": "ok",
     "timestamp": 1750196872790,
     "user": {
      "displayName": "Aman Savaria",
      "userId": "07814182865104762928"
     },
     "user_tz": 300
    },
    "id": "RCaEHNHpl34z",
    "outputId": "95ed7f34-9527-4518-b667-4f42b2803322"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  0\n",
      "Epsilon Used for the episode:  1.0\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -26271.0 || average_reward: -72.77285318559557\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  1\n",
      "Epsilon Used for the episode:  0.9898413368799944\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -27952.0 || average_reward: -77.42936288088643\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  2\n",
      "Epsilon Used for the episode:  0.9797869146048228\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -26361.0 || average_reward: -73.02216066481995\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  3\n",
      "Epsilon Used for the episode:  0.9698356635304186\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -28128.0 || average_reward: -77.91689750692521\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  4\n",
      "Epsilon Used for the episode:  0.9599865249886281\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -26990.0 || average_reward: -74.7645429362881\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  5\n",
      "Epsilon Used for the episode:  0.9502384511745833\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -25795.0 || average_reward: -71.45429362880887\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  6\n",
      "Epsilon Used for the episode:  0.9405904050352313\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -26117.0 || average_reward: -72.34626038781164\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  7\n",
      "Epsilon Used for the episode:  0.9310413601590065\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -25831.0 || average_reward: -71.55401662049861\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  8\n",
      "Epsilon Used for the episode:  0.9215903006666356\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -25365.0 || average_reward: -70.26315789473684\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  9\n",
      "Epsilon Used for the episode:  0.9122362211030629\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -26320.0 || average_reward: -72.90858725761773\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  10\n",
      "Epsilon Used for the episode:  0.9029781263304841\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -26047.0 || average_reward: -72.15235457063712\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  11\n",
      "Epsilon Used for the episode:  0.893815031422479\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -25609.0 || average_reward: -70.93905817174515\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  12\n",
      "Epsilon Used for the episode:  0.8847459615592281\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -24214.0 || average_reward: -67.07479224376732\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  13\n",
      "Epsilon Used for the episode:  0.8757699519238084\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -25802.0 || average_reward: -71.47368421052632\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  14\n",
      "Epsilon Used for the episode:  0.8668860475995483\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -24813.0 || average_reward: -68.73407202216066\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  15\n",
      "Epsilon Used for the episode:  0.8580933034684403\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -24256.0 || average_reward: -67.19113573407202\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  16\n",
      "Epsilon Used for the episode:  0.8493907841105932\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -24164.0 || average_reward: -66.93628808864266\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  17\n",
      "Epsilon Used for the episode:  0.8407775637047176\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -24467.0 || average_reward: -67.77562326869806\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  18\n",
      "Epsilon Used for the episode:  0.8322527259296313\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -23964.0 || average_reward: -66.38227146814404\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  19\n",
      "Epsilon Used for the episode:  0.8238153638667773\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -24066.0 || average_reward: -66.66481994459834\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  20\n",
      "Epsilon Used for the episode:  0.8154645799037396\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -24074.0 || average_reward: -66.68698060941828\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  21\n",
      "Epsilon Used for the episode:  0.8071994856387507\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -23768.0 || average_reward: -65.8393351800554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  22\n",
      "Epsilon Used for the episode:  0.7990192017861791\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -25297.0 || average_reward: -70.07479224376732\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  23\n",
      "Epsilon Used for the episode:  0.7909228580829857\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -22887.0 || average_reward: -63.398891966759\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  24\n",
      "Epsilon Used for the episode:  0.7829095931961402\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -23491.0 || average_reward: -65.07202216066482\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  25\n",
      "Epsilon Used for the episode:  0.7749785546309884\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -23104.0 || average_reward: -64.0\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  26\n",
      "Epsilon Used for the episode:  0.767128898640559\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -23197.0 || average_reward: -64.25761772853186\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  27\n",
      "Epsilon Used for the episode:  0.7593597901358011\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -24160.0 || average_reward: -66.92520775623268\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  28\n",
      "Epsilon Used for the episode:  0.7516704025967428\n",
      "EPISODE COMPLETE\n",
      "min_reward: -127.0 || max_reward: 0.0 || total_reward: -22366.0 || average_reward: -61.95567867036011\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  29\n",
      "Epsilon Used for the episode:  0.7440599179845617\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -23681.0 || average_reward: -65.5983379501385\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  30\n",
      "Epsilon Used for the episode:  0.7365275266545576\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -23119.0 || average_reward: -64.0415512465374\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  31\n",
      "Epsilon Used for the episode:  0.7290724272700174\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -24633.0 || average_reward: -68.2354570637119\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  32\n",
      "Epsilon Used for the episode:  0.7216938267169661\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -22730.0 || average_reward: -62.96398891966759\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  33\n",
      "Epsilon Used for the episode:  0.714390940019789\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -23352.0 || average_reward: -64.68698060941828\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  34\n",
      "Epsilon Used for the episode:  0.7071629902577232\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -22009.0 || average_reward: -60.96675900277008\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  35\n",
      "Epsilon Used for the episode:  0.700009208482204\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -23971.0 || average_reward: -66.4016620498615\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  36\n",
      "Epsilon Used for the episode:  0.6929288336350601\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -22470.0 || average_reward: -62.24376731301939\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  37\n",
      "Epsilon Used for the episode:  0.685921112467548\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -22377.0 || average_reward: -61.986149584487535\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  38\n",
      "Epsilon Used for the episode:  0.6789852994602175\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -22064.0 || average_reward: -61.119113573407205\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  39\n",
      "Epsilon Used for the episode:  0.6721206567435988\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -22794.0 || average_reward: -63.14127423822715\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  40\n",
      "Epsilon Used for the episode:  0.6653264540197047\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -21670.0 || average_reward: -60.02770083102493\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  41\n",
      "Epsilon Used for the episode:  0.6586019684843369\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -22488.0 || average_reward: -62.29362880886426\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  42\n",
      "Epsilon Used for the episode:  0.6519464847501907\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -23098.0 || average_reward: -63.983379501385045\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  43\n",
      "Epsilon Used for the episode:  0.6453592947707472\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -21974.0 || average_reward: -60.86980609418283\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  44\n",
      "Epsilon Used for the episode:  0.6388396977649489\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -23253.0 || average_reward: -64.41274238227147\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  45\n",
      "Epsilon Used for the episode:  0.6323870001426454\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -24124.0 || average_reward: -66.82548476454294\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  46\n",
      "Epsilon Used for the episode:  0.626000515430807\n",
      "EPISODE COMPLETE\n",
      "min_reward: -168.0 || max_reward: 0.0 || total_reward: -23081.0 || average_reward: -63.93628808864266\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  47\n",
      "Epsilon Used for the episode:  0.6196795642004924\n",
      "EPISODE COMPLETE\n",
      "min_reward: -128.0 || max_reward: 0.0 || total_reward: -21302.0 || average_reward: -59.00831024930748\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  48\n",
      "Epsilon Used for the episode:  0.6134234739945685\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -22337.0 || average_reward: -61.875346260387815\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  49\n",
      "Epsilon Used for the episode:  0.6072315792561702\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -22446.0 || average_reward: -62.177285318559555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  50\n",
      "Epsilon Used for the episode:  0.6011032212578952\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -21073.0 || average_reward: -58.37396121883656\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  51\n",
      "Epsilon Used for the episode:  0.5950377480317243\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -21793.0 || average_reward: -60.36842105263158\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  52\n",
      "Epsilon Used for the episode:  0.5890345142996626\n",
      "EPISODE COMPLETE\n",
      "min_reward: -129.0 || max_reward: 0.0 || total_reward: -20786.0 || average_reward: -57.578947368421055\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  53\n",
      "Epsilon Used for the episode:  0.5830928814050905\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -21837.0 || average_reward: -60.49030470914128\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  54\n",
      "Epsilon Used for the episode:  0.5772122172448201\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -21660.0 || average_reward: -60.0\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  55\n",
      "Epsilon Used for the episode:  0.5713918962018488\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -21802.0 || average_reward: -60.393351800554015\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  56\n",
      "Epsilon Used for the episode:  0.5656312990788025\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -21423.0 || average_reward: -59.34349030470914\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  57\n",
      "Epsilon Used for the episode:  0.5599298130320625\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -22812.0 || average_reward: -63.19113573407202\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  58\n",
      "Epsilon Used for the episode:  0.5542868315065671\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -21994.0 || average_reward: -60.92520775623269\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  59\n",
      "Epsilon Used for the episode:  0.5487017541712838\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -21065.0 || average_reward: -58.35180055401662\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  60\n",
      "Epsilon Used for the episode:  0.5431739868553422\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -21208.0 || average_reward: -58.74792243767313\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  61\n",
      "Epsilon Used for the episode:  0.5377029414848233\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -23139.0 || average_reward: -64.09695290858726\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  62\n",
      "Epsilon Used for the episode:  0.5322880360201966\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -21993.0 || average_reward: -60.9224376731302\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  63\n",
      "Epsilon Used for the episode:  0.5269286943944\n",
      "EPISODE COMPLETE\n",
      "min_reward: -132.0 || max_reward: 0.0 || total_reward: -21327.0 || average_reward: -59.0775623268698\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  64\n",
      "Epsilon Used for the episode:  0.5216243464515545\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -21283.0 || average_reward: -58.95567867036011\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  65\n",
      "Epsilon Used for the episode:  0.5163744278863077\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -21769.0 || average_reward: -60.30193905817175\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  66\n",
      "Epsilon Used for the episode:  0.5111783801838\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -22147.0 || average_reward: -61.34903047091413\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  67\n",
      "Epsilon Used for the episode:  0.5060356505602471\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -20778.0 || average_reward: -57.556786703601105\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  68\n",
      "Epsilon Used for the episode:  0.5009456919041315\n",
      "EPISODE COMPLETE\n",
      "min_reward: -126.0 || max_reward: 0.0 || total_reward: -21413.0 || average_reward: -59.31578947368421\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  69\n",
      "Epsilon Used for the episode:  0.4959079627179981\n",
      "EPISODE COMPLETE\n",
      "min_reward: -128.0 || max_reward: 0.0 || total_reward: -22181.0 || average_reward: -61.443213296398895\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  70\n",
      "Epsilon Used for the episode:  0.49092192706084614\n",
      "EPISODE COMPLETE\n",
      "min_reward: -130.0 || max_reward: 0.0 || total_reward: -22143.0 || average_reward: -61.337950138504155\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  71\n",
      "Epsilon Used for the episode:  0.4859870544911138\n",
      "EPISODE COMPLETE\n",
      "min_reward: -129.0 || max_reward: 0.0 || total_reward: -21294.0 || average_reward: -58.986149584487535\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  72\n",
      "Epsilon Used for the episode:  0.48110282001024607\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -22391.0 || average_reward: -62.02493074792244\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  73\n",
      "Epsilon Used for the episode:  0.4762687040068433\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -22777.0 || average_reward: -63.094182825484765\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  74\n",
      "Epsilon Used for the episode:  0.4714841922013815\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -22517.0 || average_reward: -62.37396121883656\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  75\n",
      "Epsilon Used for the episode:  0.46674877559150096\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -21448.0 || average_reward: -59.41274238227147\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  76\n",
      "Epsilon Used for the episode:  0.4620619503978553\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -21192.0 || average_reward: -58.70360110803324\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  77\n",
      "Epsilon Used for the episode:  0.45742321801051744\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -21416.0 || average_reward: -59.32409972299169\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  78\n",
      "Epsilon Used for the episode:  0.4528320849359339\n",
      "EPISODE COMPLETE\n",
      "min_reward: -133.0 || max_reward: 0.0 || total_reward: -20846.0 || average_reward: -57.745152354570635\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  79\n",
      "Epsilon Used for the episode:  0.44828806274442495\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -22451.0 || average_reward: -62.19113573407202\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  80\n",
      "Epsilon Used for the episode:  0.4437906680182224\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -22093.0 || average_reward: -61.199445983379505\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  81\n",
      "Epsilon Used for the episode:  0.43933942230004125\n",
      "EPISODE COMPLETE\n",
      "min_reward: -129.0 || max_reward: 0.0 || total_reward: -21607.0 || average_reward: -59.853185595567865\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  82\n",
      "Epsilon Used for the episode:  0.4349338520421786\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -22061.0 || average_reward: -61.11080332409972\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  83\n",
      "Epsilon Used for the episode:  0.43057348855613536\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -21550.0 || average_reward: -59.69529085872576\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  84\n",
      "Epsilon Used for the episode:  0.4262578679627543\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -20892.0 || average_reward: -57.87257617728532\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  85\n",
      "Epsilon Used for the episode:  0.42198653114287\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -21409.0 || average_reward: -59.30470914127424\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  86\n",
      "Epsilon Used for the episode:  0.4177590236884658\n",
      "EPISODE COMPLETE\n",
      "min_reward: -126.0 || max_reward: 0.0 || total_reward: -21183.0 || average_reward: -58.6786703601108\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  87\n",
      "Epsilon Used for the episode:  0.41357489585433094\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -21296.0 || average_reward: -58.99168975069252\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  88\n",
      "Epsilon Used for the episode:  0.4094337025102142\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -22352.0 || average_reward: -61.91689750692521\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  89\n",
      "Epsilon Used for the episode:  0.405335003093469\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -22010.0 || average_reward: -60.96952908587257\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  90\n",
      "Epsilon Used for the episode:  0.40127836156218316\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -22413.0 || average_reward: -62.08587257617729\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  91\n",
      "Epsilon Used for the episode:  0.39726334634879124\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -22904.0 || average_reward: -63.445983379501385\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  92\n",
      "Epsilon Used for the episode:  0.3932895303141615\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -21235.0 || average_reward: -58.822714681440445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  93\n",
      "Epsilon Used for the episode:  0.38935649070215467\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -21206.0 || average_reward: -58.742382271468145\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  94\n",
      "Epsilon Used for the episode:  0.38546380909464883\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -20962.0 || average_reward: -58.066481994459835\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  95\n",
      "Epsilon Used for the episode:  0.381611071367026\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -22233.0 || average_reward: -61.58725761772853\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  96\n",
      "Epsilon Used for the episode:  0.3777978676441149\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -20872.0 || average_reward: -57.81717451523546\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  97\n",
      "Epsilon Used for the episode:  0.3740237922565866\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -23066.0 || average_reward: -63.89473684210526\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  98\n",
      "Epsilon Used for the episode:  0.3702884436977972\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -21621.0 || average_reward: -59.89196675900277\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  99\n",
      "Epsilon Used for the episode:  0.3665914245810728\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -21062.0 || average_reward: -58.34349030470914\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  100\n",
      "Epsilon Used for the episode:  0.3629323415974344\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -23851.0 || average_reward: -66.06925207756233\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  101\n",
      "Epsilon Used for the episode:  0.35931080547375455\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -22746.0 || average_reward: -63.00831024930748\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  102\n",
      "Epsilon Used for the episode:  0.3557264309313447\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -22983.0 || average_reward: -63.664819944598335\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  103\n",
      "Epsilon Used for the episode:  0.3521788366449675\n",
      "EPISODE COMPLETE\n",
      "min_reward: -135.0 || max_reward: 0.0 || total_reward: -22880.0 || average_reward: -63.37950138504155\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  104\n",
      "Epsilon Used for the episode:  0.34866764520226884\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -22251.0 || average_reward: -61.637119113573405\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  105\n",
      "Epsilon Used for the episode:  0.345192483063627\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -23735.0 || average_reward: -65.74792243767313\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  106\n",
      "Epsilon Used for the episode:  0.34175298052241326\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -23632.0 || average_reward: -65.46260387811634\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  107\n",
      "Epsilon Used for the episode:  0.33834877166566074\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -22299.0 || average_reward: -61.770083102493075\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  108\n",
      "Epsilon Used for the episode:  0.3349794943351364\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -22504.0 || average_reward: -62.337950138504155\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  109\n",
      "Epsilon Used for the episode:  0.3316447900888127\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -23271.0 || average_reward: -64.46260387811634\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  110\n",
      "Epsilon Used for the episode:  0.32834430416273475\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -22955.0 || average_reward: -63.58725761772853\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  111\n",
      "Epsilon Used for the episode:  0.32507768543327836\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -23092.0 || average_reward: -63.96675900277008\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  112\n",
      "Epsilon Used for the episode:  0.3218445863797957\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -23985.0 || average_reward: -66.4404432132964\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  113\n",
      "Epsilon Used for the episode:  0.3186446630476443\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -23320.0 || average_reward: -64.5983379501385\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  114\n",
      "Epsilon Used for the episode:  0.3154775750115952\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -23335.0 || average_reward: -64.6398891966759\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  115\n",
      "Epsilon Used for the episode:  0.3123429853396163\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -22765.0 || average_reward: -63.06094182825485\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  116\n",
      "Epsilon Used for the episode:  0.3092405605570284\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -23781.0 || average_reward: -65.87534626038781\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  117\n",
      "Epsilon Used for the episode:  0.30616997061102796\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -24639.0 || average_reward: -68.25207756232687\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  118\n",
      "Epsilon Used for the episode:  0.30313088883557404\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -23361.0 || average_reward: -64.71191135734072\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  119\n",
      "Epsilon Used for the episode:  0.3001229919166362\n",
      "EPISODE COMPLETE\n",
      "min_reward: -133.0 || max_reward: 0.0 || total_reward: -22320.0 || average_reward: -61.82825484764543\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  120\n",
      "Epsilon Used for the episode:  0.29714595985779857\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -21586.0 || average_reward: -59.795013850415515\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  121\n",
      "Epsilon Used for the episode:  0.2941994759462167\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -24285.0 || average_reward: -67.27146814404432\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  122\n",
      "Epsilon Used for the episode:  0.29128322671892404\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -23316.0 || average_reward: -64.58725761772853\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  123\n",
      "Epsilon Used for the episode:  0.2883969019294839\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -23570.0 || average_reward: -65.29085872576178\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  124\n",
      "Epsilon Used for the episode:  0.285540194514984\n",
      "EPISODE COMPLETE\n",
      "min_reward: -133.0 || max_reward: 0.0 || total_reward: -22395.0 || average_reward: -62.03601108033241\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  125\n",
      "Epsilon Used for the episode:  0.2827128005633693\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -23724.0 || average_reward: -65.7174515235457\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  126\n",
      "Epsilon Used for the episode:  0.27991441928111016\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -25280.0 || average_reward: -70.02770083102493\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  127\n",
      "Epsilon Used for the episode:  0.27714475296120233\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -24294.0 || average_reward: -67.29639889196676\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  128\n",
      "Epsilon Used for the episode:  0.2744035069514953\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -24267.0 || average_reward: -67.22160664819944\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  129\n",
      "Epsilon Used for the episode:  0.27169038962334546\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -24212.0 || average_reward: -67.06925207756233\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  130\n",
      "Epsilon Used for the episode:  0.2690051123405914\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -23761.0 || average_reward: -65.81994459833795\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  131\n",
      "Epsilon Used for the episode:  0.2663473894288466\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -23809.0 || average_reward: -65.95290858725762\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  132\n",
      "Epsilon Used for the episode:  0.2637169381451087\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -22795.0 || average_reward: -63.14404432132964\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  133\n",
      "Epsilon Used for the episode:  0.2611134786476789\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -22874.0 || average_reward: -63.362880886426595\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  134\n",
      "Epsilon Used for the episode:  0.2585367339663915\n",
      "EPISODE COMPLETE\n",
      "min_reward: -129.0 || max_reward: 0.0 || total_reward: -22923.0 || average_reward: -63.498614958448755\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  135\n",
      "Epsilon Used for the episode:  0.255986429973148\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -23275.0 || average_reward: -64.47368421052632\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  136\n",
      "Epsilon Used for the episode:  0.25346229535275405\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -22690.0 || average_reward: -62.853185595567865\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  137\n",
      "Epsilon Used for the episode:  0.2509640615740551\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -24233.0 || average_reward: -67.12742382271468\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  138\n",
      "Epsilon Used for the episode:  0.2484914628613691\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -24188.0 || average_reward: -67.00277008310249\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  139\n",
      "Epsilon Used for the episode:  0.24604423616621154\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -24893.0 || average_reward: -68.95567867036011\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  140\n",
      "Epsilon Used for the episode:  0.24362212113931095\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -24684.0 || average_reward: -68.37673130193906\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  141\n",
      "Epsilon Used for the episode:  0.24122486010291155\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -25080.0 || average_reward: -69.47368421052632\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  142\n",
      "Epsilon Used for the episode:  0.2388521980233601\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -24370.0 || average_reward: -67.50692520775624\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  143\n",
      "Epsilon Used for the episode:  0.2365038824839741\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -24473.0 || average_reward: -67.79224376731302\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  144\n",
      "Epsilon Used for the episode:  0.2341796636581882\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -24990.0 || average_reward: -69.22437673130194\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  145\n",
      "Epsilon Used for the episode:  0.23187929428297635\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -24829.0 || average_reward: -68.77839335180056\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  146\n",
      "Epsilon Used for the episode:  0.2296025296325467\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -24715.0 || average_reward: -68.46260387811634\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  147\n",
      "Epsilon Used for the episode:  0.22734912749230618\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -23980.0 || average_reward: -66.42659279778394\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  148\n",
      "Epsilon Used for the episode:  0.22511884813309258\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -25154.0 || average_reward: -69.6786703601108\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  149\n",
      "Epsilon Used for the episode:  0.22291145428567058\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -24842.0 || average_reward: -68.81440443213296\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  150\n",
      "Epsilon Used for the episode:  0.2207267111154902\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -27409.0 || average_reward: -75.92520775623268\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  151\n",
      "Epsilon Used for the episode:  0.21856438619770327\n",
      "EPISODE COMPLETE\n",
      "min_reward: -168.0 || max_reward: 0.0 || total_reward: -26478.0 || average_reward: -73.34626038781164\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  152\n",
      "Epsilon Used for the episode:  0.2164242494924374\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -27159.0 || average_reward: -75.23268698060942\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  153\n",
      "Epsilon Used for the episode:  0.2143060733203226\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -25628.0 || average_reward: -70.99168975069252\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  154\n",
      "Epsilon Used for the episode:  0.21220963233826973\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -25340.0 || average_reward: -70.19390581717451\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  155\n",
      "Epsilon Used for the episode:  0.21013470351549732\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -24597.0 || average_reward: -68.13573407202216\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  156\n",
      "Epsilon Used for the episode:  0.20808106610980423\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -25144.0 || average_reward: -69.65096952908587\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  157\n",
      "Epsilon Used for the episode:  0.20604850164408597\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -26127.0 || average_reward: -72.37396121883657\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  158\n",
      "Epsilon Used for the episode:  0.20403679388309187\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -25872.0 || average_reward: -71.66759002770083\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  159\n",
      "Epsilon Used for the episode:  0.20204572881042085\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -25822.0 || average_reward: -71.52908587257618\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  160\n",
      "Epsilon Used for the episode:  0.20007509460575315\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -24873.0 || average_reward: -68.90027700831025\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  161\n",
      "Epsilon Used for the episode:  0.19812468162231572\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -27883.0 || average_reward: -77.23822714681441\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  162\n",
      "Epsilon Used for the episode:  0.19619428236457892\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -26789.0 || average_reward: -74.20775623268698\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  163\n",
      "Epsilon Used for the episode:  0.194283691466182\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -27534.0 || average_reward: -76.27146814404432\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  164\n",
      "Epsilon Used for the episode:  0.19239270566808503\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -27223.0 || average_reward: -75.40997229916897\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  165\n",
      "Epsilon Used for the episode:  0.19052112379694522\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -27441.0 || average_reward: -76.01385041551247\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  166\n",
      "Epsilon Used for the episode:  0.1886687467437149\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -27804.0 || average_reward: -77.01939058171745\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  167\n",
      "Epsilon Used for the episode:  0.18683537744245937\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -29037.0 || average_reward: -80.43490304709141\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  168\n",
      "Epsilon Used for the episode:  0.18502082084939167\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -27632.0 || average_reward: -76.54293628808864\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  169\n",
      "Epsilon Used for the episode:  0.18322488392212316\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -26899.0 || average_reward: -74.51246537396122\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  170\n",
      "Epsilon Used for the episode:  0.18144737559912627\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -26385.0 || average_reward: -73.08864265927978\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  171\n",
      "Epsilon Used for the episode:  0.1796881067794085\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -26959.0 || average_reward: -74.6786703601108\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  172\n",
      "Epsilon Used for the episode:  0.1779468903023948\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -26161.0 || average_reward: -72.46814404432133\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  173\n",
      "Epsilon Used for the episode:  0.17622354092801643\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -28293.0 || average_reward: -78.37396121883657\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  174\n",
      "Epsilon Used for the episode:  0.1745178753170041\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -26338.0 || average_reward: -72.9584487534626\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  175\n",
      "Epsilon Used for the episode:  0.17282971201138336\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -27963.0 || average_reward: -77.45983379501385\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  176\n",
      "Epsilon Used for the episode:  0.17115887141517003\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -26230.0 || average_reward: -72.65927977839335\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  177\n",
      "Epsilon Used for the episode:  0.16950517577526397\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -27366.0 || average_reward: -75.80609418282549\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  178\n",
      "Epsilon Used for the episode:  0.1678684491625385\n",
      "EPISODE COMPLETE\n",
      "min_reward: -191.0 || max_reward: 0.0 || total_reward: -26653.0 || average_reward: -73.83102493074792\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  179\n",
      "Epsilon Used for the episode:  0.1662485174531244\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -26746.0 || average_reward: -74.08864265927978\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  180\n",
      "Epsilon Used for the episode:  0.1646452083098854\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -28227.0 || average_reward: -78.19113573407202\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  181\n",
      "Epsilon Used for the episode:  0.16305835116408415\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -27164.0 || average_reward: -75.24653739612188\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  182\n",
      "Epsilon Used for the episode:  0.1614877771972362\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -27923.0 || average_reward: -77.34903047091413\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  183\n",
      "Epsilon Used for the episode:  0.15993331932315016\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -27823.0 || average_reward: -77.07202216066482\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  184\n",
      "Epsilon Used for the episode:  0.15839481217015206\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -27244.0 || average_reward: -75.46814404432133\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  185\n",
      "Epsilon Used for the episode:  0.15687209206349237\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -25945.0 || average_reward: -71.86980609418282\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  186\n",
      "Epsilon Used for the episode:  0.1553649970079333\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -26454.0 || average_reward: -73.2797783933518\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  187\n",
      "Epsilon Used for the episode:  0.1538733666705149\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -25338.0 || average_reward: -70.18836565096953\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  188\n",
      "Epsilon Used for the episode:  0.1523970423634979\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -25238.0 || average_reward: -69.91135734072022\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  189\n",
      "Epsilon Used for the episode:  0.1509358670274818\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -26991.0 || average_reward: -74.76731301939058\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  190\n",
      "Epsilon Used for the episode:  0.14948968521469583\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -27525.0 || average_reward: -76.24653739612188\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  191\n",
      "Epsilon Used for the episode:  0.14805834307246177\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -27345.0 || average_reward: -75.74792243767313\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  192\n",
      "Epsilon Used for the episode:  0.14664168832682611\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -27557.0 || average_reward: -76.33518005540166\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  193\n",
      "Epsilon Used for the episode:  0.1452395702663604\n",
      "EPISODE COMPLETE\n",
      "min_reward: -178.0 || max_reward: 0.0 || total_reward: -27880.0 || average_reward: -77.22991689750693\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  194\n",
      "Epsilon Used for the episode:  0.14385183972612783\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -29668.0 || average_reward: -82.18282548476455\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  195\n",
      "Epsilon Used for the episode:  0.14247834907181398\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -27816.0 || average_reward: -77.05263157894737\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  196\n",
      "Epsilon Used for the episode:  0.14111895218402096\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -27369.0 || average_reward: -75.81440443213296\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  197\n",
      "Epsilon Used for the episode:  0.13977350444272235\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -28029.0 || average_reward: -77.6426592797784\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  198\n",
      "Epsilon Used for the episode:  0.13844186271187772\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -27998.0 || average_reward: -77.5567867036011\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  199\n",
      "Epsilon Used for the episode:  0.13712388532420502\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -28861.0 || average_reward: -79.94736842105263\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  200\n",
      "Epsilon Used for the episode:  0.13581943206610925\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -29308.0 || average_reward: -81.18559556786704\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  201\n",
      "Epsilon Used for the episode:  0.13452836416276576\n",
      "EPISODE COMPLETE\n",
      "min_reward: -175.0 || max_reward: 0.0 || total_reward: -29862.0 || average_reward: -82.7202216066482\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  202\n",
      "Epsilon Used for the episode:  0.13325054426335672\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -30302.0 || average_reward: -83.93905817174515\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  203\n",
      "Epsilon Used for the episode:  0.13198583642645897\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -28816.0 || average_reward: -79.82271468144044\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  204\n",
      "Epsilon Used for the episode:  0.1307341061055817\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -27516.0 || average_reward: -76.22160664819944\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  205\n",
      "Epsilon Used for the episode:  0.12949522013485276\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -29072.0 || average_reward: -80.53185595567867\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  206\n",
      "Epsilon Used for the episode:  0.12826904671485187\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -29069.0 || average_reward: -80.5235457063712\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  207\n",
      "Epsilon Used for the episode:  0.12705545539858887\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -28745.0 || average_reward: -79.62603878116343\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  208\n",
      "Epsilon Used for the episode:  0.12585431707762612\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -29508.0 || average_reward: -81.73961218836565\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  209\n",
      "Epsilon Used for the episode:  0.12466550396834336\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -29208.0 || average_reward: -80.90858725761773\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  210\n",
      "Epsilon Used for the episode:  0.12348888959834328\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -28643.0 || average_reward: -79.34349030470914\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  211\n",
      "Epsilon Used for the episode:  0.12232434879299674\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -29234.0 || average_reward: -80.98060941828255\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  212\n",
      "Epsilon Used for the episode:  0.12117175766212597\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -28634.0 || average_reward: -79.31855955678671\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  213\n",
      "Epsilon Used for the episode:  0.1200309935868245\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -29013.0 || average_reward: -80.36842105263158\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  214\n",
      "Epsilon Used for the episode:  0.11890193520641232\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -27850.0 || average_reward: -77.14681440443213\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  215\n",
      "Epsilon Used for the episode:  0.11778446240552481\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -29179.0 || average_reward: -80.82825484764543\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  216\n",
      "Epsilon Used for the episode:  0.11667845630133426\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -29073.0 || average_reward: -80.53462603878117\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  217\n",
      "Epsilon Used for the episode:  0.11558379923090245\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -28039.0 || average_reward: -77.67036011080333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  218\n",
      "Epsilon Used for the episode:  0.11450037473866301\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -26931.0 || average_reward: -74.60110803324099\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  219\n",
      "Epsilon Used for the episode:  0.11342806756403227\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -28581.0 || average_reward: -79.17174515235457\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  220\n",
      "Epsilon Used for the episode:  0.11236676362914723\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -29263.0 || average_reward: -81.06094182825485\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  221\n",
      "Epsilon Used for the episode:  0.1113163500267293\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -27891.0 || average_reward: -77.26038781163435\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  222\n",
      "Epsilon Used for the episode:  0.11027671500807261\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -27312.0 || average_reward: -75.65650969529086\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  223\n",
      "Epsilon Used for the episode:  0.1092477479711556\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -27984.0 || average_reward: -77.5180055401662\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  224\n",
      "Epsilon Used for the episode:  0.10822933944887461\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -27742.0 || average_reward: -76.84764542936288\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  225\n",
      "Epsilon Used for the episode:  0.10722138109739804\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -26993.0 || average_reward: -74.77285318559557\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  226\n",
      "Epsilon Used for the episode:  0.1062237656846403\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -27588.0 || average_reward: -76.42105263157895\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  227\n",
      "Epsilon Used for the episode:  0.10523638707885383\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -27695.0 || average_reward: -76.7174515235457\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  228\n",
      "Epsilon Used for the episode:  0.10425914023733812\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -28066.0 || average_reward: -77.74515235457064\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  229\n",
      "Epsilon Used for the episode:  0.10329192119526491\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -28196.0 || average_reward: -78.10526315789474\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  230\n",
      "Epsilon Used for the episode:  0.10233462705461761\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -29561.0 || average_reward: -81.88642659279779\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  231\n",
      "Epsilon Used for the episode:  0.10138715597324466\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -30196.0 || average_reward: -83.64542936288089\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  232\n",
      "Epsilon Used for the episode:  0.10044940715402487\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -27214.0 || average_reward: -75.38504155124653\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  233\n",
      "Epsilon Used for the episode:  0.09952128083414409\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -26500.0 || average_reward: -73.40720221606648\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  234\n",
      "Epsilon Used for the episode:  0.09860267827448198\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -30232.0 || average_reward: -83.74515235457064\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  235\n",
      "Epsilon Used for the episode:  0.09769350174910751\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -27771.0 || average_reward: -76.92797783933518\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  236\n",
      "Epsilon Used for the episode:  0.09679365453488246\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -28128.0 || average_reward: -77.91689750692521\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  237\n",
      "Epsilon Used for the episode:  0.09590304090117133\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -28141.0 || average_reward: -77.95290858725762\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  238\n",
      "Epsilon Used for the episode:  0.09502156609965712\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -28551.0 || average_reward: -79.08864265927978\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  239\n",
      "Epsilon Used for the episode:  0.09414913635426145\n",
      "EPISODE COMPLETE\n",
      "min_reward: -173.0 || max_reward: 0.0 || total_reward: -27565.0 || average_reward: -76.3573407202216\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  240\n",
      "Epsilon Used for the episode:  0.09328565885116816\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -25636.0 || average_reward: -71.01385041551247\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  241\n",
      "Epsilon Used for the episode:  0.09243104172894923\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -26602.0 || average_reward: -73.68975069252078\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  242\n",
      "Epsilon Used for the episode:  0.09158519406879212\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -28501.0 || average_reward: -78.95013850415512\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  243\n",
      "Epsilon Used for the episode:  0.09074802588482733\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -24782.0 || average_reward: -68.64819944598338\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  244\n",
      "Epsilon Used for the episode:  0.08991944811455516\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -25580.0 || average_reward: -70.85872576177286\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  245\n",
      "Epsilon Used for the episode:  0.08909937260937077\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -24963.0 || average_reward: -69.14958448753463\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  246\n",
      "Epsilon Used for the episode:  0.08828771212518653\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -26892.0 || average_reward: -74.49307479224376\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  247\n",
      "Epsilon Used for the episode:  0.08748438031315042\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -26737.0 || average_reward: -74.06371191135734\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  248\n",
      "Epsilon Used for the episode:  0.08668929171045982\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -27856.0 || average_reward: -77.16343490304709\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  249\n",
      "Epsilon Used for the episode:  0.0859023617312695\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -26122.0 || average_reward: -72.3601108033241\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  250\n",
      "Epsilon Used for the episode:  0.08512350665769297\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -29255.0 || average_reward: -81.0387811634349\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  251\n",
      "Epsilon Used for the episode:  0.084352643630896\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -27051.0 || average_reward: -74.93351800554017\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  252\n",
      "Epsilon Used for the episode:  0.08358969064228175\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -28873.0 || average_reward: -79.98060941828255\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  253\n",
      "Epsilon Used for the episode:  0.08283456652476621\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -28768.0 || average_reward: -79.68975069252078\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  254\n",
      "Epsilon Used for the episode:  0.08208719094414324\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -26848.0 || average_reward: -74.37119113573407\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  255\n",
      "Epsilon Used for the episode:  0.08134748439053811\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -28469.0 || average_reward: -78.86149584487535\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  256\n",
      "Epsilon Used for the episode:  0.08061536816994888\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -29320.0 || average_reward: -81.21883656509695\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  257\n",
      "Epsilon Used for the episode:  0.07989076439587446\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -30335.0 || average_reward: -84.03047091412742\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  258\n",
      "Epsilon Used for the episode:  0.07917359598102862\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -27115.0 || average_reward: -75.11080332409972\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  259\n",
      "Epsilon Used for the episode:  0.07846378662913908\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -29330.0 || average_reward: -81.24653739612188\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  260\n",
      "Epsilon Used for the episode:  0.07776126082683064\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -29958.0 || average_reward: -82.98614958448753\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  261\n",
      "Epsilon Used for the episode:  0.07706594383559165\n",
      "EPISODE COMPLETE\n",
      "min_reward: -179.0 || max_reward: 0.0 || total_reward: -29755.0 || average_reward: -82.42382271468144\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  262\n",
      "Epsilon Used for the episode:  0.07637776168382296\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -30310.0 || average_reward: -83.9612188365651\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  263\n",
      "Epsilon Used for the episode:  0.07569664115896844\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -29314.0 || average_reward: -81.202216066482\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  264\n",
      "Epsilon Used for the episode:  0.07502250979972616\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -28547.0 || average_reward: -79.07756232686981\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  265\n",
      "Epsilon Used for the episode:  0.07435529588833963\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -29709.0 || average_reward: -82.29639889196676\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  266\n",
      "Epsilon Used for the episode:  0.07369492844296796\n",
      "EPISODE COMPLETE\n",
      "min_reward: -172.0 || max_reward: 0.0 || total_reward: -28940.0 || average_reward: -80.16620498614958\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  267\n",
      "Epsilon Used for the episode:  0.07304133721013466\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -30798.0 || average_reward: -85.31301939058172\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  268\n",
      "Epsilon Used for the episode:  0.07239445265725342\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -30315.0 || average_reward: -83.97506925207756\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  269\n",
      "Epsilon Used for the episode:  0.07175420596523101\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -28416.0 || average_reward: -78.71468144044321\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  270\n",
      "Epsilon Used for the episode:  0.07112052902114593\n",
      "EPISODE COMPLETE\n",
      "min_reward: -173.0 || max_reward: 0.0 || total_reward: -28273.0 || average_reward: -78.31855955678671\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  271\n",
      "Epsilon Used for the episode:  0.07049335441100214\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -28605.0 || average_reward: -79.23822714681441\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  272\n",
      "Epsilon Used for the episode:  0.06987261541255721\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -28351.0 || average_reward: -78.53462603878117\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  273\n",
      "Epsilon Used for the episode:  0.06925824598822405\n",
      "EPISODE COMPLETE\n",
      "min_reward: -176.0 || max_reward: 0.0 || total_reward: -28401.0 || average_reward: -78.67313019390582\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  274\n",
      "Epsilon Used for the episode:  0.06865018077804547\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -29469.0 || average_reward: -81.63157894736842\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  275\n",
      "Epsilon Used for the episode:  0.06804835509274082\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -27590.0 || average_reward: -76.42659279778394\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  276\n",
      "Epsilon Used for the episode:  0.06745270490682405\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -28354.0 || average_reward: -78.54293628808864\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  277\n",
      "Epsilon Used for the episode:  0.06686316685179221\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -28497.0 || average_reward: -78.93905817174515\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  278\n",
      "Epsilon Used for the episode:  0.06627967820938409\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -28205.0 || average_reward: -78.13019390581718\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  279\n",
      "Epsilon Used for the episode:  0.06570217690490787\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -28893.0 || average_reward: -80.03601108033241\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  280\n",
      "Epsilon Used for the episode:  0.06513060150063725\n",
      "EPISODE COMPLETE\n",
      "min_reward: -182.0 || max_reward: 0.0 || total_reward: -29154.0 || average_reward: -80.7590027700831\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  281\n",
      "Epsilon Used for the episode:  0.06456489118927537\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -28714.0 || average_reward: -79.54016620498615\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  282\n",
      "Epsilon Used for the episode:  0.06400498578748587\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -28305.0 || average_reward: -78.40720221606648\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  283\n",
      "Epsilon Used for the episode:  0.06345082572949014\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -30860.0 || average_reward: -85.48476454293629\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  284\n",
      "Epsilon Used for the episode:  0.06290235206073053\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -29423.0 || average_reward: -81.50415512465374\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  285\n",
      "Epsilon Used for the episode:  0.06235950643159831\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -29474.0 || average_reward: -81.64542936288089\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  286\n",
      "Epsilon Used for the episode:  0.061822231091226205\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -26282.0 || average_reward: -72.80332409972299\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  287\n",
      "Epsilon Used for the episode:  0.06129046888134455\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -30061.0 || average_reward: -83.27146814404432\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  288\n",
      "Epsilon Used for the episode:  0.06076416323020039\n",
      "EPISODE COMPLETE\n",
      "min_reward: -175.0 || max_reward: 0.0 || total_reward: -28794.0 || average_reward: -79.76177285318559\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  289\n",
      "Epsilon Used for the episode:  0.06024325814653919\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -29382.0 || average_reward: -81.39058171745152\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  290\n",
      "Epsilon Used for the episode:  0.05972769821364811\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -30549.0 || average_reward: -84.62326869806094\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  291\n",
      "Epsilon Used for the episode:  0.05921742858346047\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -29181.0 || average_reward: -80.83379501385042\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  292\n",
      "Epsilon Used for the episode:  0.05871239497072076\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -28832.0 || average_reward: -79.86703601108033\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  293\n",
      "Epsilon Used for the episode:  0.05821254364720944\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -28803.0 || average_reward: -79.78670360110803\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  294\n",
      "Epsilon Used for the episode:  0.05771782143602705\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -27995.0 || average_reward: -77.54847645429363\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  295\n",
      "Epsilon Used for the episode:  0.05722817570593696\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -27888.0 || average_reward: -77.25207756232687\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  296\n",
      "Epsilon Used for the episode:  0.056743554365766204\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -27724.0 || average_reward: -76.797783933518\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  297\n",
      "Epsilon Used for the episode:  0.05626390585886369\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -27883.0 || average_reward: -77.23822714681441\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  298\n",
      "Epsilon Used for the episode:  0.05578917915761535\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -29056.0 || average_reward: -80.48753462603878\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  299\n",
      "Epsilon Used for the episode:  0.05531932375801556\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -27017.0 || average_reward: -74.8393351800554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  300\n",
      "Epsilon Used for the episode:  0.05485428967429419\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -30319.0 || average_reward: -83.98614958448753\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  301\n",
      "Epsilon Used for the episode:  0.05439402743359894\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -32541.0 || average_reward: -90.14127423822714\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  302\n",
      "Epsilon Used for the episode:  0.053938488070732045\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -32189.0 || average_reward: -89.16620498614958\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  303\n",
      "Epsilon Used for the episode:  0.05348762312294118\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -29885.0 || average_reward: -82.78393351800554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  304\n",
      "Epsilon Used for the episode:  0.05304138462476368\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -29470.0 || average_reward: -81.63434903047091\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  305\n",
      "Epsilon Used for the episode:  0.05259972510292371\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -31336.0 || average_reward: -86.80332409972299\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  306\n",
      "Epsilon Used for the episode:  0.05216259757128184\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -30114.0 || average_reward: -83.41828254847645\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  307\n",
      "Epsilon Used for the episode:  0.05172995552583637\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -29705.0 || average_reward: -82.28531855955679\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  308\n",
      "Epsilon Used for the episode:  0.05130175293977598\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -30328.0 || average_reward: -84.01108033240997\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  309\n",
      "Epsilon Used for the episode:  0.050877944258583156\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -31020.0 || average_reward: -85.92797783933518\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  310\n",
      "Epsilon Used for the episode:  0.05045848439518789\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -30800.0 || average_reward: -85.31855955678671\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  311\n",
      "Epsilon Used for the episode:  0.050043328725171\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -28587.0 || average_reward: -79.18836565096953\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  312\n",
      "Epsilon Used for the episode:  0.0496324330820168\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -31463.0 || average_reward: -87.1551246537396\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  313\n",
      "Epsilon Used for the episode:  0.0492257537524144\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -28423.0 || average_reward: -78.73407202216066\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  314\n",
      "Epsilon Used for the episode:  0.04882324747160725\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -28588.0 || average_reward: -79.19113573407202\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  315\n",
      "Epsilon Used for the episode:  0.04842487141879041\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -27543.0 || average_reward: -76.29639889196676\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  316\n",
      "Epsilon Used for the episode:  0.04803058321255504\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -28533.0 || average_reward: -79.0387811634349\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  317\n",
      "Epsilon Used for the episode:  0.04764034090637959\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -28240.0 || average_reward: -78.22714681440443\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  318\n",
      "Epsilon Used for the episode:  0.047254102984167386\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -27868.0 || average_reward: -77.19667590027701\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  319\n",
      "Epsilon Used for the episode:  0.046871828355829856\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -26728.0 || average_reward: -74.0387811634349\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  320\n",
      "Epsilon Used for the episode:  0.04649347635291516\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -28368.0 || average_reward: -78.58171745152355\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  321\n",
      "Epsilon Used for the episode:  0.04611900672428167\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -30615.0 || average_reward: -84.80609418282549\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  322\n",
      "Epsilon Used for the episode:  0.045748379631815814\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -28807.0 || average_reward: -79.797783933518\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  323\n",
      "Epsilon Used for the episode:  0.04538155564619391\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -28984.0 || average_reward: -80.28808864265928\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  324\n",
      "Epsilon Used for the episode:  0.045018495742687424\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -29475.0 || average_reward: -81.64819944598338\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  325\n",
      "Epsilon Used for the episode:  0.04465916129701135\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -27627.0 || average_reward: -76.52908587257618\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  326\n",
      "Epsilon Used for the episode:  0.04430351408121511\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -27542.0 || average_reward: -76.29362880886427\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  327\n",
      "Epsilon Used for the episode:  0.04395151625961568\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -29059.0 || average_reward: -80.49584487534626\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  328\n",
      "Epsilon Used for the episode:  0.04360313038477242\n",
      "EPISODE COMPLETE\n",
      "min_reward: -172.0 || max_reward: 0.0 || total_reward: -28706.0 || average_reward: -79.5180055401662\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  329\n",
      "Epsilon Used for the episode:  0.04325831939350319\n",
      "EPISODE COMPLETE\n",
      "min_reward: -173.0 || max_reward: 0.0 || total_reward: -28075.0 || average_reward: -77.77008310249307\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  330\n",
      "Epsilon Used for the episode:  0.042917046602941426\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -27099.0 || average_reward: -75.06648199445983\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  331\n",
      "Epsilon Used for the episode:  0.042579275706633536\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -30958.0 || average_reward: -85.75623268698061\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  332\n",
      "Epsilon Used for the episode:  0.042244970770676495\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -29601.0 || average_reward: -81.99722991689751\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  333\n",
      "Epsilon Used for the episode:  0.04191409622989494\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -30236.0 || average_reward: -83.75623268698061\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  334\n",
      "Epsilon Used for the episode:  0.04158661688405764\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -30757.0 || average_reward: -85.1994459833795\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  335\n",
      "Epsilon Used for the episode:  0.04126249789413257\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -30226.0 || average_reward: -83.72853185595568\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  336\n",
      "Epsilon Used for the episode:  0.04094170477858067\n",
      "EPISODE COMPLETE\n",
      "min_reward: -175.0 || max_reward: 0.0 || total_reward: -31550.0 || average_reward: -87.3961218836565\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  337\n",
      "Epsilon Used for the episode:  0.04062420340968746\n",
      "EPISODE COMPLETE\n",
      "min_reward: -168.0 || max_reward: 0.0 || total_reward: -29770.0 || average_reward: -82.46537396121883\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  338\n",
      "Epsilon Used for the episode:  0.040309960009932366\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -29781.0 || average_reward: -82.49584487534626\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  339\n",
      "Epsilon Used for the episode:  0.03999894114839525\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -31545.0 || average_reward: -87.38227146814404\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  340\n",
      "Epsilon Used for the episode:  0.03969111373719987\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -28438.0 || average_reward: -78.77562326869806\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  341\n",
      "Epsilon Used for the episode:  0.03938644502799383\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -31680.0 || average_reward: -87.75623268698061\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  342\n",
      "Epsilon Used for the episode:  0.03908490260846458\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -29577.0 || average_reward: -81.93074792243767\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  343\n",
      "Epsilon Used for the episode:  0.03878645439889128\n",
      "EPISODE COMPLETE\n",
      "min_reward: -173.0 || max_reward: 0.0 || total_reward: -29041.0 || average_reward: -80.44598337950139\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  344\n",
      "Epsilon Used for the episode:  0.038491068648731946\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -29877.0 || average_reward: -82.76177285318559\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  345\n",
      "Epsilon Used for the episode:  0.038198713933245663\n",
      "EPISODE COMPLETE\n",
      "min_reward: -172.0 || max_reward: 0.0 || total_reward: -29551.0 || average_reward: -81.85872576177286\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  346\n",
      "Epsilon Used for the episode:  0.037909359150149445\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -30784.0 || average_reward: -85.27423822714681\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  347\n",
      "Epsilon Used for the episode:  0.03762297351630943\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -29214.0 || average_reward: -80.92520775623268\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  348\n",
      "Epsilon Used for the episode:  0.037339526564465965\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -30359.0 || average_reward: -84.09695290858726\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  349\n",
      "Epsilon Used for the episode:  0.03705898813999238\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -28548.0 || average_reward: -79.0803324099723\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  350\n",
      "Epsilon Used for the episode:  0.03678132839768691\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -30859.0 || average_reward: -85.4819944598338\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  351\n",
      "Epsilon Used for the episode:  0.036506517798597676\n",
      "EPISODE COMPLETE\n",
      "min_reward: -172.0 || max_reward: 0.0 || total_reward: -30574.0 || average_reward: -84.69252077562327\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  352\n",
      "Epsilon Used for the episode:  0.03623452710688011\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -31021.0 || average_reward: -85.93074792243767\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  353\n",
      "Epsilon Used for the episode:  0.03596532738668672\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -28412.0 || average_reward: -78.70360110803324\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  354\n",
      "Epsilon Used for the episode:  0.03569888999908874\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -30575.0 || average_reward: -84.69529085872576\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  355\n",
      "Epsilon Used for the episode:  0.03543518659902932\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -29847.0 || average_reward: -82.6786703601108\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  356\n",
      "Epsilon Used for the episode:  0.03517418913230808\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -28397.0 || average_reward: -78.66204986149584\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  357\n",
      "Epsilon Used for the episode:  0.03491586983259654\n",
      "EPISODE COMPLETE\n",
      "min_reward: -185.0 || max_reward: 0.0 || total_reward: -30110.0 || average_reward: -83.40720221606648\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  358\n",
      "Epsilon Used for the episode:  0.03466020121848416\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -28678.0 || average_reward: -79.4404432132964\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  359\n",
      "Epsilon Used for the episode:  0.03440715609055473\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -30984.0 || average_reward: -85.82825484764543\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  360\n",
      "Epsilon Used for the episode:  0.03415670752849277\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -31319.0 || average_reward: -86.75623268698061\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  361\n",
      "Epsilon Used for the episode:  0.033908828888219576\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -31107.0 || average_reward: -86.16897506925208\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  362\n",
      "Epsilon Used for the episode:  0.03366349379905868\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -30295.0 || average_reward: -83.9196675900277\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  363\n",
      "Epsilon Used for the episode:  0.033420676160930425\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -31707.0 || average_reward: -87.83102493074792\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  364\n",
      "Epsilon Used for the episode:  0.03318035014157523\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -30380.0 || average_reward: -84.1551246537396\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  365\n",
      "Epsilon Used for the episode:  0.032942490173805496\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -29883.0 || average_reward: -82.77839335180056\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  366\n",
      "Epsilon Used for the episode:  0.032707070952785564\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -30807.0 || average_reward: -85.33795013850416\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  367\n",
      "Epsilon Used for the episode:  0.03247406743333969\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -29886.0 || average_reward: -82.78670360110803\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  368\n",
      "Epsilon Used for the episode:  0.03224345482728758\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -30310.0 || average_reward: -83.9612188365651\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  369\n",
      "Epsilon Used for the episode:  0.0320152086008073\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -30894.0 || average_reward: -85.57894736842105\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  370\n",
      "Epsilon Used for the episode:  0.03178930447182523\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -28046.0 || average_reward: -77.68975069252078\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  371\n",
      "Epsilon Used for the episode:  0.031565718407432795\n",
      "EPISODE COMPLETE\n",
      "min_reward: -168.0 || max_reward: 0.0 || total_reward: -28690.0 || average_reward: -79.47368421052632\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  372\n",
      "Epsilon Used for the episode:  0.03134442662132975\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -27167.0 || average_reward: -75.25484764542936\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  373\n",
      "Epsilon Used for the episode:  0.031125405571293625\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -26080.0 || average_reward: -72.24376731301939\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  374\n",
      "Epsilon Used for the episode:  0.030908631956675175\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -27310.0 || average_reward: -75.65096952908587\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  375\n",
      "Epsilon Used for the episode:  0.03069408271591957\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -27717.0 || average_reward: -76.77839335180056\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  376\n",
      "Epsilon Used for the episode:  0.030481735024112944\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -27375.0 || average_reward: -75.83102493074792\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  377\n",
      "Epsilon Used for the episode:  0.03027156629055417\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -27813.0 || average_reward: -77.04432132963989\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  378\n",
      "Epsilon Used for the episode:  0.030063554156351542\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -27189.0 || average_reward: -75.3157894736842\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  379\n",
      "Epsilon Used for the episode:  0.029857676492044107\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -27803.0 || average_reward: -77.01662049861496\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  380\n",
      "Epsilon Used for the episode:  0.029653911395247402\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -29780.0 || average_reward: -82.49307479224376\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  381\n",
      "Epsilon Used for the episode:  0.029452237188323405\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -28805.0 || average_reward: -79.79224376731302\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  382\n",
      "Epsilon Used for the episode:  0.029252632416074287\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -29306.0 || average_reward: -81.18005540166205\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  383\n",
      "Epsilon Used for the episode:  0.029055075843459947\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -27781.0 || average_reward: -76.95567867036011\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  384\n",
      "Epsilon Used for the episode:  0.028859546453338873\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -29714.0 || average_reward: -82.31024930747922\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  385\n",
      "Epsilon Used for the episode:  0.02866602344423224\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -29656.0 || average_reward: -82.14958448753463\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  386\n",
      "Epsilon Used for the episode:  0.02847448622811094\n",
      "EPISODE COMPLETE\n",
      "min_reward: -196.0 || max_reward: 0.0 || total_reward: -31562.0 || average_reward: -87.42936288088643\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  387\n",
      "Epsilon Used for the episode:  0.02828491442820532\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -30716.0 || average_reward: -85.08587257617728\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  388\n",
      "Epsilon Used for the episode:  0.02809728787683738\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -30338.0 || average_reward: -84.0387811634349\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  389\n",
      "Epsilon Used for the episode:  0.027911586613275197\n",
      "EPISODE COMPLETE\n",
      "min_reward: -168.0 || max_reward: 0.0 || total_reward: -30683.0 || average_reward: -84.99445983379502\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  390\n",
      "Epsilon Used for the episode:  0.027727790881609482\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -30039.0 || average_reward: -83.21052631578948\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  391\n",
      "Epsilon Used for the episode:  0.027545881128651716\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -29054.0 || average_reward: -80.4819944598338\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  392\n",
      "Epsilon Used for the episode:  0.02736583800185411\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -30266.0 || average_reward: -83.8393351800554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  393\n",
      "Epsilon Used for the episode:  0.027187642347250636\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -30173.0 || average_reward: -83.58171745152355\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  394\n",
      "Epsilon Used for the episode:  0.027011275207419472\n",
      "EPISODE COMPLETE\n",
      "min_reward: -176.0 || max_reward: 0.0 || total_reward: -29812.0 || average_reward: -82.58171745152355\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  395\n",
      "Epsilon Used for the episode:  0.026836717819466054\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -30388.0 || average_reward: -84.17728531855956\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  396\n",
      "Epsilon Used for the episode:  0.02666395161302712\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -30078.0 || average_reward: -83.31855955678671\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  397\n",
      "Epsilon Used for the episode:  0.026492958208294975\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -30238.0 || average_reward: -83.76177285318559\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  398\n",
      "Epsilon Used for the episode:  0.026323719414062252\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -30143.0 || average_reward: -83.49861495844875\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  399\n",
      "Epsilon Used for the episode:  0.02615621722578653\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -31159.0 || average_reward: -86.31301939058172\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  400\n",
      "Epsilon Used for the episode:  0.025990433823675024\n",
      "EPISODE COMPLETE\n",
      "min_reward: -182.0 || max_reward: 0.0 || total_reward: -30839.0 || average_reward: -85.42659279778394\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  401\n",
      "Epsilon Used for the episode:  0.0258263515707887\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -29332.0 || average_reward: -81.25207756232687\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  402\n",
      "Epsilon Used for the episode:  0.025663953011166064\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -29465.0 || average_reward: -81.62049861495845\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  403\n",
      "Epsilon Used for the episode:  0.025503220867966022\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -30612.0 || average_reward: -84.797783933518\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  404\n",
      "Epsilon Used for the episode:  0.02534413804162996\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -29496.0 || average_reward: -81.70637119113573\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  405\n",
      "Epsilon Used for the episode:  0.025186687608062495\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -29271.0 || average_reward: -81.08310249307479\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  406\n",
      "Epsilon Used for the episode:  0.025030852816831116\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -28530.0 || average_reward: -79.03047091412742\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  407\n",
      "Epsilon Used for the episode:  0.024876617089384063\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -28553.0 || average_reward: -79.09418282548476\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  408\n",
      "Epsilon Used for the episode:  0.024723964017286725\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -29398.0 || average_reward: -81.43490304709141\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  409\n",
      "Epsilon Used for the episode:  0.02457287736047591\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -29645.0 || average_reward: -82.1191135734072\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  410\n",
      "Epsilon Used for the episode:  0.024423341045532246\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -30237.0 || average_reward: -83.7590027700831\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  411\n",
      "Epsilon Used for the episode:  0.02427533916397011\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -30079.0 || average_reward: -83.3213296398892\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  412\n",
      "Epsilon Used for the episode:  0.024128855970545267\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -30152.0 || average_reward: -83.5235457063712\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  413\n",
      "Epsilon Used for the episode:  0.023983875881579755\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -29032.0 || average_reward: -80.42105263157895\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  414\n",
      "Epsilon Used for the episode:  0.023840383473304066\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -30246.0 || average_reward: -83.78393351800554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  415\n",
      "Epsilon Used for the episode:  0.023698363480216193\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -29889.0 || average_reward: -82.79501385041551\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  416\n",
      "Epsilon Used for the episode:  0.023557800793457706\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -29122.0 || average_reward: -80.67036011080333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  417\n",
      "Epsilon Used for the episode:  0.023418680459206306\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -29211.0 || average_reward: -80.91689750692521\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  418\n",
      "Epsilon Used for the episode:  0.023280987677085016\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -30135.0 || average_reward: -83.4764542936288\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  419\n",
      "Epsilon Used for the episode:  0.023144707798587585\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -28396.0 || average_reward: -78.65927977839335\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  420\n",
      "Epsilon Used for the episode:  0.023009826325520157\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -29936.0 || average_reward: -82.92520775623268\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  421\n",
      "Epsilon Used for the episode:  0.022876328908458796\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -31570.0 || average_reward: -87.45152354570637\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  422\n",
      "Epsilon Used for the episode:  0.02274420134522302\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -31017.0 || average_reward: -85.9196675900277\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  423\n",
      "Epsilon Used for the episode:  0.02261342957936479\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -30776.0 || average_reward: -85.25207756232687\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  424\n",
      "Epsilon Used for the episode:  0.022483999698673193\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -31326.0 || average_reward: -86.77562326869806\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  425\n",
      "Epsilon Used for the episode:  0.02235589793369433\n",
      "EPISODE COMPLETE\n",
      "min_reward: -168.0 || max_reward: 0.0 || total_reward: -31441.0 || average_reward: -87.09418282548476\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  426\n",
      "Epsilon Used for the episode:  0.02222911065626648\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -29847.0 || average_reward: -82.6786703601108\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  427\n",
      "Epsilon Used for the episode:  0.02210362437807023\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -30265.0 || average_reward: -83.83656509695291\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  428\n",
      "Epsilon Used for the episode:  0.021979425749193565\n",
      "EPISODE COMPLETE\n",
      "min_reward: -199.0 || max_reward: 0.0 || total_reward: -30918.0 || average_reward: -85.64542936288089\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  429\n",
      "Epsilon Used for the episode:  0.02185650155671156\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -31283.0 || average_reward: -86.65650969529086\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  430\n",
      "Epsilon Used for the episode:  0.021734838723280803\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -31459.0 || average_reward: -87.14404432132964\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  431\n",
      "Epsilon Used for the episode:  0.021614424305748062\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -30293.0 || average_reward: -83.91412742382272\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  432\n",
      "Epsilon Used for the episode:  0.02149524549377342\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -29623.0 || average_reward: -82.05817174515235\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  433\n",
      "Epsilon Used for the episode:  0.021377289608467347\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -28362.0 || average_reward: -78.56509695290859\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  434\n",
      "Epsilon Used for the episode:  0.021260544101041938\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -28597.0 || average_reward: -79.21606648199446\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  435\n",
      "Epsilon Used for the episode:  0.02114499655147582\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -30232.0 || average_reward: -83.74515235457064\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  436\n",
      "Epsilon Used for the episode:  0.021030634667192925\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -29024.0 || average_reward: -80.39889196675901\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  437\n",
      "Epsilon Used for the episode:  0.02091744628175467\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -29987.0 || average_reward: -83.06648199445983\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  438\n",
      "Epsilon Used for the episode:  0.020805419353565682\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -29039.0 || average_reward: -80.4404432132964\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  439\n",
      "Epsilon Used for the episode:  0.020694541964592687\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -28575.0 || average_reward: -79.1551246537396\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  440\n",
      "Epsilon Used for the episode:  0.02058480231909667\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -27917.0 || average_reward: -77.33240997229917\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  441\n",
      "Epsilon Used for the episode:  0.02047618874237792\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -28325.0 || average_reward: -78.46260387811634\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  442\n",
      "Epsilon Used for the episode:  0.02036868967953407\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -28647.0 || average_reward: -79.35457063711911\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  443\n",
      "Epsilon Used for the episode:  0.020262293694230767\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -29115.0 || average_reward: -80.65096952908587\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  444\n",
      "Epsilon Used for the episode:  0.020156989467485065\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -29170.0 || average_reward: -80.80332409972299\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  445\n",
      "Epsilon Used for the episode:  0.020052765796461203\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -27136.0 || average_reward: -75.16897506925208\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  446\n",
      "Epsilon Used for the episode:  0.019949611593278814\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -29060.0 || average_reward: -80.49861495844875\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  447\n",
      "Epsilon Used for the episode:  0.01984751588383334\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -29762.0 || average_reward: -82.4432132963989\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  448\n",
      "Epsilon Used for the episode:  0.019746467806628518\n",
      "EPISODE COMPLETE\n",
      "min_reward: -187.0 || max_reward: 0.0 || total_reward: -29592.0 || average_reward: -81.97229916897507\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  449\n",
      "Epsilon Used for the episode:  0.019646456611620927\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -27588.0 || average_reward: -76.42105263157895\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  450\n",
      "Epsilon Used for the episode:  0.01954747165907627\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -29777.0 || average_reward: -82.48476454293629\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  451\n",
      "Epsilon Used for the episode:  0.019449502418437528\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -29191.0 || average_reward: -80.86149584487535\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  452\n",
      "Epsilon Used for the episode:  0.01935253846720461\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -30140.0 || average_reward: -83.49030470914127\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  453\n",
      "Epsilon Used for the episode:  0.019256569489825598\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -30095.0 || average_reward: -83.36565096952909\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  454\n",
      "Epsilon Used for the episode:  0.01916158527659927\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -29545.0 || average_reward: -81.84210526315789\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  455\n",
      "Epsilon Used for the episode:  0.019067575722588992\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -29989.0 || average_reward: -83.07202216066482\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  456\n",
      "Epsilon Used for the episode:  0.018974530826547653\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -29193.0 || average_reward: -80.86703601108033\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  457\n",
      "Epsilon Used for the episode:  0.018882440689853716\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -31251.0 || average_reward: -86.56786703601108\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  458\n",
      "Epsilon Used for the episode:  0.0187912955154581\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -29632.0 || average_reward: -82.08310249307479\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  459\n",
      "Epsilon Used for the episode:  0.018701085606841986\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -30475.0 || average_reward: -84.41828254847645\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  460\n",
      "Epsilon Used for the episode:  0.018611801366985176\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -29098.0 || average_reward: -80.6038781163435\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  461\n",
      "Epsilon Used for the episode:  0.018523433297345175\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -29078.0 || average_reward: -80.54847645429363\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  462\n",
      "Epsilon Used for the episode:  0.018435971996846615\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -29594.0 || average_reward: -81.97783933518005\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  463\n",
      "Epsilon Used for the episode:  0.018349408160881202\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -28843.0 || average_reward: -79.89750692520775\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  464\n",
      "Epsilon Used for the episode:  0.018263732580317743\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -28058.0 || average_reward: -77.7229916897507\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  465\n",
      "Epsilon Used for the episode:  0.01817893614052253\n",
      "EPISODE COMPLETE\n",
      "min_reward: -176.0 || max_reward: 0.0 || total_reward: -30012.0 || average_reward: -83.13573407202216\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  466\n",
      "Epsilon Used for the episode:  0.018095009820389592\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -29267.0 || average_reward: -81.07202216066482\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  467\n",
      "Epsilon Used for the episode:  0.01801194469138103\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -29998.0 || average_reward: -83.09695290858726\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  468\n",
      "Epsilon Used for the episode:  0.01792973191657713\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -28828.0 || average_reward: -79.85595567867036\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  469\n",
      "Epsilon Used for the episode:  0.017848362749736266\n",
      "EPISODE COMPLETE\n",
      "min_reward: -190.0 || max_reward: 0.0 || total_reward: -28776.0 || average_reward: -79.71191135734072\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  470\n",
      "Epsilon Used for the episode:  0.017767828534364368\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -30192.0 || average_reward: -83.63434903047091\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  471\n",
      "Epsilon Used for the episode:  0.017688120702794095\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -27139.0 || average_reward: -75.17728531855956\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  472\n",
      "Epsilon Used for the episode:  0.017609230775273255\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -25343.0 || average_reward: -70.202216066482\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  473\n",
      "Epsilon Used for the episode:  0.017531150359062775\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -25139.0 || average_reward: -69.6371191135734\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  474\n",
      "Epsilon Used for the episode:  0.01745387114754375\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -26049.0 || average_reward: -72.15789473684211\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  475\n",
      "Epsilon Used for the episode:  0.01737738491933383\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -25615.0 || average_reward: -70.95567867036011\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  476\n",
      "Epsilon Used for the episode:  0.01730168353741249\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -25174.0 || average_reward: -69.73407202216066\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  477\n",
      "Epsilon Used for the episode:  0.01722675894825546\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -26531.0 || average_reward: -73.49307479224376\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  478\n",
      "Epsilon Used for the episode:  0.017152603180977867\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -25770.0 || average_reward: -71.38504155124653\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  479\n",
      "Epsilon Used for the episode:  0.017079208346486317\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -25602.0 || average_reward: -70.9196675900277\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  480\n",
      "Epsilon Used for the episode:  0.01700656663663956\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -29324.0 || average_reward: -81.22991689750693\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  481\n",
      "Epsilon Used for the episode:  0.016934670323417854\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -27467.0 || average_reward: -76.08587257617728\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  482\n",
      "Epsilon Used for the episode:  0.016863511758100778\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -27664.0 || average_reward: -76.63157894736842\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  483\n",
      "Epsilon Used for the episode:  0.016793083370453564\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -27687.0 || average_reward: -76.69529085872576\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  484\n",
      "Epsilon Used for the episode:  0.016723377667921695\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -26610.0 || average_reward: -73.71191135734072\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  485\n",
      "Epsilon Used for the episode:  0.016654387234833833\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -27168.0 || average_reward: -75.25761772853186\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  486\n",
      "Epsilon Used for the episode:  0.016586104731612875\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -27705.0 || average_reward: -76.74515235457064\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  487\n",
      "Epsilon Used for the episode:  0.01651852289399517\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -28382.0 || average_reward: -78.62049861495845\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  488\n",
      "Epsilon Used for the episode:  0.01645163453225765\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -28052.0 || average_reward: -77.70637119113573\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  489\n",
      "Epsilon Used for the episode:  0.016385432530453006\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -27795.0 || average_reward: -76.99445983379502\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  490\n",
      "Epsilon Used for the episode:  0.016319909845652604\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -28027.0 || average_reward: -77.6371191135734\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  491\n",
      "Epsilon Used for the episode:  0.01625505950719726\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -26826.0 || average_reward: -74.31024930747922\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  492\n",
      "Epsilon Used for the episode:  0.016190874615955635\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -26843.0 || average_reward: -74.3573407202216\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  493\n",
      "Epsilon Used for the episode:  0.016127348343590298\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -26414.0 || average_reward: -73.16897506925208\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  494\n",
      "Epsilon Used for the episode:  0.016064473931831247\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -27574.0 || average_reward: -76.38227146814404\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  495\n",
      "Epsilon Used for the episode:  0.016002244691756977\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -28112.0 || average_reward: -77.87257617728532\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  496\n",
      "Epsilon Used for the episode:  0.01594065400308283\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -28020.0 || average_reward: -77.61772853185596\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  497\n",
      "Epsilon Used for the episode:  0.01587969531345674\n",
      "EPISODE COMPLETE\n",
      "min_reward: -168.0 || max_reward: 0.0 || total_reward: -28007.0 || average_reward: -77.58171745152355\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  498\n",
      "Epsilon Used for the episode:  0.015819362137762107\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -28131.0 || average_reward: -77.92520775623268\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  499\n",
      "Epsilon Used for the episode:  0.015759648057427928\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -27137.0 || average_reward: -75.17174515235457\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "episodeRewards = [] # to store the episode rewards for logging/plotting purposes\n",
    "stepsDone = 0\n",
    "# creating a loop for training\n",
    "for ep in range(PARAM_episodes):\n",
    "    print('###-'* 30)\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Epsilon Used for the episode: \", INUSE_epsilon)\n",
    "    # reset the environment\n",
    "    state = env.reset()\n",
    "    epReward = []\n",
    "    done = False\n",
    "\n",
    "    # training in the episode\n",
    "    while not done:\n",
    "        # if stepsDone%250==0:\n",
    "        #     print(\"250 Steps done!, \", stepsDone)\n",
    "        # recording the previous action\n",
    "        # recording previous action the previous TL phase\n",
    "        # print(\"stepsDone: \", stepsDone)\n",
    "        # if stepsDone==0:\n",
    "        #     prevAction = 0\n",
    "        # else:\n",
    "        #     prevAction = action\n",
    "        # print(\"Prev action: \", prevAction)\n",
    "        # select an action and perform it\n",
    "        action = selectAction(state, INUSE_epsilon)\n",
    "        stepsDone+=1\n",
    "        # print(\"ACTION SELECTED: \", action)\n",
    "        nextSt, r, done = env.takeAction(action)\n",
    "        # print(\"nextSt: \", nextSt)\n",
    "        # debug\n",
    "        # if env.numSteps == 10800:\n",
    "        #     print(\"\\n!!!10800 steps done!!!\\n\")\n",
    "\n",
    "        # store current state, action, reward, next state, done in replay memory\n",
    "        memory.insert(state, action, r, nextSt, done)\n",
    "\n",
    "        # update current state to next state\n",
    "        state = nextSt\n",
    "        epReward.append(r)\n",
    "\n",
    "        # update model parameters\n",
    "        # if memory does not have enough tuples to sample (batch_size), we cant sample so we continue the loop\n",
    "        # print('Memory size: ', memory.size())\n",
    "        if memory.size() < PARAM_batch_size:\n",
    "            continue\n",
    "        # else, we train the networks\n",
    "        # take a sample\n",
    "        stBatch, aBatch, rBatch, nstBatch, doneBatch = memory.sample(PARAM_batch_size)\n",
    "        stBatch = torch.tensor(stBatch, device=PARAM_device, dtype=torch.float32)\n",
    "        aBatch = torch.tensor(aBatch, device=PARAM_device).unsqueeze(1)\n",
    "        rBatch = torch.tensor(rBatch, device=PARAM_device, dtype=torch.float32)\n",
    "        nstBatch = torch.tensor(nstBatch, device=PARAM_device, dtype=torch.float32)\n",
    "        # getting Q values for current states\n",
    "        QVals = policyNet(stBatch).gather(1, aBatch)\n",
    "\n",
    "        # Updating q values using target network\n",
    "        with torch.no_grad():\n",
    "            # next state q values using target network\n",
    "            nextQVals = targetNet(nstBatch).max(1)[0]\n",
    "            # updating the q values for policy network\n",
    "            targetQVals = rBatch + PARAM_gamma * nextQVals\n",
    "\n",
    "        # calculating the loss\n",
    "        lossFunction = nn.SmoothL1Loss()\n",
    "        loss = lossFunction(QVals, targetQVals.unsqueeze(1))\n",
    "\n",
    "        # updating model weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # updaing the target network periodically\n",
    "        if ep%PARAM_target_update_freq==0:\n",
    "            # targetNetStateDict = targetNet.state_dict()\n",
    "            # policyNetStateDict = policyNet.state_dict()\n",
    "            # for key in policyNetStateDict:\n",
    "            #     targetNetStateDict[key] = policyNetStateDict*PARAM_tau\n",
    "            targetNet.load_state_dict(policyNet.state_dict())\n",
    "\n",
    "\n",
    "    # decay epsilon after episode is complete\n",
    "    INUSE_epsilon = PARAM_epsilon_min + (PARAM_epsilon - PARAM_epsilon_min) * math.exp(-1. * stepsDone/PARAM_epsilon_decay)\n",
    "    episodeRewards.append(epReward)\n",
    "\n",
    "    # printing training stats\n",
    "    print(\"EPISODE COMPLETE\")\n",
    "    print(f\"min_reward: {min(epReward)} || max_reward: {max(epReward)} || total_reward: {sum(epReward)} || average_reward: {np.mean(epReward)}\")\n",
    "\n",
    "\n",
    "# saving the models\n",
    "# need to save everything\n",
    "import pickle\n",
    "# saving the models\n",
    "torch.save(targetNet, \"models/TargetNet_SUMOv1.pth\")\n",
    "torch.save(policyNet, \"models/PolicyNet_SUMOv1.pth\")\n",
    "\n",
    "# saving the lists\n",
    "with open('models/rewardList_SUMOv1.pkl', 'wb') as f:\n",
    "    pickle.dump(episodeRewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions with the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained models\n",
    "import pickle\n",
    "policyNet = torch.load('models/PolicyNet_SUMOv1.pth', weights_only=False)\n",
    "targetNet = torch.load('models/TargetNet_SUMOv1.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "QVals:  tensor([[-21.6330, -13.5150, -16.0006, -20.7427]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  1\n",
      "QVals:  tensor([[-19.5598, -14.6083, -11.1939, -16.2256]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  2\n",
      "QVals:  tensor([[-11.6953, -13.9393, -19.6652, -17.5573]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  3\n",
      "QVals:  tensor([[-21.6330, -13.5150, -16.0006, -20.7427]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  4\n",
      "QVals:  tensor([[-23.6308, -22.9434, -24.6676, -16.8830]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  5\n",
      "QVals:  tensor([[-24.1591, -27.1343, -16.8294, -35.2210]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  6\n",
      "QVals:  tensor([[-30.4071, -29.5124, -26.8509, -17.6017]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  7\n",
      "QVals:  tensor([[-18.7001, -26.6221, -25.6950, -24.8646]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  8\n",
      "QVals:  tensor([[-21.5560, -21.3047, -33.2734, -26.5313]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  9\n",
      "QVals:  tensor([[-25.1279, -29.2724, -36.0723, -23.9046]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  10\n",
      "QVals:  tensor([[-24.1407, -29.2346, -25.3350, -28.8199]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  11\n",
      "QVals:  tensor([[-33.5073, -24.9624, -22.3828, -25.3924]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  12\n",
      "QVals:  tensor([[-46.9811, -37.7437, -38.9066, -32.3735]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  13\n",
      "QVals:  tensor([[-33.9013, -23.5562, -42.1175, -32.0095]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  14\n",
      "QVals:  tensor([[-17.7297, -27.4516, -47.3411, -37.8192]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  15\n",
      "QVals:  tensor([[-39.5245, -30.1008, -26.9581, -33.2920]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  16\n",
      "QVals:  tensor([[-28.5381, -39.6919, -46.5542, -41.2356]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  17\n",
      "QVals:  tensor([[-35.8787, -30.5736, -36.0723, -24.5430]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  18\n",
      "QVals:  tensor([[-32.8135, -32.1254, -30.2402, -31.1932]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  19\n",
      "QVals:  tensor([[-40.2019, -39.7894, -39.2245, -39.6010]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  20\n",
      "QVals:  tensor([[-46.9049, -42.1418, -43.9550, -44.2025]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  21\n",
      "QVals:  tensor([[-49.7940, -45.4136, -64.7494, -47.6692]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  22\n",
      "QVals:  tensor([[-42.9947, -51.2350, -49.0172, -42.5491]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  23\n",
      "QVals:  tensor([[-39.0762, -49.6297, -34.1763, -56.2217]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  24\n",
      "QVals:  tensor([[-34.7810, -44.0035, -65.8418, -57.5958]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  25\n",
      "QVals:  tensor([[-49.9475, -35.9113, -52.9405, -40.4232]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  26\n",
      "QVals:  tensor([[-63.4384, -57.0179, -41.9715, -37.4290]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  27\n",
      "QVals:  tensor([[-39.2179, -36.0356, -35.3007, -47.0988]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  28\n",
      "QVals:  tensor([[-38.7587, -39.9554, -47.7222, -43.1011]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  29\n",
      "QVals:  tensor([[-53.8630, -40.8607, -48.2452, -42.8437]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  30\n",
      "QVals:  tensor([[-40.9829, -44.2420, -33.4192, -43.5654]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  31\n",
      "QVals:  tensor([[-32.6513, -38.5041, -35.0750, -26.9481]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  32\n",
      "QVals:  tensor([[-32.6084, -38.9797, -32.4363, -38.7309]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  33\n",
      "QVals:  tensor([[-22.1135, -31.5501, -27.9889, -39.3035]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  34\n",
      "QVals:  tensor([[-35.3281, -27.1870, -33.6905, -28.0191]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  35\n",
      "QVals:  tensor([[-32.2686, -36.8764, -27.0978, -26.0463]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  36\n",
      "QVals:  tensor([[-34.8326, -45.3734, -34.8545, -40.4597]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  37\n",
      "QVals:  tensor([[-37.5480, -29.5612, -28.3466, -30.9507]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  38\n",
      "QVals:  tensor([[-44.0281, -33.8237, -36.3822, -32.1852]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  39\n",
      "QVals:  tensor([[-36.1645, -34.8384, -33.5312, -44.8577]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  40\n",
      "QVals:  tensor([[-30.8337, -37.7203, -35.3459, -38.9545]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  41\n",
      "QVals:  tensor([[-46.5410, -36.5609, -43.9348, -33.2460]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  42\n",
      "QVals:  tensor([[-37.2943, -34.9878, -42.7242, -42.0033]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  43\n",
      "QVals:  tensor([[-30.9321, -33.7981, -30.2617, -27.8687]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  44\n",
      "QVals:  tensor([[-32.3387, -33.4938, -28.3334, -42.2530]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  45\n",
      "QVals:  tensor([[-21.8669, -33.9083, -31.7017, -37.2231]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  46\n",
      "QVals:  tensor([[-31.8282, -21.0412, -22.0812, -24.5689]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  47\n",
      "QVals:  tensor([[-33.8611, -30.2279, -26.5914, -23.2387]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  48\n",
      "QVals:  tensor([[-26.8373, -25.8780, -22.0259, -30.9545]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  49\n",
      "QVals:  tensor([[-24.3551, -23.8109, -25.5481, -29.5704]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  50\n",
      "QVals:  tensor([[-19.5598, -14.6083, -11.1939, -16.2256]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  51\n",
      "QVals:  tensor([[-44.1329, -36.3636, -36.6015, -33.1476]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  52\n",
      "QVals:  tensor([[-37.2943, -34.9878, -42.7242, -42.0033]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  53\n",
      "QVals:  tensor([[-37.1177, -46.1605, -35.9600, -41.0714]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  54\n",
      "QVals:  tensor([[-37.0748, -43.4073, -47.9607, -38.1150]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  55\n",
      "QVals:  tensor([[-56.5754, -36.9049, -39.6024, -32.2775]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  56\n",
      "QVals:  tensor([[-39.5038, -22.0903, -55.0150, -41.0784]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  57\n",
      "QVals:  tensor([[-23.4284, -28.9729, -20.4167, -26.9581]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  58\n",
      "QVals:  tensor([[-28.5062, -28.1897, -26.4760, -19.3916]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  59\n",
      "QVals:  tensor([[-20.3051, -36.3355, -31.4302, -26.6224]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  60\n",
      "QVals:  tensor([[-34.3197, -30.1497, -40.3391, -22.5269]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  61\n",
      "QVals:  tensor([[-27.8172, -29.4683, -27.7108, -28.1233]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  62\n",
      "QVals:  tensor([[-26.2354, -35.8289, -32.3573, -34.7184]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  63\n",
      "QVals:  tensor([[-27.4876, -24.5631, -33.4096, -27.1533]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  64\n",
      "QVals:  tensor([[-35.0372, -37.6709, -43.4940, -23.9898]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  65\n",
      "QVals:  tensor([[-27.8083, -33.0221, -30.8659, -26.0739]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  66\n",
      "QVals:  tensor([[-29.5402, -36.6786, -37.9889, -34.7051]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  67\n",
      "QVals:  tensor([[-53.2220, -38.0342, -41.6002, -38.9827]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  68\n",
      "QVals:  tensor([[-45.2714, -42.1897, -41.8288, -27.7964]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  69\n",
      "QVals:  tensor([[-31.8137, -33.0659, -26.4041, -38.3910]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  70\n",
      "QVals:  tensor([[-30.0741, -35.9493, -31.3053, -22.1879]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  71\n",
      "QVals:  tensor([[-33.0063, -36.2140, -33.0884, -41.8589]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  72\n",
      "QVals:  tensor([[-42.6450, -36.0670, -32.7929, -29.2594]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  73\n",
      "QVals:  tensor([[-35.7755, -28.5237, -32.6166, -37.5502]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  74\n",
      "QVals:  tensor([[-33.8145, -36.8404, -30.4376, -40.6370]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  75\n",
      "QVals:  tensor([[-47.2131, -45.8094, -38.7876, -43.0157]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  76\n",
      "QVals:  tensor([[-53.7702, -43.8168, -54.8209, -37.7133]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  77\n",
      "QVals:  tensor([[-35.5108, -41.8713, -39.7350, -43.1383]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  78\n",
      "QVals:  tensor([[-54.8042, -37.8004, -37.9664, -52.8303]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  79\n",
      "QVals:  tensor([[-50.9672, -46.1549, -37.4968, -48.9933]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  80\n",
      "QVals:  tensor([[-43.1280, -38.1987, -42.9088, -34.7858]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  81\n",
      "QVals:  tensor([[-38.2025, -49.2613, -39.8040, -42.7054]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  82\n",
      "QVals:  tensor([[-48.1739, -43.3087, -39.1420, -40.9420]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  83\n",
      "QVals:  tensor([[-57.4428, -42.4571, -46.7994, -38.7324]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  84\n",
      "QVals:  tensor([[-40.7372, -43.7018, -36.3081, -44.9879]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  85\n",
      "QVals:  tensor([[-35.9505, -31.9975, -41.8664, -32.7400]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  86\n",
      "QVals:  tensor([[-29.3133, -49.8045, -67.2806, -34.3450]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  87\n",
      "QVals:  tensor([[-39.6261, -41.8610, -45.2291, -34.7236]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  88\n",
      "QVals:  tensor([[-41.5643, -28.7489, -44.6487, -45.3268]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  89\n",
      "QVals:  tensor([[-45.7259, -37.0147, -32.8538, -38.4192]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  90\n",
      "QVals:  tensor([[-37.4253, -38.6180, -34.3294, -32.0315]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  91\n",
      "QVals:  tensor([[-35.1773, -34.9611, -35.5891, -37.3089]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  92\n",
      "QVals:  tensor([[-32.5924, -38.8903, -32.2109, -25.7353]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  93\n",
      "QVals:  tensor([[-33.4435, -44.7713, -33.3063, -40.6809]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  94\n",
      "QVals:  tensor([[-115.0962, -125.6849, -160.2675, -132.8521]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  95\n",
      "QVals:  tensor([[ -97.4017,  -95.8349, -132.0096, -131.3325]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  96\n",
      "QVals:  tensor([[-106.9540, -146.7337, -183.8593, -143.3614]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  97\n",
      "QVals:  tensor([[-140.7689, -125.5341, -164.0784, -158.9207]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  98\n",
      "QVals:  tensor([[-128.5136, -156.1148, -169.9681, -162.6881]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  99\n",
      "QVals:  tensor([[-141.1241, -129.0392, -167.5392, -171.5692]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  100\n",
      "QVals:  tensor([[-131.2207, -167.3634, -182.0775, -163.0643]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  101\n",
      "QVals:  tensor([[-153.1089, -147.3231, -181.2660, -178.3307]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  102\n",
      "QVals:  tensor([[-159.7494, -193.4586, -197.0918, -186.0057]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  103\n",
      "QVals:  tensor([[-174.2871, -170.2023, -189.2300, -175.6679]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  104\n",
      "QVals:  tensor([[-175.4460, -211.3142, -210.4914, -201.0064]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  105\n",
      "QVals:  tensor([[-178.8980, -177.7669, -193.8025, -185.1558]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  106\n",
      "QVals:  tensor([[-175.8493, -211.0066, -204.5283, -200.5724]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  107\n",
      "QVals:  tensor([[-191.0042, -186.0323, -198.4135, -187.4908]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  108\n",
      "QVals:  tensor([[-197.7879, -239.3128, -231.5918, -219.4821]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  109\n",
      "QVals:  tensor([[-227.2971, -213.8602, -228.1060, -194.0771]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  110\n",
      "QVals:  tensor([[-209.9623, -210.3911, -235.6181, -254.0300]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  111\n",
      "QVals:  tensor([[-242.0381, -212.7494, -244.5855, -263.9749]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  112\n",
      "QVals:  tensor([[-219.2369, -240.6913, -245.3404, -242.7914]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  113\n",
      "QVals:  tensor([[-247.0799, -220.7757, -260.4697, -274.1258]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  114\n",
      "QVals:  tensor([[-228.7852, -248.9644, -259.9836, -256.5762]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  115\n",
      "QVals:  tensor([[-236.8936, -227.0339, -244.8578, -246.2293]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  116\n",
      "QVals:  tensor([[-243.2988, -263.6649, -273.8027, -272.0383]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  117\n",
      "QVals:  tensor([[-254.8295, -249.9118, -268.9503, -267.1332]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  118\n",
      "QVals:  tensor([[-276.1352, -304.2202, -290.8063, -293.9702]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  119\n",
      "QVals:  tensor([[-303.7035, -300.3438, -299.5255, -296.4931]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  120\n",
      "QVals:  tensor([[-316.0432, -311.5015, -328.4694, -353.9727]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  121\n",
      "QVals:  tensor([[-317.8425, -340.0126, -351.5403, -355.5909]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  122\n",
      "QVals:  tensor([[-322.4187, -309.7494, -332.0694, -343.5129]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  123\n",
      "QVals:  tensor([[-298.0259, -317.4161, -310.8934, -318.2490]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  124\n",
      "QVals:  tensor([[-326.7298, -310.8367, -322.2775, -329.5995]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  125\n",
      "QVals:  tensor([[-321.1225, -342.3786, -349.1555, -348.3914]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  126\n",
      "QVals:  tensor([[-322.5612, -310.4444, -353.3213, -328.9146]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  127\n",
      "QVals:  tensor([[-308.8128, -326.9512, -347.3328, -324.7624]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  128\n",
      "QVals:  tensor([[-342.5029, -338.0222, -352.1805, -324.4886]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  129\n",
      "QVals:  tensor([[-385.9904, -370.4189, -401.1486, -416.1740]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  130\n",
      "QVals:  tensor([[-385.8213, -419.2916, -431.9312, -427.2314]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  131\n",
      "QVals:  tensor([[-411.3169, -392.0974, -407.7688, -427.9061]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  132\n",
      "QVals:  tensor([[-372.3865, -395.5961, -380.6809, -411.3889]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  133\n",
      "QVals:  tensor([[-384.7497, -365.8800, -366.1639, -398.7770]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  134\n",
      "QVals:  tensor([[-369.5228, -396.0062, -368.8997, -393.2433]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  135\n",
      "QVals:  tensor([[-375.9197, -399.2269, -422.7865, -430.3484]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  136\n",
      "QVals:  tensor([[-411.9417, -393.0962, -433.1257, -425.9149]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  137\n",
      "QVals:  tensor([[-370.2166, -390.7098, -404.9977, -417.1122]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  138\n",
      "QVals:  tensor([[-376.9355, -373.4767, -393.3631, -397.9210]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  139\n",
      "QVals:  tensor([[-365.8463, -391.6861, -400.7417, -410.4731]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  140\n",
      "QVals:  tensor([[-394.3850, -382.0255, -402.2013, -404.6198]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  141\n",
      "QVals:  tensor([[-378.4660, -397.0892, -386.7907, -403.4695]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  142\n",
      "QVals:  tensor([[-424.1777, -388.6000, -409.8305, -432.8776]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  143\n",
      "QVals:  tensor([[-397.0543, -415.3954, -399.3423, -424.7745]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  144\n",
      "QVals:  tensor([[-436.9707, -396.6289, -414.1650, -442.7104]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  145\n",
      "QVals:  tensor([[-404.4490, -423.0032, -401.2611, -425.7069]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  146\n",
      "QVals:  tensor([[-433.9601, -453.3950, -462.5235, -483.1360]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  147\n",
      "QVals:  tensor([[-478.1073, -463.3621, -494.4632, -479.9455]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  148\n",
      "QVals:  tensor([[-440.4384, -464.2563, -476.1965, -477.3477]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  149\n",
      "QVals:  tensor([[-433.4378, -425.6854, -463.6141, -458.3480]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  150\n",
      "QVals:  tensor([[-412.4209, -434.3308, -447.8488, -431.0513]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  151\n",
      "QVals:  tensor([[-436.1676, -421.8420, -441.5186, -432.1915]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  152\n",
      "QVals:  tensor([[-426.8241, -446.0863, -436.4533, -435.0509]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  153\n",
      "QVals:  tensor([[-446.4104, -430.9975, -440.6334, -437.1195]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  154\n",
      "QVals:  tensor([[-443.6000, -462.4957, -460.0539, -444.5840]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  155\n",
      "QVals:  tensor([[-468.3098, -454.4623, -465.9355, -451.4930]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  156\n",
      "QVals:  tensor([[-476.2712, -456.8881, -489.6141, -484.4794]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  157\n",
      "QVals:  tensor([[-494.8894, -550.5228, -539.8298, -532.3877]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  158\n",
      "QVals:  tensor([[-519.2247, -503.1869, -537.6021, -534.4769]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  159\n",
      "QVals:  tensor([[-488.8155, -512.7679, -518.9771, -525.0146]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  160\n",
      "QVals:  tensor([[-511.7138, -490.8274, -487.0613, -512.0789]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  161\n",
      "QVals:  tensor([[-526.9575, -500.9220, -543.3157, -538.7069]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  162\n",
      "QVals:  tensor([[-485.9119, -508.5868, -519.1100, -524.0242]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  163\n",
      "QVals:  tensor([[-549.3021, -534.1431, -550.9539, -538.4840]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  164\n",
      "QVals:  tensor([[-489.4156, -506.9736, -516.2307, -519.4191]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  165\n",
      "QVals:  tensor([[-534.9106, -518.1209, -534.5868, -519.0114]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  166\n",
      "QVals:  tensor([[-507.8250, -527.3600, -519.0562, -507.4351]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  167\n",
      "QVals:  tensor([[-530.4651, -557.2313, -557.4905, -565.1616]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  168\n",
      "QVals:  tensor([[-561.6940, -543.9024, -566.3838, -570.1929]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  169\n",
      "QVals:  tensor([[-531.9159, -578.7440, -568.8420, -583.4238]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  170\n",
      "QVals:  tensor([[-550.9314, -533.8433, -545.8254, -565.5417]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  171\n",
      "QVals:  tensor([[-526.5536, -540.4316, -534.3771, -556.5005]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  172\n",
      "QVals:  tensor([[-567.4870, -546.3452, -543.9973, -560.1718]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  173\n",
      "QVals:  tensor([[-569.5663, -540.5586, -581.1390, -555.1818]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  174\n",
      "QVals:  tensor([[-550.3071, -572.3156, -573.5114, -569.7041]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  175\n",
      "QVals:  tensor([[-586.8488, -569.2646, -584.2004, -557.5483]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  176\n",
      "QVals:  tensor([[-561.7164, -572.3517, -560.5992, -606.3092]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  177\n",
      "QVals:  tensor([[-566.8824, -554.6799, -590.4402, -617.2693]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  178\n",
      "QVals:  tensor([[-616.8324, -621.1748, -626.7684, -627.6213]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  179\n",
      "QVals:  tensor([[-670.3088, -623.0004, -633.6797, -631.4692]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  180\n",
      "QVals:  tensor([[-607.1994, -629.7234, -616.0014, -623.7913]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  181\n",
      "QVals:  tensor([[-556.0663, -531.4119, -563.7671, -545.7020]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  182\n",
      "QVals:  tensor([[-500.5136, -525.3502, -522.2797, -518.8293]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  183\n",
      "QVals:  tensor([[-525.1335, -508.7813, -525.0841, -513.7311]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  184\n",
      "QVals:  tensor([[-509.9313, -539.7212, -531.2507, -521.3564]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  185\n",
      "QVals:  tensor([[-536.4841, -526.5450, -532.2336, -522.9865]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  186\n",
      "QVals:  tensor([[-540.3562, -518.9285, -546.9912, -559.9629]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  187\n",
      "QVals:  tensor([[-530.5514, -576.9196, -564.8161, -583.0166]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  188\n",
      "QVals:  tensor([[-569.3043, -546.0948, -570.1066, -577.1470]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  189\n",
      "QVals:  tensor([[-539.7240, -551.5629, -550.4728, -562.6146]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  190\n",
      "QVals:  tensor([[-551.7667, -528.4402, -538.5757, -543.7982]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  191\n",
      "QVals:  tensor([[-537.7292, -563.5836, -544.5764, -553.4633]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  192\n",
      "QVals:  tensor([[-560.7901, -541.0632, -543.3039, -546.5673]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  193\n",
      "QVals:  tensor([[-539.9959, -567.4940, -543.0981, -557.5924]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  194\n",
      "QVals:  tensor([[-580.4713, -564.9056, -558.8202, -568.5186]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  195\n",
      "QVals:  tensor([[-594.6089, -565.9651, -604.2237, -572.7681]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  196\n",
      "QVals:  tensor([[-560.1626, -576.3408, -579.5580, -577.8923]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  197\n",
      "QVals:  tensor([[-591.3658, -573.5331, -586.3448, -571.0010]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  198\n",
      "QVals:  tensor([[-601.5239, -606.0496, -595.2035, -641.7732]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  199\n",
      "QVals:  tensor([[-589.3541, -605.8892, -607.7932, -629.4142]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  200\n",
      "QVals:  tensor([[-636.5948, -611.5264, -613.3413, -632.9771]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  201\n",
      "QVals:  tensor([[-620.4805, -646.1262, -644.0823, -657.4708]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  202\n",
      "QVals:  tensor([[-661.1624, -622.2314, -647.3928, -616.1854]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  203\n",
      "QVals:  tensor([[-631.8441, -600.6263, -625.6284, -647.8553]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  204\n",
      "QVals:  tensor([[-593.1632, -636.9468, -589.4169, -645.5781]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  205\n",
      "QVals:  tensor([[-584.3410, -606.0227, -626.3869, -617.5129]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  206\n",
      "QVals:  tensor([[-623.7570, -619.5635, -608.6358, -607.7828]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  207\n",
      "QVals:  tensor([[-630.6080, -601.8449, -627.0452, -670.3908]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  208\n",
      "QVals:  tensor([[-644.1609, -660.9105, -636.1628, -654.1393]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  209\n",
      "QVals:  tensor([[-622.0954, -599.4319, -629.4126, -635.3790]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  210\n",
      "QVals:  tensor([[-616.9333, -642.6719, -626.3922, -608.8719]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  211\n",
      "QVals:  tensor([[-621.0630, -632.2198, -616.1515, -669.3158]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  212\n",
      "QVals:  tensor([[-606.5974, -629.2695, -619.4191, -637.1530]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  213\n",
      "QVals:  tensor([[-676.3433, -607.4814, -644.8679, -612.9402]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  214\n",
      "QVals:  tensor([[-622.5778, -643.9203, -637.5818, -608.7954]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  215\n",
      "QVals:  tensor([[-606.0243, -591.1325, -617.3493, -645.3610]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  216\n",
      "QVals:  tensor([[-543.5192, -585.0466, -591.4280, -572.3829]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  217\n",
      "QVals:  tensor([[-536.7804, -528.8542, -575.0826, -556.1232]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  218\n",
      "QVals:  tensor([[-527.9871, -558.2995, -557.9095, -562.5479]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  219\n",
      "QVals:  tensor([[-527.2340, -512.3940, -524.3824, -520.1194]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  220\n",
      "QVals:  tensor([[-488.2394, -509.7582, -495.4769, -499.5811]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  221\n",
      "QVals:  tensor([[-530.7853, -514.1492, -513.4230, -520.4401]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  222\n",
      "QVals:  tensor([[-568.4741, -538.5833, -580.4744, -560.0709]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  223\n",
      "QVals:  tensor([[-553.7949, -574.8928, -577.5117, -563.6227]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  224\n",
      "QVals:  tensor([[-590.1189, -570.8301, -590.2028, -561.3195]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  225\n",
      "QVals:  tensor([[-582.4789, -586.6057, -578.1083, -621.7347]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  226\n",
      "QVals:  tensor([[-591.9504, -566.9282, -609.0467, -634.0891]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  227\n",
      "QVals:  tensor([[-625.9556, -616.5212, -640.5806, -596.7236]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  228\n",
      "QVals:  tensor([[-629.3842, -622.2409, -644.5978, -674.0312]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  229\n",
      "QVals:  tensor([[-616.9186, -644.6373, -606.1310, -618.3784]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  230\n",
      "QVals:  tensor([[-592.5447, -610.2227, -600.1425, -619.8662]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  231\n",
      "QVals:  tensor([[-672.9094, -598.9221, -635.8129, -628.3287]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  232\n",
      "QVals:  tensor([[-587.5977, -609.0222, -607.0286, -601.3057]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  233\n",
      "QVals:  tensor([[-565.3279, -543.7756, -552.4431, -561.6770]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  234\n",
      "QVals:  tensor([[-474.7228, -495.1921, -496.5433, -507.9554]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  235\n",
      "QVals:  tensor([[-515.3903, -500.6511, -508.3139, -510.1068]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  236\n",
      "QVals:  tensor([[-485.9484, -505.1852, -493.1790, -505.5877]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  237\n",
      "QVals:  tensor([[-504.6257, -495.6578, -492.9907, -498.1397]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  238\n",
      "QVals:  tensor([[-536.0865, -511.5494, -552.3961, -533.6767]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  239\n",
      "QVals:  tensor([[-517.3138, -542.1275, -548.9016, -529.3606]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  240\n",
      "QVals:  tensor([[-555.3682, -536.2891, -558.6183, -533.4830]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  241\n",
      "QVals:  tensor([[-554.7728, -553.2421, -548.9739, -593.9651]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  242\n",
      "QVals:  tensor([[-597.8557, -591.7607, -618.9289, -616.8844]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  243\n",
      "QVals:  tensor([[-591.4095, -590.0110, -602.5565, -570.3105]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  244\n",
      "QVals:  tensor([[-623.5427, -606.0828, -632.8867, -666.1559]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  245\n",
      "QVals:  tensor([[-647.5665, -664.7596, -628.8450, -640.1707]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  246\n",
      "QVals:  tensor([[-627.1060, -640.6726, -641.2128, -656.5749]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  247\n",
      "QVals:  tensor([[-679.1112, -603.1279, -650.6226, -635.1926]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  248\n",
      "QVals:  tensor([[-595.5618, -616.8525, -605.7109, -607.7224]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  249\n",
      "QVals:  tensor([[-593.1934, -586.6395, -580.8896, -578.8171]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  250\n",
      "QVals:  tensor([[-564.6105, -554.0523, -554.2909, -592.3906]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  251\n",
      "QVals:  tensor([[-516.2855, -545.7128, -529.1324, -533.9783]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  252\n",
      "QVals:  tensor([[-541.6656, -523.6934, -544.0195, -543.3413]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  253\n",
      "QVals:  tensor([[-514.0731, -529.9819, -534.7355, -540.2120]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  254\n",
      "QVals:  tensor([[-525.9844, -505.2220, -508.9504, -518.3529]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  255\n",
      "QVals:  tensor([[-476.8322, -506.7700, -485.9852, -496.8575]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  256\n",
      "QVals:  tensor([[-503.1044, -494.7211, -493.4991, -498.4150]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  257\n",
      "QVals:  tensor([[-575.4572, -554.8284, -588.6414, -569.7381]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  258\n",
      "QVals:  tensor([[-568.7706, -588.6519, -597.1567, -564.7847]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  259\n",
      "QVals:  tensor([[-578.4532, -571.6457, -589.2357, -593.4933]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  260\n",
      "QVals:  tensor([[-626.5739, -648.5620, -617.3329, -626.3814]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  261\n",
      "QVals:  tensor([[-589.0123, -601.2284, -603.9886, -616.0613]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  262\n",
      "QVals:  tensor([[-682.1239, -609.0894, -654.8976, -639.2091]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  263\n",
      "QVals:  tensor([[-590.0654, -617.0872, -607.7707, -614.4865]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  264\n",
      "QVals:  tensor([[-615.1956, -592.5082, -619.2042, -588.3046]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  265\n",
      "QVals:  tensor([[-552.4359, -555.6985, -575.1018, -597.6897]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  266\n",
      "QVals:  tensor([[-547.1432, -504.6994, -581.3298, -574.3678]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  267\n",
      "QVals:  tensor([[-474.6896, -519.7646, -559.6712, -557.0367]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  268\n",
      "QVals:  tensor([[-553.7755, -520.0145, -555.1078, -547.5270]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  269\n",
      "QVals:  tensor([[-560.1472, -585.6761, -585.3646, -573.8613]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  270\n",
      "QVals:  tensor([[-563.3971, -548.4045, -559.5336, -546.9005]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  271\n",
      "QVals:  tensor([[-555.9709, -556.8232, -561.9587, -598.5372]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  272\n",
      "QVals:  tensor([[-626.4236, -570.2372, -609.6973, -628.8387]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  273\n",
      "QVals:  tensor([[-574.1182, -610.7191, -604.7592, -624.6021]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  274\n",
      "QVals:  tensor([[-637.7870, -592.9312, -617.4828, -621.5201]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  275\n",
      "QVals:  tensor([[-560.2593, -595.6309, -573.0991, -597.1558]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  276\n",
      "QVals:  tensor([[-584.9097, -568.1967, -568.9330, -576.0139]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  277\n",
      "QVals:  tensor([[-566.1527, -604.3311, -567.9323, -573.2518]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  278\n",
      "QVals:  tensor([[-599.1152, -587.3004, -589.3321, -584.9221]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  279\n",
      "QVals:  tensor([[-593.8309, -592.0853, -580.8580, -633.3615]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  280\n",
      "QVals:  tensor([[-594.7531, -608.3394, -612.8060, -642.0856]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  281\n",
      "QVals:  tensor([[-682.8347, -645.1622, -644.6381, -661.3510]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  282\n",
      "QVals:  tensor([[-668.2174, -663.4300, -710.5671, -658.5913]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  283\n",
      "QVals:  tensor([[-688.9132, -671.0975, -788.5368, -729.6536]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  284\n",
      "QVals:  tensor([[-671.2699, -660.6859, -671.8427, -649.3913]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  285\n",
      "QVals:  tensor([[-627.4789, -649.1760, -636.8662, -690.5690]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  286\n",
      "QVals:  tensor([[-693.1460, -650.6677, -654.8130, -673.6079]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  287\n",
      "QVals:  tensor([[-622.5255, -660.6019, -639.2574, -622.5284]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  288\n",
      "QVals:  tensor([[-625.7869, -591.1191, -598.2529, -596.9448]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  289\n",
      "QVals:  tensor([[-567.1317, -572.2931, -553.2461, -596.1420]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  290\n",
      "QVals:  tensor([[-537.5115, -542.1840, -553.7814, -575.1768]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  291\n",
      "QVals:  tensor([[-565.8538, -535.7333, -570.7972, -546.0011]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  292\n",
      "QVals:  tensor([[-533.5291, -563.5295, -563.2637, -557.1004]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  293\n",
      "QVals:  tensor([[-557.1901, -537.5555, -564.0415, -545.6908]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  294\n",
      "QVals:  tensor([[-539.4025, -551.4971, -536.9027, -565.5464]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  295\n",
      "QVals:  tensor([[-533.0086, -551.6556, -562.2647, -558.6369]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  296\n",
      "QVals:  tensor([[-575.4269, -550.3115, -583.1322, -549.6021]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  297\n",
      "QVals:  tensor([[-567.1840, -569.2626, -575.7057, -602.9561]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  298\n",
      "QVals:  tensor([[-642.7587, -585.4152, -608.5212, -633.4570]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  299\n",
      "QVals:  tensor([[-626.0808, -640.4894, -636.5338, -655.0054]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  300\n",
      "QVals:  tensor([[-675.8024, -608.7206, -641.4791, -644.3738]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  301\n",
      "QVals:  tensor([[-559.9628, -588.6885, -567.1250, -598.8177]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  302\n",
      "QVals:  tensor([[-569.9761, -545.0898, -548.3441, -573.4807]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  303\n",
      "QVals:  tensor([[-560.8151, -588.1047, -555.6463, -577.8775]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  304\n",
      "QVals:  tensor([[-574.1664, -583.2493, -596.2496, -586.7404]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  305\n",
      "QVals:  tensor([[-596.1820, -567.0502, -599.5868, -567.5969]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  306\n",
      "QVals:  tensor([[-579.4747, -594.0596, -599.6317, -577.8270]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  307\n",
      "QVals:  tensor([[-585.4182, -558.2159, -595.5135, -604.8448]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  308\n",
      "QVals:  tensor([[-597.7039, -629.6832, -601.5182, -621.7509]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  309\n",
      "QVals:  tensor([[-647.9064, -630.0267, -625.9099, -637.0628]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  310\n",
      "QVals:  tensor([[-622.7343, -625.5158, -635.4753, -631.3848]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  311\n",
      "QVals:  tensor([[-642.8464, -597.7957, -642.9023, -602.8786]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  312\n",
      "QVals:  tensor([[-594.7042, -598.5722, -604.8811, -581.6600]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  313\n",
      "QVals:  tensor([[-592.5981, -590.4239, -588.8542, -624.9404]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  314\n",
      "QVals:  tensor([[-596.3229, -573.7726, -607.3916, -604.0795]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  315\n",
      "QVals:  tensor([[-597.7984, -614.1169, -597.1352, -587.0927]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  316\n",
      "QVals:  tensor([[-586.8380, -611.2442, -637.5688, -675.2299]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  317\n",
      "QVals:  tensor([[-676.7869, -640.5296, -623.9514, -665.7463]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  318\n",
      "QVals:  tensor([[-637.6919, -610.0826, -638.8963, -640.3483]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  319\n",
      "QVals:  tensor([[-609.2723, -618.8754, -616.7429, -581.1481]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  320\n",
      "QVals:  tensor([[-598.8870, -560.4902, -601.4915, -613.7046]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  321\n",
      "QVals:  tensor([[-582.7987, -615.4454, -571.4491, -585.7747]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  322\n",
      "QVals:  tensor([[-629.0754, -636.1373, -627.6537, -644.8942]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  323\n",
      "QVals:  tensor([[-596.0967, -604.8262, -618.4185, -611.0300]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  324\n",
      "QVals:  tensor([[-696.7002, -604.6386, -669.6994, -615.9255]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  325\n",
      "QVals:  tensor([[-633.3652, -645.6464, -640.3325, -600.4688]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  326\n",
      "QVals:  tensor([[-628.2576, -588.7425, -619.6473, -633.1123]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  327\n",
      "QVals:  tensor([[-581.3112, -604.1741, -575.4061, -588.6644]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  328\n",
      "QVals:  tensor([[-565.6240, -577.8300, -586.3746, -591.9873]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  329\n",
      "QVals:  tensor([[-608.7219, -542.9413, -582.0611, -564.0996]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  330\n",
      "QVals:  tensor([[-581.4886, -593.3391, -590.9478, -569.8477]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  331\n",
      "QVals:  tensor([[-579.6228, -553.7547, -577.3310, -613.8082]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  332\n",
      "QVals:  tensor([[-520.1050, -529.7900, -551.5361, -551.7726]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  333\n",
      "QVals:  tensor([[-530.4605, -515.2307, -553.3333, -544.0670]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  334\n",
      "QVals:  tensor([[-500.3961, -519.6010, -526.9372, -556.1964]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  335\n",
      "QVals:  tensor([[-465.1244, -459.9118, -473.6650, -486.4696]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  336\n",
      "QVals:  tensor([[-460.3927, -505.3796, -453.5114, -492.9290]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  337\n",
      "QVals:  tensor([[-477.6186, -492.4433, -505.1133, -474.9467]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  338\n",
      "QVals:  tensor([[-529.3231, -509.0956, -546.0753, -568.3315]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  339\n",
      "QVals:  tensor([[-542.1420, -570.2521, -542.3656, -546.6992]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  340\n",
      "QVals:  tensor([[-599.2858, -561.7333, -566.5515, -589.8945]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  341\n",
      "QVals:  tensor([[-512.5734, -538.5035, -518.0392, -547.3506]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  342\n",
      "QVals:  tensor([[-466.4193, -450.7262, -476.3676, -493.9830]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  343\n",
      "QVals:  tensor([[-450.5619, -491.9975, -454.9285, -486.6395]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  344\n",
      "QVals:  tensor([[-482.4554, -468.5483, -480.2261, -490.3136]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  345\n",
      "QVals:  tensor([[-473.0383, -516.5496, -473.5312, -506.4289]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  346\n",
      "QVals:  tensor([[-505.6598, -489.2838, -496.0143, -509.0701]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  347\n",
      "QVals:  tensor([[-492.5631, -542.3586, -490.5748, -513.7794]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  348\n",
      "QVals:  tensor([[-486.6936, -510.5691, -518.8091, -480.4829]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  349\n",
      "QVals:  tensor([[-528.4834, -512.0722, -560.8144, -573.0876]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  350\n",
      "QVals:  tensor([[-481.8636, -497.8854, -489.5794, -495.0135]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  351\n",
      "QVals:  tensor([[-526.7006, -490.5478, -491.2826, -510.8202]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  352\n",
      "QVals:  tensor([[-477.2796, -471.5222, -449.2376, -462.3346]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  353\n",
      "QVals:  tensor([[-474.2309, -456.8914, -474.2478, -433.2710]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  354\n",
      "QVals:  tensor([[-502.6429, -470.3591, -514.1686, -534.8405]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  355\n",
      "QVals:  tensor([[-489.1508, -497.0891, -491.5514, -497.5148]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  356\n",
      "QVals:  tensor([[-496.0107, -455.2131, -448.5776, -487.5221]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  357\n",
      "QVals:  tensor([[-485.4915, -453.3205, -511.0047, -459.4276]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  358\n",
      "QVals:  tensor([[-458.7787, -469.5018, -471.7736, -455.5112]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  359\n",
      "QVals:  tensor([[-489.4160, -468.9883, -497.6918, -526.2096]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  360\n",
      "QVals:  tensor([[-432.8513, -465.6371, -446.1875, -473.6577]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: True\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = SUMOEnvironment('data/SingleIntersection.sumocfg', 'sumo-gui')\n",
    "done = False\n",
    "state = env._getState()\n",
    "# prevAct = 0\n",
    "rewards = []\n",
    "step = 0\n",
    "while not done:\n",
    "    print(\"Step: \", step)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=PARAM_device).unsqueeze(0)\n",
    "    # get the prediction from the model\n",
    "    with torch.no_grad():\n",
    "        QVals = policyNet(state_tensor)\n",
    "        print(\"QVals: \", QVals)\n",
    "        # get the best action from QVals\n",
    "        bestAction = torch.argmax(QVals).item()\n",
    "        print(\"best Action: \", bestAction)\n",
    "    # perform the action\n",
    "    nextState, r, done = env.takeAction(bestAction)\n",
    "    print(\"Done:\", done)\n",
    "    # print('State: ', state.cpu().numpy())\n",
    "    print('------------------------------')\n",
    "\n",
    "    # update the current state\n",
    "    state = nextState\n",
    "    rewards.append(r)\n",
    "    step+=1\n",
    "    # if step==5:\n",
    "    #     break\n",
    "    # break\n",
    "    # prevAct = bestAction\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running V2 and checking the peformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyNetV2 = torch.load('models/PolicyNet_SUMOv2.pth', weights_only=False)\n",
    "targetNetV2 = torch.load('models/TargetNet_SUMOv2.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'policyNetV2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# get the prediction from the model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 12\u001b[0m     QVals \u001b[38;5;241m=\u001b[39m \u001b[43mpolicyNetV2\u001b[49m(state_tensor)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQVals: \u001b[39m\u001b[38;5;124m\"\u001b[39m, QVals)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# get the best action from QVals\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policyNetV2' is not defined"
     ]
    }
   ],
   "source": [
    "env = SUMOEnvironment('data/SingleIntersection.sumocfg', 'sumo-gui', logPath='logs/RLLogV2.yml')\n",
    "done = False\n",
    "state = env._getState()\n",
    "# prevAct = 0\n",
    "rewards = []\n",
    "step = 0\n",
    "while not done:\n",
    "    print(\"Step: \", step)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=PARAM_device).unsqueeze(0)\n",
    "    # get the prediction from the model\n",
    "    with torch.no_grad():\n",
    "        QVals = policyNetV2(state_tensor)\n",
    "        print(\"QVals: \", QVals)\n",
    "        # get the best action from QVals\n",
    "        bestAction = torch.argmax(QVals).item()\n",
    "        print(\"best Action: \", bestAction)\n",
    "    # perform the action\n",
    "    nextState, r, done = env.takeAction(bestAction)\n",
    "    print(\"Done:\", done)\n",
    "    # print('State: ', state.cpu().numpy())\n",
    "    print('------------------------------')\n",
    "\n",
    "    # update the current state\n",
    "    state = nextState\n",
    "    rewards.append(r)\n",
    "    step+=1\n",
    "    # if step==5:\n",
    "    #     break\n",
    "    # break\n",
    "    # prevAct = bestAction\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to use traci with the log time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.start([\"sumo-gui\", \"-c\", \"data/SingleIntersection.sumocfg\", \"--log\", \"logs/traciLog.txt\", \"--duration-log.statistics\", \"true\"])\n",
    "for i in range(3600):\n",
    "    traci.simulationStep()\n",
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mSUMOEnvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/SingleIntersection.sumocfg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogPath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogs/RLLogV3.yml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m, in \u001b[0;36mSUMOEnvironment.__init__\u001b[1;34m(self, sumoCfgPath, sumoMode, maxTime, enableLogging, logPath)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# starting the simulation\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enableLogging:\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mtraci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msumoMode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msumoCfgPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--log\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--duration-log.statistics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     traci\u001b[38;5;241m.\u001b[39mstart([sumoMode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-c\u001b[39m\u001b[38;5;124m'\u001b[39m, sumoCfgPath])\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\main.py:145\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(cmd, port, numRetries, label, verbose, traceFile, traceGetters, stdout, doSwitch)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmd2))\n\u001b[1;32m--> 145\u001b[0m sumoProcess \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     result \u001b[38;5;241m=\u001b[39m init(sumoPort, numRetries, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m, label, sumoProcess, doSwitch, traceFile, traceGetters)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\sumo\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\sumo\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1436\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1437\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "env = SUMOEnvironment('data/SingleIntersection.sumocfg', 'sum', logPath='logs/RLLogV3.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running V3 and checking the performance\n",
    "___\n",
    "This is done using the new reward function and state definitions, and hence, a new class definition is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SUMOEnvironment:\n",
    "    '''\n",
    "        This class is the environment implemented using SUMO and TRACI for a single intersection.\n",
    "    '''\n",
    "    def __init__(self, sumoCfgPath, sumoMode='sumo', maxTime=3600.0, enableLogging=True, logPath='logs/RLLog.txt'):\n",
    "        self.sumoMode = sumoMode\n",
    "        self.sumoCfgPath = sumoCfgPath # these two are made into class variables because they will also be used in the reset function\n",
    "        self.maxTime = maxTime\n",
    "        self.currTime = 0.0\n",
    "        self.directions = ( # movement directions for calculating the reward function\n",
    "            ('E1_2', '-E3_2'), # left\n",
    "            ('E1_1', 'E2_1'), # straight\n",
    "            ('E1_0', 'E4_0'), # right\n",
    "\n",
    "            ('-E4_2', '-E1_2'), # left\n",
    "            ('-E4_1', '-E3_1'), # straight\n",
    "            ('-E4_0', 'E2_0'), # right\n",
    "\n",
    "            ('-E2_2', 'E4_2'), # left\n",
    "            ('-E2_1', '-E1_1'), # straight\n",
    "            ('-E2_0', '-E3_0'), # right\n",
    "\n",
    "            ('E3_2', 'E2_2'), # left\n",
    "            ('E3_1', 'E4_1'), # straight\n",
    "            ('E3_0', '-E1_0') # right\n",
    "        )\n",
    "        self.incoming = [t[0] for t in self.directions]\n",
    "        self.capacity = 40\n",
    "\n",
    "        # starting the simulation\n",
    "        if enableLogging:\n",
    "            traci.start([sumoMode, '-c', sumoCfgPath, \"--log\", logPath, \"--duration-log.statistics\", \"true\"])\n",
    "        else:\n",
    "            traci.start([sumoMode, '-c', sumoCfgPath])\n",
    "\n",
    "    def _getState(self, intersectionId='Inter'):\n",
    "        '''\n",
    "            This function returns the state at the current time step.\n",
    "        '''\n",
    "        stArray = []\n",
    "        for l in self.incoming:\n",
    "            # getting the number of waiting vehicles in the lane\n",
    "            vc = traci.lane.getLastStepHaltingNumber(l)\n",
    "            stArray.append(vc)\n",
    "\n",
    "        # at the end, appending the current state of the intersection\n",
    "        cs = traci.trafficlight.getPhase(intersectionId)\n",
    "        # ohe the phase\n",
    "        csOhe = [0]*4\n",
    "        csOhe[cs] = 1\n",
    "        stArray.extend(csOhe)\n",
    "        return stArray\n",
    "\n",
    "    def _waitTimeReward(self):\n",
    "        '''\n",
    "            This function defines the reward function based on the waiting time of vehicles in the incoming lanes.\n",
    "            The reward is defined as summ_incomingLanes (-1 * waiitTime(lane)).\n",
    "        '''\n",
    "        r = 0\n",
    "        for l in self.incoming:\n",
    "            r_i = -1 * traci.lane.getWaitingTime(l)\n",
    "            r += r_i\n",
    "        return r\n",
    "    \n",
    "    def _vicCountReward(self):\n",
    "        r = 0\n",
    "        # looping through the directions and calculating individual rewards\n",
    "        for d in self.directions:\n",
    "            # waiting in the incoming lane\n",
    "            vIn = traci.lane.getLastStepHaltingNumber(d[0])\n",
    "            vOut = traci.lane.getLastStepHaltingNumber(d[1])\n",
    "            r_i = -1 * vIn * (1 - (vOut/self.capacity))\n",
    "            r += r_i\n",
    "        return r\n",
    "\n",
    "    def _getReward(self, rewardType='waittime'):\n",
    "        '''\n",
    "            This function returns the reward as of the current state of the intersection.\n",
    "        '''\n",
    "        if rewardType=='waittime':\n",
    "            r = self._waitTimeReward()\n",
    "            return r\n",
    "        elif rewardType=='viccounts':\n",
    "            r = self._vicCountReward()\n",
    "            return r\n",
    "        else:\n",
    "            raise ValueError(\"Please provide the correct reward function type! Accepted values [CASE SENSITIVE]: waiittime, viccounts.\")\n",
    "\n",
    "    def _step(self, t=10):\n",
    "        '''\n",
    "            This function moves the simulation t timesteps ahead. And if the total number of steps reaches the max allowed steps, it stop and returns if the iteration is done.\n",
    "        '''\n",
    "        finished = False\n",
    "        for i in range(t):\n",
    "            if self.currTime==self.maxTime:\n",
    "                finished = True\n",
    "                break\n",
    "            # if not, the continue\n",
    "            self.currTime  = self.currTime + 1.0\n",
    "            traci.simulationStep()\n",
    "        return finished\n",
    "\n",
    "    def takeAction(self, action, intersectionId='Inter', t=10):\n",
    "        '''\n",
    "            This function performs the given action, steps the environment ahead for next t seconds/steps, and then returns the next state, reward and whether the simulation has finished or not.\n",
    "        '''\n",
    "        # take action: set the tl phase to the action value\n",
    "        traci.trafficlight.setPhase(intersectionId, action)\n",
    "        # simulate next t time steps and get the next state\n",
    "        finished = self._step(t)\n",
    "        # get the next state\n",
    "        next_state = self._getState()\n",
    "        # get the reward\n",
    "        reward = self._getReward()\n",
    "\n",
    "        return next_state, reward, finished\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "            This function resets the environment to the start and returns the starting state.\n",
    "        '''\n",
    "        # reseting the sumo engine\n",
    "        traci.load([\"-c\", self.sumoCfgPath])\n",
    "        self.currTime = 0.0\n",
    "        return self._getState()\n",
    "\n",
    "    def close(self):\n",
    "        '''\n",
    "            This function closes the connection of traci with the sumo environment.\n",
    "            NOTE: After calling this function, you will need to reinitialize the object, as now the connection to SUMO has been closed for this. NEED TO FIND A BETTER WAY TO DO THIS.\n",
    "        '''\n",
    "        traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyNetV3 = torch.load('models/PolicyNet_SUMOv3.pth', weights_only=False)\n",
    "targetNetV3 = torch.load('models/TargetNet_SUMOv3.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "QVals:  tensor([[ -99.0406,  -16.9018, -104.3037, -131.5349]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  1\n",
      "QVals:  tensor([[ -15.7218,  -26.1438, -131.3186, -163.9613]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  2\n",
      "QVals:  tensor([[ -9.6225, -17.1546, -53.6041, -69.4822]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  3\n",
      "QVals:  tensor([[ -99.0406,  -16.9018, -104.3037, -131.5349]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  4\n",
      "QVals:  tensor([[-153.9749, -193.7386, -289.6562, -233.4969]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  5\n",
      "QVals:  tensor([[-343.6591, -266.3618, -268.9498, -169.3960]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  6\n",
      "QVals:  tensor([[-287.7279, -267.5357, -166.4020, -257.9536]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  7\n",
      "QVals:  tensor([[-160.9809, -310.2879, -270.1592, -212.1425]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  8\n",
      "QVals:  tensor([[-212.5574, -205.3598, -264.1602, -193.8973]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  9\n",
      "QVals:  tensor([[-265.8293, -222.6992, -275.7700, -291.3037]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  10\n",
      "QVals:  tensor([[-264.4986, -449.3086, -260.2690, -404.2922]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  11\n",
      "QVals:  tensor([[-258.6303, -522.4258, -626.3906, -535.4408]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  12\n",
      "QVals:  tensor([[-454.2940, -318.4818, -447.6130, -365.8780]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  13\n",
      "QVals:  tensor([[-328.1398, -546.2014, -461.9195, -436.1851]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  14\n",
      "QVals:  tensor([[-527.2656, -336.4087, -385.4843, -333.3830]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  15\n",
      "QVals:  tensor([[-404.5220, -311.7529, -322.6079, -438.1820]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  16\n",
      "QVals:  tensor([[-284.2953, -385.9187, -228.8478, -567.9948]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  17\n",
      "QVals:  tensor([[-194.5493, -449.1387, -362.8317, -315.1812]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  18\n",
      "QVals:  tensor([[-338.2542, -292.6416, -277.2986, -230.8652]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  19\n",
      "QVals:  tensor([[-338.1745, -311.9422, -260.7916, -344.3587]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  20\n",
      "QVals:  tensor([[-347.8648, -268.4682, -379.6314, -407.3378]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  21\n",
      "QVals:  tensor([[-260.1561, -458.2231, -487.7891, -367.1193]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  22\n",
      "QVals:  tensor([[-467.6385, -301.2965, -385.7606, -256.0524]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  23\n",
      "QVals:  tensor([[-573.1205, -368.1864, -267.6629, -494.1984]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  24\n",
      "QVals:  tensor([[-426.0999, -279.7962, -584.7210, -563.7355]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  25\n",
      "QVals:  tensor([[-306.4637, -443.0468, -509.4805, -445.2524]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  26\n",
      "QVals:  tensor([[-583.8710, -383.0613, -360.6389, -541.7465]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  27\n",
      "QVals:  tensor([[-616.3466, -370.6703, -741.8649, -404.9355]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  28\n",
      "QVals:  tensor([[-400.4247, -540.3560, -550.8909, -349.0647]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  29\n",
      "QVals:  tensor([[-363.2346, -454.3112, -389.5795, -521.4298]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  30\n",
      "QVals:  tensor([[-561.8613, -472.9733, -361.5173, -481.9080]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  31\n",
      "QVals:  tensor([[-385.0578, -367.6120, -459.4104, -353.5214]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  32\n",
      "QVals:  tensor([[-434.2758, -311.7455, -337.6149, -441.8143]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  33\n",
      "QVals:  tensor([[-437.6774, -376.8085, -266.3419, -411.5136]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  34\n",
      "QVals:  tensor([[-193.2704, -340.1267, -363.4702, -331.3628]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  35\n",
      "QVals:  tensor([[-334.0191, -334.4661, -264.3732, -228.5876]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  36\n",
      "QVals:  tensor([[-297.5392, -330.7433, -242.2393, -317.5216]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  37\n",
      "QVals:  tensor([[-347.4475, -435.5168, -583.7167, -450.0239]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  38\n",
      "QVals:  tensor([[-578.9808, -444.4476, -663.6699, -348.7097]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  39\n",
      "QVals:  tensor([[-434.9985, -408.1506, -332.4780, -524.5982]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  40\n",
      "QVals:  tensor([[-295.9009, -272.7427, -392.0882, -350.8964]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  41\n",
      "QVals:  tensor([[-213.5793, -474.2276, -512.5272, -387.3507]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  42\n",
      "QVals:  tensor([[-347.8329, -403.0882, -395.7701, -195.1217]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  43\n",
      "QVals:  tensor([[-247.6294, -249.1328, -208.2612, -235.5305]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  44\n",
      "QVals:  tensor([[-223.5846, -269.7830, -316.9280, -263.7308]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  45\n",
      "QVals:  tensor([[-584.5264, -221.3107, -379.4761, -318.2511]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  46\n",
      "QVals:  tensor([[-345.1503, -298.2313, -278.7608, -199.7126]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  47\n",
      "QVals:  tensor([[-345.9012, -309.2311, -168.2113, -302.5019]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  48\n",
      "QVals:  tensor([[-383.0132, -357.4247, -362.9354, -173.1146]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  49\n",
      "QVals:  tensor([[-316.5255, -182.4598, -268.6053, -280.0923]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  50\n",
      "QVals:  tensor([[-265.9583, -251.1308, -244.7098, -228.1818]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  51\n",
      "QVals:  tensor([[-446.2076, -266.9097, -333.9000, -388.3373]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  52\n",
      "QVals:  tensor([[-439.2920, -454.5081, -410.8225, -349.4231]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  53\n",
      "QVals:  tensor([[-454.9494, -418.3309, -349.6834, -630.8076]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  54\n",
      "QVals:  tensor([[-279.7862, -342.7839, -640.0002, -612.8825]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  55\n",
      "QVals:  tensor([[-773.6657, -226.1592, -716.4982, -539.8654]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  56\n",
      "QVals:  tensor([[-259.1057, -461.8936, -431.9899, -451.0191]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  57\n",
      "QVals:  tensor([[-379.4814, -354.0727, -386.1409, -272.1388]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  58\n",
      "QVals:  tensor([[-324.7131, -256.2152, -284.7122, -309.4876]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  59\n",
      "QVals:  tensor([[-278.2376, -387.0729, -414.3728, -331.9934]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  60\n",
      "QVals:  tensor([[-400.1496, -422.6692, -440.1048, -235.2517]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  61\n",
      "QVals:  tensor([[-267.0169, -277.3631, -221.8378, -270.3677]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  62\n",
      "QVals:  tensor([[-227.5364, -318.5321, -380.1050, -342.0750]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  63\n",
      "QVals:  tensor([[-373.6749, -355.1569, -395.0341, -247.7682]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  64\n",
      "QVals:  tensor([[-286.3428, -263.7653, -337.9263, -311.9964]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  65\n",
      "QVals:  tensor([[-364.8368, -422.9559, -429.6302, -301.7472]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  66\n",
      "QVals:  tensor([[-413.2410, -345.1392, -509.1517, -517.8417]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  67\n",
      "QVals:  tensor([[-316.6617, -531.4899, -512.4865, -371.5284]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  68\n",
      "QVals:  tensor([[-601.0084, -613.2437, -559.3354, -294.9449]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  69\n",
      "QVals:  tensor([[-357.0645, -343.6806, -228.4821, -365.1669]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  70\n",
      "QVals:  tensor([[-266.6181, -370.4222, -401.8036, -265.6261]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  71\n",
      "QVals:  tensor([[-264.5416, -311.9159, -316.1697, -374.3885]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  72\n",
      "QVals:  tensor([[-444.0921, -396.0258, -464.7898, -256.3843]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  73\n",
      "QVals:  tensor([[-467.7340, -350.1777, -323.1441, -435.1188]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  74\n",
      "QVals:  tensor([[-363.0349, -330.2035, -356.7202, -338.7123]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  75\n",
      "QVals:  tensor([[-448.0467, -556.3470, -476.0965, -435.4206]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  76\n",
      "QVals:  tensor([[-468.8025, -530.9199, -458.1286, -670.1323]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  77\n",
      "QVals:  tensor([[-409.1413, -574.0377, -760.2880, -578.1962]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  78\n",
      "QVals:  tensor([[-562.1680, -430.0669, -683.3388, -348.5808]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  79\n",
      "QVals:  tensor([[-540.3127, -328.9653, -348.0933, -496.6295]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  80\n",
      "QVals:  tensor([[-515.0817, -701.7429, -612.3347, -603.4420]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  81\n",
      "QVals:  tensor([[-688.1732, -519.7471, -487.7665, -339.4221]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  82\n",
      "QVals:  tensor([[-498.4188, -518.6752, -353.4771, -601.8534]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  83\n",
      "QVals:  tensor([[-427.9471, -418.4003, -668.0248, -445.6146]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  84\n",
      "QVals:  tensor([[-371.8080, -788.7614, -596.6880, -445.6204]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  85\n",
      "QVals:  tensor([[-476.3664, -539.9920, -402.5255, -267.6060]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  86\n",
      "QVals:  tensor([[-327.3069, -406.6899, -234.4873, -394.5854]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  87\n",
      "QVals:  tensor([[-406.0364, -331.9425, -544.0706, -350.0276]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  88\n",
      "QVals:  tensor([[-310.1738, -456.3145, -446.1820, -339.8731]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  89\n",
      "QVals:  tensor([[-397.0167, -404.9673, -397.6747, -310.9282]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  90\n",
      "QVals:  tensor([[-332.5579, -333.6406, -351.6617, -397.8146]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  91\n",
      "QVals:  tensor([[-430.4412, -373.5892, -323.1241, -567.2737]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  92\n",
      "QVals:  tensor([[-640.9818, -376.6600, -740.8486, -548.3173]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  93\n",
      "QVals:  tensor([[-443.8816, -626.7029, -660.4324, -524.4847]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  94\n",
      "QVals:  tensor([[-1957.3553, -1765.7917, -1732.3816, -1521.2837]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  95\n",
      "QVals:  tensor([[-1797.0531, -1580.0479, -1729.6986, -2016.1678]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  96\n",
      "QVals:  tensor([[-1710.1747, -2228.4832, -1556.4751, -2475.4739]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  97\n",
      "QVals:  tensor([[-1488.1594, -2074.9031, -2469.8594, -2332.6799]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  98\n",
      "QVals:  tensor([[-2065.8699, -1607.9861, -2104.1328, -1937.3445]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  99\n",
      "QVals:  tensor([[-1476.6910, -1882.6313, -2016.5570, -1928.8856]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  100\n",
      "QVals:  tensor([[-1984.9373, -1473.2432, -1866.7086, -1654.7031]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  101\n",
      "QVals:  tensor([[-1809.5005, -2291.8367, -2061.7507, -1935.4266]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  102\n",
      "QVals:  tensor([[-2368.4290, -2061.2004, -2241.8923, -1999.3293]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  103\n",
      "QVals:  tensor([[-2227.1672, -2026.6338, -2189.1313, -2492.8716]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  104\n",
      "QVals:  tensor([[-1977.1993, -2545.3479, -2122.2109, -2883.3110]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  105\n",
      "QVals:  tensor([[-2198.0818, -2174.2029, -1704.5057, -2658.1948]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  106\n",
      "QVals:  tensor([[-1632.5048, -1688.8748, -2600.8042, -2357.1814]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  107\n",
      "QVals:  tensor([[-1994.2465, -1514.4812, -2164.5222, -1787.5398]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  108\n",
      "QVals:  tensor([[-1616.4213, -1967.6201, -2175.7258, -2081.5010]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  109\n",
      "QVals:  tensor([[-2362.4397, -1876.0565, -2363.7568, -1904.0950]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  110\n",
      "QVals:  tensor([[-1788.0013, -2168.3040, -2242.9849, -1959.8345]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  111\n",
      "QVals:  tensor([[-2243.9888, -1879.6289, -2248.6074, -1775.9192]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  112\n",
      "QVals:  tensor([[-2458.8218, -1885.2373, -2777.7847, -2710.6501]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  113\n",
      "QVals:  tensor([[-1712.4282, -2281.9011, -2482.8059, -2572.8613]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  114\n",
      "QVals:  tensor([[-2292.0266, -2117.9519, -2137.9656, -2347.4246]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  115\n",
      "QVals:  tensor([[-2096.1880, -2564.4211, -2478.6843, -2458.9517]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  116\n",
      "QVals:  tensor([[-2686.7590, -2253.5862, -2578.8318, -2240.7249]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  117\n",
      "QVals:  tensor([[-2971.3621, -2587.9302, -3015.2075, -3045.0540]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  118\n",
      "QVals:  tensor([[-2331.4641, -3091.6619, -2555.0764, -3413.8223]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  119\n",
      "QVals:  tensor([[-3233.0217, -3108.9478, -2761.7061, -3155.5947]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  120\n",
      "QVals:  tensor([[-2962.6768, -2754.8796, -3798.1353, -3064.4951]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  121\n",
      "QVals:  tensor([[-2482.6614, -3215.1326, -3663.3301, -2875.2544]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  122\n",
      "QVals:  tensor([[-2839.2385, -2590.2029, -3275.7993, -2273.1201]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  123\n",
      "QVals:  tensor([[-2583.0911, -2397.4851, -3327.2781, -3039.7573]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  124\n",
      "QVals:  tensor([[-2439.3098, -3067.9446, -3455.1128, -3149.9893]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  125\n",
      "QVals:  tensor([[-2924.7581, -2586.0361, -3115.5684, -2476.3901]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  126\n",
      "QVals:  tensor([[-2732.4180, -2440.8604, -3494.5869, -3206.0159]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  127\n",
      "QVals:  tensor([[-2513.9651, -3139.9185, -3386.0386, -3505.4185]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  128\n",
      "QVals:  tensor([[-2898.3433, -2600.7847, -2559.5808, -2753.3857]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  129\n",
      "QVals:  tensor([[-2991.3162, -2563.5989, -3628.1016, -3399.0796]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  130\n",
      "QVals:  tensor([[-2597.5515, -3232.6628, -3522.2334, -3153.6047]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  131\n",
      "QVals:  tensor([[-3169.8127, -2911.2849, -3123.2473, -2641.6367]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  132\n",
      "QVals:  tensor([[-3450.8669, -2920.1746, -3685.0286, -3721.6562]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  133\n",
      "QVals:  tensor([[-2757.8669, -3524.8625, -3385.8286, -3678.9453]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  134\n",
      "QVals:  tensor([[-3311.8843, -3000.0437, -2748.2500, -3045.5764]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  135\n",
      "QVals:  tensor([[-3473.0002, -2726.4773, -3823.3174, -3530.0347]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  136\n",
      "QVals:  tensor([[-2651.3850, -3304.0959, -3578.5132, -3292.3774]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  137\n",
      "QVals:  tensor([[-3091.4600, -2815.4197, -3043.8494, -2631.9382]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  138\n",
      "QVals:  tensor([[-2853.3416, -2471.2405, -3465.0730, -3188.4817]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  139\n",
      "QVals:  tensor([[-2378.2825, -2970.1365, -2978.2429, -3172.3584]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  140\n",
      "QVals:  tensor([[-2630.7126, -2488.1375, -2216.1714, -2717.6243]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  141\n",
      "QVals:  tensor([[-2984.7014, -2410.0386, -3570.4243, -3541.1619]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  142\n",
      "QVals:  tensor([[-2378.2849, -2949.8682, -3378.6155, -3305.8088]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  143\n",
      "QVals:  tensor([[-2595.5383, -2578.1394, -2415.0854, -2625.6465]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  144\n",
      "QVals:  tensor([[-3010.0720, -2490.5288, -3717.0271, -3677.0352]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  145\n",
      "QVals:  tensor([[-2413.6316, -3076.7502, -3557.5015, -3410.6270]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  146\n",
      "QVals:  tensor([[-2800.2012, -2632.5554, -2564.3774, -2697.7578]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  147\n",
      "QVals:  tensor([[-3274.5981, -2546.7209, -3736.2681, -3495.5659]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  148\n",
      "QVals:  tensor([[-2500.8147, -3190.5120, -3792.5874, -3314.0317]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  149\n",
      "QVals:  tensor([[-3159.6448, -2850.1968, -3444.0771, -2614.6497]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  150\n",
      "QVals:  tensor([[-3020.0850, -2619.8313, -3824.0503, -3579.6313]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  151\n",
      "QVals:  tensor([[-2463.8508, -3067.2810, -3535.4082, -3562.5962]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  152\n",
      "QVals:  tensor([[-2939.8357, -2707.4700, -2591.9377, -2950.4907]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  153\n",
      "QVals:  tensor([[-3018.3381, -2459.9158, -3494.4050, -3439.5449]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  154\n",
      "QVals:  tensor([[-2197.8459, -2912.8838, -3525.7576, -3266.7346]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  155\n",
      "QVals:  tensor([[-2750.2781, -2502.6450, -2903.9697, -2419.1123]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  156\n",
      "QVals:  tensor([[-2853.8950, -2403.0110, -3663.8813, -3343.7148]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  157\n",
      "QVals:  tensor([[-1985.8854, -2723.6028, -3267.7178, -3292.5698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  158\n",
      "QVals:  tensor([[-2760.3474, -2551.3918, -2388.7227, -3114.4824]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  159\n",
      "QVals:  tensor([[-3000.4954, -2444.5073, -3772.5396, -3827.8796]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  160\n",
      "QVals:  tensor([[-2090.6228, -2809.1172, -3467.8259, -3138.3467]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  161\n",
      "QVals:  tensor([[-2939.9270, -2480.2231, -2828.6064, -2750.9600]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  162\n",
      "QVals:  tensor([[-2382.4114, -2956.6321, -2922.8257, -2933.4287]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  163\n",
      "QVals:  tensor([[-2839.5942, -2774.5457, -2761.3159, -2515.3049]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  164\n",
      "QVals:  tensor([[-3129.8230, -2748.0957, -3061.3350, -3217.9314]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  165\n",
      "QVals:  tensor([[-2594.9441, -3218.2480, -3001.1416, -3285.7480]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  166\n",
      "QVals:  tensor([[-3053.9822, -2745.5642, -2627.7417, -2783.7185]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  167\n",
      "QVals:  tensor([[-3474.5305, -2641.7241, -4061.0959, -3794.1523]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  168\n",
      "QVals:  tensor([[-2605.9509, -3176.8889, -3633.9456, -3357.7114]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  169\n",
      "QVals:  tensor([[-3248.3552, -3057.5078, -3108.6660, -2858.1460]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  170\n",
      "QVals:  tensor([[-3636.2410, -3009.6702, -3933.3635, -4013.2104]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  171\n",
      "QVals:  tensor([[-2565.0498, -3334.5798, -3490.0967, -3787.7507]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  172\n",
      "QVals:  tensor([[-2943.3784, -2706.7800, -2433.1858, -3132.1504]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  173\n",
      "QVals:  tensor([[-3020.9573, -2532.8955, -3808.2017, -3516.0708]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  174\n",
      "QVals:  tensor([[-2561.9844, -3310.3655, -3771.7925, -3534.6396]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  175\n",
      "QVals:  tensor([[-2953.1902, -2649.4666, -3114.7632, -2666.9460]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  176\n",
      "QVals:  tensor([[-2533.9976, -3150.7932, -3267.8538, -2911.7700]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  177\n",
      "QVals:  tensor([[-3006.2869, -2638.0308, -3045.8628, -2536.0918]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  178\n",
      "QVals:  tensor([[-2826.3894, -2767.8667, -3168.4014, -3411.3777]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  179\n",
      "QVals:  tensor([[-2636.8665, -3254.6541, -3163.9961, -3493.6804]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  180\n",
      "QVals:  tensor([[-3004.5066, -2688.4377, -2536.0503, -3048.5801]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  181\n",
      "QVals:  tensor([[-2917.2371, -2543.9199, -3867.8613, -3537.1934]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  182\n",
      "QVals:  tensor([[-2360.8713, -2968.6467, -3659.0894, -3352.9214]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  183\n",
      "QVals:  tensor([[-2711.2996, -2340.6697, -3112.3188, -2574.0840]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  184\n",
      "QVals:  tensor([[-2340.5522, -2922.0759, -3275.3643, -2826.1890]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  185\n",
      "QVals:  tensor([[-2629.9133, -2483.4568, -2757.4285, -2188.8291]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  186\n",
      "QVals:  tensor([[-2971.7263, -2610.7324, -3143.8447, -3247.7515]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  187\n",
      "QVals:  tensor([[-2549.3430, -3108.5125, -3127.4756, -3278.8794]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  188\n",
      "QVals:  tensor([[-2971.5352, -2758.4358, -2600.2988, -2821.9668]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  189\n",
      "QVals:  tensor([[-2903.3430, -2583.1770, -3585.2310, -3200.5708]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  190\n",
      "QVals:  tensor([[-2339.7590, -3084.0125, -3887.5205, -3215.1660]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  191\n",
      "QVals:  tensor([[-2998.1460, -2630.2971, -3361.3140, -2530.2893]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  192\n",
      "QVals:  tensor([[-2847.7832, -2334.0642, -3703.4109, -3387.6611]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  193\n",
      "QVals:  tensor([[-1979.3265, -2588.5115, -3301.9839, -3204.5630]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  194\n",
      "QVals:  tensor([[-2566.9858, -2319.8340, -2531.3608, -2540.8110]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  195\n",
      "QVals:  tensor([[-2359.4504, -2980.5427, -2936.9927, -2891.9255]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  196\n",
      "QVals:  tensor([[-2774.4807, -2418.5256, -2272.1526, -2825.7856]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  197\n",
      "QVals:  tensor([[-3381.5305, -2470.5071, -3566.5376, -3769.8447]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  198\n",
      "QVals:  tensor([[-2592.9934, -3247.3528, -3229.0459, -2981.4585]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  199\n",
      "QVals:  tensor([[-2998.2837, -2736.5007, -2771.3298, -2749.0513]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  200\n",
      "QVals:  tensor([[-3026.1111, -3491.1091, -3196.9209, -3038.7012]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  201\n",
      "QVals:  tensor([[-3667.2634, -3462.5159, -3177.6025, -3128.2188]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  202\n",
      "QVals:  tensor([[-3628.4531, -3361.3420, -3283.5625, -4150.3770]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  203\n",
      "QVals:  tensor([[-3900.2947, -2792.9233, -4381.5449, -4275.8975]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  204\n",
      "QVals:  tensor([[-3969.8738, -4505.1367, -4975.2363, -4635.3555]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  205\n",
      "QVals:  tensor([[-4511.6104, -3873.9402, -4345.2578, -4276.3115]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  206\n",
      "QVals:  tensor([[-3292.3430, -3990.6433, -3991.4873, -3913.1899]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  207\n",
      "QVals:  tensor([[-3527.2112, -3464.7952, -3257.3501, -3160.8755]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  208\n",
      "QVals:  tensor([[-3374.8772, -3026.7595, -3146.4578, -3650.7368]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  209\n",
      "QVals:  tensor([[-3451.3748, -3929.0471, -3286.2161, -3636.5400]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  210\n",
      "QVals:  tensor([[-3531.4541, -3849.9185, -5076.2051, -4112.6191]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  211\n",
      "QVals:  tensor([[-3988.1104, -3417.6702, -4303.0806, -3420.0317]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  212\n",
      "QVals:  tensor([[-3097.5139, -4046.6968, -4462.9873, -3814.2144]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  213\n",
      "QVals:  tensor([[-3652.4490, -3509.8621, -4033.9248, -2855.5811]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  214\n",
      "QVals:  tensor([[-3755.1799, -2976.9324, -3998.9956, -3969.2751]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  215\n",
      "QVals:  tensor([[-2950.8293, -3516.1846, -3317.7466, -3415.5400]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  216\n",
      "QVals:  tensor([[-3445.5271, -3265.1968, -3256.7627, -3020.1055]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  217\n",
      "QVals:  tensor([[-3414.9971, -2988.3342, -3766.9746, -3878.2720]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  218\n",
      "QVals:  tensor([[-3205.6941, -3903.9675, -3799.9517, -3757.8743]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  219\n",
      "QVals:  tensor([[-3402.9968, -3304.1562, -2803.7305, -3443.4204]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  220\n",
      "QVals:  tensor([[-3511.4365, -3052.0513, -4066.7307, -3467.1211]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  221\n",
      "QVals:  tensor([[-2945.4714, -3733.1858, -4044.6440, -3450.2737]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  222\n",
      "QVals:  tensor([[-3677.0513, -3412.1443, -3536.1538, -3079.8879]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  223\n",
      "QVals:  tensor([[-3771.2202, -3132.8230, -3963.9705, -4112.7266]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  224\n",
      "QVals:  tensor([[-2813.0442, -3550.8772, -3509.5994, -3704.4824]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  225\n",
      "QVals:  tensor([[-3310.8188, -3160.8479, -2792.0891, -3357.1074]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  226\n",
      "QVals:  tensor([[-3183.4949, -2667.3765, -3650.5205, -3358.8198]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  227\n",
      "QVals:  tensor([[-2987.8586, -3663.5364, -3815.1177, -3570.7402]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  228\n",
      "QVals:  tensor([[-3314.1060, -3163.8391, -3091.9661, -3036.7026]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  229\n",
      "QVals:  tensor([[-3495.5999, -3111.8235, -3809.8354, -4010.2324]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  230\n",
      "QVals:  tensor([[-2635.2791, -3418.2791, -3518.5601, -3991.6919]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  231\n",
      "QVals:  tensor([[-3111.3445, -3001.1060, -2459.9209, -3518.5161]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  232\n",
      "QVals:  tensor([[-2999.3696, -2669.4226, -3989.7151, -3730.9121]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  233\n",
      "QVals:  tensor([[-2222.1672, -2980.1292, -3484.0078, -3573.2598]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  234\n",
      "QVals:  tensor([[-2952.3765, -2809.7571, -2714.2412, -2952.9153]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  235\n",
      "QVals:  tensor([[-3112.1531, -2897.9377, -4051.6382, -3445.0930]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  236\n",
      "QVals:  tensor([[-2884.6780, -3664.7815, -3908.7163, -3924.1934]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  237\n",
      "QVals:  tensor([[-3077.2646, -2912.5454, -2748.1912, -3036.4390]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  238\n",
      "QVals:  tensor([[-3290.2583, -2880.0703, -3985.6062, -3427.8025]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  239\n",
      "QVals:  tensor([[-2868.2627, -3657.0850, -3946.9814, -3622.3862]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  240\n",
      "QVals:  tensor([[-3247.5593, -3001.7935, -3361.9526, -2741.1846]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  241\n",
      "QVals:  tensor([[-3212.0242, -2732.2617, -3808.1367, -3632.6692]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  242\n",
      "QVals:  tensor([[-2708.4365, -3566.3474, -3395.1553, -3742.2905]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  243\n",
      "QVals:  tensor([[-3337.3274, -3239.9297, -2643.1633, -3615.0781]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  244\n",
      "QVals:  tensor([[-3661.1128, -2912.4500, -4243.5928, -3909.9692]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  245\n",
      "QVals:  tensor([[-2790.5623, -3705.5918, -3991.5933, -3659.0850]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  246\n",
      "QVals:  tensor([[-3602.6707, -3457.1123, -3129.6909, -3522.7180]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  247\n",
      "QVals:  tensor([[-3536.4163, -3161.6946, -4312.1074, -3754.3433]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  248\n",
      "QVals:  tensor([[-3153.2415, -4148.6108, -4638.1348, -3922.1768]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  249\n",
      "QVals:  tensor([[-3578.5754, -3440.3752, -4050.0088, -3011.7224]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  250\n",
      "QVals:  tensor([[-3160.3364, -3057.3130, -3831.0217, -4025.3716]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  251\n",
      "QVals:  tensor([[-3220.8474, -3992.0344, -4095.5156, -3739.0278]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  252\n",
      "QVals:  tensor([[-3700.6809, -3422.4812, -3412.4932, -3242.5396]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  253\n",
      "QVals:  tensor([[-3536.9731, -3286.7380, -3976.5078, -4255.7148]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  254\n",
      "QVals:  tensor([[-2660.6028, -3539.9084, -3515.1504, -4038.9043]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  255\n",
      "QVals:  tensor([[-3118.0500, -3113.8130, -2412.5186, -3711.0576]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  256\n",
      "QVals:  tensor([[-3371.5129, -2746.1357, -4024.1724, -3909.2900]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  257\n",
      "QVals:  tensor([[-2990.0715, -3652.2173, -3981.5874, -3795.4399]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  258\n",
      "QVals:  tensor([[-3608.0823, -3422.1499, -3302.5728, -3272.1343]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  259\n",
      "QVals:  tensor([[-3275.2458, -3128.9453, -3603.9175, -3931.9827]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  260\n",
      "QVals:  tensor([[-3238.4343, -3923.0164, -3693.1973, -4098.0669]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  261\n",
      "QVals:  tensor([[-3753.1858, -3525.8701, -3095.2812, -3595.4832]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  262\n",
      "QVals:  tensor([[-3391.2910, -2998.2605, -4250.3442, -3808.6865]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  263\n",
      "QVals:  tensor([[-3108.7869, -3886.4617, -4285.4521, -4020.0674]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  264\n",
      "QVals:  tensor([[-3804.8967, -3479.7251, -3783.6860, -3248.0605]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  265\n",
      "QVals:  tensor([[-3556.2249, -3378.7874, -4282.4121, -4468.1885]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  266\n",
      "QVals:  tensor([[-3299.7751, -4033.7356, -4038.9404, -4309.9795]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  267\n",
      "QVals:  tensor([[-3857.8054, -3608.3538, -3249.5396, -3642.6089]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  268\n",
      "QVals:  tensor([[-3720.8616, -3330.0876, -4658.1523, -3884.5513]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  269\n",
      "QVals:  tensor([[-3125.5059, -4065.8574, -4541.4375, -3867.2700]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  270\n",
      "QVals:  tensor([[-4129.0239, -3960.2856, -3956.6511, -3330.9023]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  271\n",
      "QVals:  tensor([[-3609.9761, -3278.8372, -3762.3496, -4066.7961]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  272\n",
      "QVals:  tensor([[-3159.3743, -3931.1018, -3742.2808, -4387.1880]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  273\n",
      "QVals:  tensor([[-3836.0171, -3684.4041, -3231.7029, -3701.1802]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  274\n",
      "QVals:  tensor([[-3476.0544, -3130.6792, -4384.1279, -3681.1318]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  275\n",
      "QVals:  tensor([[-3271.8564, -4122.0508, -4352.4580, -3941.7144]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  276\n",
      "QVals:  tensor([[-3893.8408, -3577.7170, -3771.5405, -3263.8967]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  277\n",
      "QVals:  tensor([[-3513.0701, -3080.1252, -3865.8718, -3903.5918]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  278\n",
      "QVals:  tensor([[-3287.6692, -3894.9329, -4016.9749, -4042.8701]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  279\n",
      "QVals:  tensor([[-3873.9854, -3491.7178, -3291.4404, -3438.4729]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  280\n",
      "QVals:  tensor([[-3890.2341, -3239.3984, -4511.9580, -3754.5664]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  281\n",
      "QVals:  tensor([[-3299.6157, -3982.8770, -4433.6499, -3770.1396]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  282\n",
      "QVals:  tensor([[-4008.2466, -3532.1279, -4003.3672, -3147.0410]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  283\n",
      "QVals:  tensor([[-3795.4646, -3089.2610, -3993.5054, -3884.7188]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  284\n",
      "QVals:  tensor([[-3129.4377, -3764.8660, -3768.0610, -3680.6343]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  285\n",
      "QVals:  tensor([[-4066.1101, -3825.3394, -3554.4866, -3531.9639]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  286\n",
      "QVals:  tensor([[-4087.4812, -3619.7175, -3901.2903, -4572.6572]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  287\n",
      "QVals:  tensor([[-3280.8708, -4004.6504, -3447.6060, -4356.4658]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  288\n",
      "QVals:  tensor([[-3972.6973, -3846.2258, -3035.2197, -4186.7822]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  289\n",
      "QVals:  tensor([[-3236.0706, -2852.3059, -4104.4756, -3545.2959]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  290\n",
      "QVals:  tensor([[-3098.4146, -3791.4714, -3860.7227, -3697.9102]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  291\n",
      "QVals:  tensor([[-3731.9521, -3508.7595, -3171.7280, -3473.9702]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  292\n",
      "QVals:  tensor([[-3535.2578, -3226.0393, -4433.9785, -3641.9370]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  293\n",
      "QVals:  tensor([[-3117.4866, -3936.1016, -4242.9600, -3595.3257]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  294\n",
      "QVals:  tensor([[-3427.8025, -3207.9551, -3563.4045, -2679.8704]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  295\n",
      "QVals:  tensor([[-3026.4595, -2644.7642, -3603.1982, -3388.1611]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  296\n",
      "QVals:  tensor([[-2751.7654, -3407.8450, -3676.3137, -3588.6421]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  297\n",
      "QVals:  tensor([[-3136.7209, -2942.0061, -2939.9844, -2789.6401]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  298\n",
      "QVals:  tensor([[-3085.8401, -2697.3708, -3479.6196, -3413.0571]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  299\n",
      "QVals:  tensor([[-2568.3025, -3375.3784, -3493.3286, -3866.7117]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  300\n",
      "QVals:  tensor([[-3167.7559, -3039.4780, -2508.4661, -3541.2358]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  301\n",
      "QVals:  tensor([[-3652.8552, -2849.9224, -4228.4326, -3820.6719]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  302\n",
      "QVals:  tensor([[-2944.3284, -3568.2795, -3829.3103, -3701.4336]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  303\n",
      "QVals:  tensor([[-3643.3474, -3274.1448, -3171.8606, -3332.7168]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  304\n",
      "QVals:  tensor([[-3451.9309, -2986.3689, -4273.0225, -3711.4810]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  305\n",
      "QVals:  tensor([[-2821.7827, -3588.1140, -4012.0427, -3397.1660]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  306\n",
      "QVals:  tensor([[-3654.7859, -3186.6282, -3815.8730, -3131.1599]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  307\n",
      "QVals:  tensor([[-4335.8291, -3120.4375, -4311.8994, -4475.6641]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  308\n",
      "QVals:  tensor([[-2886.3960, -3611.5684, -3381.1792, -3865.6682]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  309\n",
      "QVals:  tensor([[-3596.5896, -3300.8137, -2867.2966, -3421.6792]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  310\n",
      "QVals:  tensor([[-3736.6101, -2964.4094, -4375.2051, -3871.3262]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  311\n",
      "QVals:  tensor([[-2868.2991, -3491.3794, -3759.5439, -3465.8943]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  312\n",
      "QVals:  tensor([[-3447.9324, -3111.7981, -3273.8066, -2893.7900]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  313\n",
      "QVals:  tensor([[-3667.7722, -3188.7908, -4021.6030, -4140.8311]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  314\n",
      "QVals:  tensor([[-2792.3237, -3588.1182, -3752.5469, -4271.9746]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  315\n",
      "QVals:  tensor([[-2971.7671, -3022.5476, -2384.1724, -3622.5774]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  316\n",
      "QVals:  tensor([[-2885.5535, -2519.5352, -3693.1509, -3360.1025]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  317\n",
      "QVals:  tensor([[-2584.8218, -3250.3098, -3628.5266, -3496.2441]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  318\n",
      "QVals:  tensor([[-3042.8733, -2843.6919, -2934.5000, -2700.4263]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  319\n",
      "QVals:  tensor([[-3118.2473, -2745.3972, -3514.1807, -3422.8826]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  320\n",
      "QVals:  tensor([[-2454.3328, -3112.9255, -3338.5723, -3560.3918]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  321\n",
      "QVals:  tensor([[-2935.2920, -2644.6204, -2569.0342, -2786.2520]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  322\n",
      "QVals:  tensor([[-3585.7515, -2876.2249, -4159.4146, -3765.7754]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  323\n",
      "QVals:  tensor([[-2804.5029, -3513.3645, -3799.4707, -3418.6804]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  324\n",
      "QVals:  tensor([[-3397.2859, -2949.5569, -3360.9170, -2838.6663]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  325\n",
      "QVals:  tensor([[-3498.7117, -2785.6023, -4213.3623, -3895.2563]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  326\n",
      "QVals:  tensor([[-2745.0100, -3368.0120, -3768.4329, -3597.1313]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  327\n",
      "QVals:  tensor([[-3357.8792, -3119.3586, -2870.0522, -3249.3623]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  328\n",
      "QVals:  tensor([[-3340.6033, -2878.1106, -3983.9907, -3413.7612]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  329\n",
      "QVals:  tensor([[-2498.3164, -3198.8186, -3819.5449, -3236.8921]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  330\n",
      "QVals:  tensor([[-3333.6060, -3021.1497, -3439.3408, -2715.0071]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  331\n",
      "QVals:  tensor([[-3125.3975, -2726.0610, -3580.7969, -3469.2261]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  332\n",
      "QVals:  tensor([[-2750.0325, -3420.6943, -3531.5676, -3618.2329]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  333\n",
      "QVals:  tensor([[-3371.7283, -3152.5325, -2733.8396, -3335.7979]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  334\n",
      "QVals:  tensor([[-3413.1230, -2861.3728, -4125.7334, -3669.2495]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  335\n",
      "QVals:  tensor([[-3236.9419, -3905.8357, -4178.9785, -3840.4741]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  336\n",
      "QVals:  tensor([[-3590.2153, -3234.0022, -3345.3521, -3134.8022]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  337\n",
      "QVals:  tensor([[-3425.0896, -3114.2188, -3987.3350, -4091.7197]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  338\n",
      "QVals:  tensor([[-3121.5613, -3765.5579, -3844.1987, -3999.7056]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  339\n",
      "QVals:  tensor([[-3608.2153, -3342.1531, -3066.2654, -3405.8699]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  340\n",
      "QVals:  tensor([[-3620.3137, -3165.5403, -4371.9824, -3748.5283]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  341\n",
      "QVals:  tensor([[-3072.6321, -3888.0366, -4279.6294, -3917.9258]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  342\n",
      "QVals:  tensor([[-3828.4895, -3500.6838, -3892.2551, -3127.5273]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  343\n",
      "QVals:  tensor([[-3664.3872, -3110.3308, -3748.3975, -3738.2104]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  344\n",
      "QVals:  tensor([[-2969.2473, -3567.2007, -3664.2371, -3632.2339]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  345\n",
      "QVals:  tensor([[-3664.4260, -3318.2732, -3278.9875, -3143.4644]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  346\n",
      "QVals:  tensor([[-3591.6152, -3247.1367, -3726.3638, -4015.5642]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  347\n",
      "QVals:  tensor([[-3053.4241, -3702.3499, -3683.2788, -4007.1770]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  348\n",
      "QVals:  tensor([[-3721.6714, -3366.6960, -3070.8892, -3401.4231]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  349\n",
      "QVals:  tensor([[-3774.3240, -3066.5513, -4314.9248, -3910.1189]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  350\n",
      "QVals:  tensor([[-2813.8945, -3679.2712, -4258.4380, -3776.6299]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  351\n",
      "QVals:  tensor([[-3418.5400, -3094.8606, -3608.3318, -2763.7642]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  352\n",
      "QVals:  tensor([[-3508.9604, -2989.1711, -3751.7295, -3981.2969]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  353\n",
      "QVals:  tensor([[-2908.9553, -3677.3647, -3763.3235, -3912.1992]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  354\n",
      "QVals:  tensor([[-3471.3630, -3175.9404, -2915.3960, -3301.9722]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  355\n",
      "QVals:  tensor([[-3706.2131, -3118.2415, -4449.1826, -4196.8701]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  356\n",
      "QVals:  tensor([[-2874.4485, -3830.6863, -4284.7441, -3716.5415]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  357\n",
      "QVals:  tensor([[-3166.4368, -2843.1255, -3341.7256, -2640.0916]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  358\n",
      "QVals:  tensor([[-3217.9951, -2786.2722, -3683.1729, -3642.3245]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  359\n",
      "QVals:  tensor([[-2803.3518, -3366.7568, -3554.8450, -3531.8335]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  360\n",
      "QVals:  tensor([[-3264.6033, -2963.3547, -2929.5110, -2985.5957]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: True\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = SUMOEnvironment('data/SingleIntersection.sumocfg', 'sumo-gui', logPath='logs/RLLogV3TestFinal.yml')\n",
    "done = False\n",
    "state = env._getState()\n",
    "# prevAct = 0\n",
    "rewards = []\n",
    "step = 0\n",
    "while not done:\n",
    "    print(\"Step: \", step)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=PARAM_device).unsqueeze(0)\n",
    "    # get the prediction from the model\n",
    "    with torch.no_grad():\n",
    "        QVals = policyNetV3(state_tensor)\n",
    "        print(\"QVals: \", QVals)\n",
    "        # get the best action from QVals\n",
    "        bestAction = torch.argmax(QVals).item()\n",
    "        print(\"best Action: \", bestAction)\n",
    "    # perform the action\n",
    "    nextState, r, done = env.takeAction(bestAction)\n",
    "    print(\"Done:\", done)\n",
    "    # print('State: ', state.cpu().numpy())\n",
    "    print('------------------------------')\n",
    "\n",
    "    # update the current state\n",
    "    state = nextState\n",
    "    rewards.append(r)\n",
    "    step+=1\n",
    "    # if step==5:\n",
    "    #     break\n",
    "    # break\n",
    "    # prevAct = bestAction\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# running simulation with 10 second timings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# traci.start([\"sumo-gui\", \"-c\", \"data/SingleIntersection.sumocfg\", \"--log\", \"logs/traciLog10Sec.yml\", \"--duration-log.statistics\", \"true\"])\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# for i in range(3600):\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     traci.simulationStep()\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtraci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\main.py:264\u001b[0m, in \u001b[0;36mclose\u001b[1;34m(wait)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclose\u001b[39m(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    Tells TraCI to close the connection.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\connection.py:398\u001b[0m, in \u001b[0;36mConnection.close\u001b[1;34m(self, wait)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremoveStepListener(listenerID)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCMD_CLOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\connection.py:232\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\connection.py:130\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recvExact()\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    }
   ],
   "source": [
    "# running simulation with 10 second timings\n",
    "# traci.start([\"sumo-gui\", \"-c\", \"data/SingleIntersection.sumocfg\", \"--log\", \"logs/traciLog10Sec.yml\", \"--duration-log.statistics\", \"true\"])\n",
    "# for i in range(3600):\n",
    "#     traci.simulationStep()\n",
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 109\u001b[0m, in \u001b[0;36mSUMOEnvironment.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m        This function closes the connection of traci with the sumo environment.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m        NOTE: After calling this function, you will need to reinitialize the object, as now the connection to SUMO has been closed for this. NEED TO FIND A BETTER WAY TO DO THIS.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     \u001b[43mtraci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\main.py:264\u001b[0m, in \u001b[0;36mclose\u001b[1;34m(wait)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclose\u001b[39m(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    261\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m    Tells TraCI to close the connection.\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\connection.py:398\u001b[0m, in \u001b[0;36mConnection.close\u001b[1;34m(self, wait)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremoveStepListener(listenerID)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCMD_CLOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\connection.py:232\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\SUMO\\tools\\traci\\connection.py:130\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recvExact()\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMiXKVT6sbVeg2NIagLouuz",
   "gpuType": "T4",
   "mount_file_id": "1qIgVMlxvTIfKlUHFBKyi86gvWzOLhcOn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
