{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb322ec",
   "metadata": {},
   "source": [
    "# Training the base model for single intersection traffic light management.\n",
    "This is my first attempt at sort of like a PoC/prototype for a single intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb1b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2f2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# # import cityflow\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "import copy\n",
    "# import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc99005",
   "metadata": {},
   "source": [
    "# Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a516d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityFlowEnv:\n",
    "    '''\n",
    "        This class is the environment implemented in cityflow for a single intersection.\n",
    "    '''\n",
    "    def __init__(self, maxSteps, configPath=os.path.join('generated', 'config.json'), numThreads=1):\n",
    "        # initializing the cityflow engine\n",
    "        self.engine = cityflow.Engine(configPath, thread_num=numThreads)\n",
    "        self.numSteps = 0 # to track how many steps have been taken\n",
    "        self.maxSteps = maxSteps # the maximum number of steps allowed\n",
    "    \n",
    "    def _getState(self):\n",
    "        '''\n",
    "            This function returns the state the environment is in right now\n",
    "        '''\n",
    "        # get lanecounts\n",
    "        laneCounts = self.engine.get_lane_vehicle_count()\n",
    "        # add to a dictionary and return\n",
    "        cumLaneLenghts = {'road_0_1_0':0, 'road_2_1_2':0, 'road_1_2_3':0, 'road_1_0_1':0}\n",
    "        for k,v in laneCounts.items():\n",
    "            if k.startswith('road_0_1_0'):\n",
    "                cumLaneLenghts['road_0_1_0'] += v\n",
    "            elif k.startswith('road_2_1_2'):\n",
    "                cumLaneLenghts['road_2_1_2'] += v\n",
    "            elif k.startswith('road_1_2_3'):\n",
    "                cumLaneLenghts['road_1_2_3'] += v\n",
    "            elif k.startswith('road_1_0_1'):\n",
    "                cumLaneLenghts['road_1_0_1'] += v\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return list(cumLaneLenghts.values())\n",
    "    \n",
    "    def _getReward(self):\n",
    "        '''\n",
    "            This function returns the reward after taking the current state\n",
    "        '''\n",
    "        # NOTE: reward will be generated after the action is done, so we need to implement the do_action and simulate traffic for the next 10 seconds\n",
    "        # after that, calculate the reward\n",
    "        # get the lanelengths\n",
    "        laneLengths = -1 * sum(self._getState())\n",
    "        return laneLengths\n",
    "    \n",
    "    def _peformAction(self):\n",
    "        '''\n",
    "            This function will take action, which is setting the traffic light to a specific phase.\n",
    "        '''\n",
    "        pass\n",
    "        # set trafficlight phase\n",
    "        # simulate for the next 10 seconds\n",
    "        self._step(10)\n",
    "\n",
    "    def _step(self, t=10):\n",
    "        '''\n",
    "            This function steps the environment for the next t seconds.\n",
    "        '''\n",
    "        # NOTE TO SELF: rn, the interval is hardcoded to 1 second, same as the config definition, REMEMBER to make this dynamic\n",
    "        finished = False\n",
    "        for i in range(t):\n",
    "            self.numSteps+=1\n",
    "            if self.numSteps==self.maxSteps:\n",
    "                finished = True\n",
    "                break\n",
    "            self.engine.next_step()\n",
    "        return finished\n",
    "\n",
    "    def take_action(self, action, t=10, intersection_id='intersection_1_1'):\n",
    "        '''\n",
    "            This is the main callable function for taking a step in the environment. It does the following:\n",
    "                1. takes the action.\n",
    "                2. simulates for the next t seconds.\n",
    "                3. gets the reward\n",
    "                4. get next state\n",
    "            Action will be the index of the tl phase for the intersection defined as defined in the roadnet file for that intersection\n",
    "        '''\n",
    "        # take action, set the tl phase to the provided index\n",
    "        self.engine.set_tl_phase(intersection_id, action)\n",
    "        # run the engine\n",
    "        finished = self._step(t)\n",
    "        # get the state\n",
    "        next_state = self._getState()\n",
    "        # get the reward\n",
    "        r = self._getReward()\n",
    "\n",
    "        return next_state, r, finished\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "            This function resets the environment to the original state.\n",
    "        '''\n",
    "        self.engine.reset()\n",
    "        self.numSteps = 0\n",
    "        # clearing the replay and the roadnetlog files\n",
    "        open(os.path.join('generated', 'GeneratedRoadNetLogExpt.json'), 'w').close()\n",
    "        open(os.path.join('generated', 'GeneratedReplayLogExpt.txt'), 'w').close()\n",
    "        return self._getState()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de97842",
   "metadata": {},
   "source": [
    "## Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3cf084e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityFlowEnv(10800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b7d11",
   "metadata": {},
   "source": [
    "# Defining the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e06a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "PARAM_learning_rate = 0.001\n",
    "PARAM_gamma = 0.8\n",
    "PARAM_epsilon = 1.0\n",
    "PARAM_epsilon_min = 0.01\n",
    "PARAM_epsilon_decay = 35000\n",
    "PARAM_batch_size = 64\n",
    "PARAM_target_update_freq = 10\n",
    "PARAM_memory_size = 10000\n",
    "PARAM_episodes = 500\n",
    "PARAM_TAU = 0.005\n",
    "# PARAM_device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc676202",
   "metadata": {},
   "source": [
    "# CREATING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37b9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''\n",
    "        This class defines the neural network used for the model.\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        # creating the layers\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00665f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the input and output dimensions\n",
    "input_dim, output_dim = 13, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe7ee5",
   "metadata": {},
   "source": [
    "### Creating the replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9de7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139593e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    '''\n",
    "        This class defines the replay memory\n",
    "    '''\n",
    "    def __init__(self, bufferSize):\n",
    "        self.bufferSize = bufferSize\n",
    "        self.memory = deque(maxlen=self.bufferSize)\n",
    "\n",
    "    def insert(self, state, action, reward, nextState, done):\n",
    "        self.memory.append((state, action, reward, nextState, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        sampleBatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, nextStates, done = zip(*sampleBatch)\n",
    "        return np.array(states), np.array(actions, dtype=int), np.array(rewards), np.array(nextStates), np.array(done)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833666e7",
   "metadata": {},
   "source": [
    "#### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b144ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectAction(state, epsilon):\n",
    "    '''\n",
    "        This function selects and returns an action using the epsilon greedy method.\n",
    "    '''\n",
    "    # select a random number between 0 and 1\n",
    "    rNum = random.random()\n",
    "    # print('Rnum: ', rNum)\n",
    "    if rNum<epsilon:\n",
    "        # explore, select a random action\n",
    "        return random.choice([0,1,2,3])\n",
    "    else: # exploit, get the best action\n",
    "        state = torch.tensor(state, device=PARAM_device, dtype=torch.float32).unsqueeze(0)\n",
    "        qVals = policyNet(state)\n",
    "        return torch.argmax(qVals).item() # the action that has the highest Q value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e1b03",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52ff4210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the epsilon value to use\n",
    "INUSE_epsilon = copy.deepcopy(PARAM_epsilon)\n",
    "memory = ReplayMemory(PARAM_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f89d715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyNet = DQN(input_dim, output_dim)\n",
    "targetNet = DQN(input_dim, output_dim)\n",
    "# loading the state dict from policy to target\n",
    "targetNet.load_state_dict(policyNet.state_dict())\n",
    "targetNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73f8c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.Adam(policyNet.parameters(), lr=PARAM_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "af8fe94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving the models to gpu\n",
    "policyNet.to(PARAM_device)\n",
    "targetNet.to(PARAM_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1e9aaba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Episode:  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  0\n",
      "250 Steps done!,  250\n",
      "250 Steps done!,  500\n",
      "250 Steps done!,  750\n",
      "250 Steps done!,  1000\n",
      "-------------------------------------------------------------\n",
      "Episode:  1\n",
      "250 Steps done!,  1250\n",
      "250 Steps done!,  1500\n",
      "250 Steps done!,  1750\n",
      "250 Steps done!,  2000\n",
      "-------------------------------------------------------------\n",
      "Episode:  2\n",
      "250 Steps done!,  2250\n",
      "250 Steps done!,  2500\n",
      "250 Steps done!,  2750\n",
      "250 Steps done!,  3000\n",
      "-------------------------------------------------------------\n",
      "Episode:  3\n",
      "250 Steps done!,  3250\n",
      "250 Steps done!,  3500\n",
      "250 Steps done!,  3750\n",
      "250 Steps done!,  4000\n",
      "250 Steps done!,  4250\n",
      "-------------------------------------------------------------\n",
      "Episode:  4\n",
      "250 Steps done!,  4500\n",
      "250 Steps done!,  4750\n",
      "250 Steps done!,  5000\n",
      "250 Steps done!,  5250\n",
      "-------------------------------------------------------------\n",
      "Episode:  5\n",
      "250 Steps done!,  5500\n",
      "250 Steps done!,  5750\n",
      "250 Steps done!,  6000\n",
      "250 Steps done!,  6250\n",
      "-------------------------------------------------------------\n",
      "Episode:  6\n",
      "250 Steps done!,  6500\n",
      "250 Steps done!,  6750\n",
      "250 Steps done!,  7000\n",
      "250 Steps done!,  7250\n",
      "-------------------------------------------------------------\n",
      "Episode:  7\n",
      "250 Steps done!,  7500\n",
      "250 Steps done!,  7750\n",
      "250 Steps done!,  8000\n",
      "250 Steps done!,  8250\n",
      "250 Steps done!,  8500\n",
      "-------------------------------------------------------------\n",
      "Episode:  8\n",
      "250 Steps done!,  8750\n",
      "250 Steps done!,  9000\n",
      "250 Steps done!,  9250\n",
      "250 Steps done!,  9500\n",
      "-------------------------------------------------------------\n",
      "Episode:  9\n",
      "250 Steps done!,  9750\n",
      "250 Steps done!,  10000\n",
      "250 Steps done!,  10250\n",
      "250 Steps done!,  10500\n",
      "-------------------------------------------------------------\n",
      "Episode:  10\n",
      "250 Steps done!,  10750\n",
      "250 Steps done!,  11000\n",
      "250 Steps done!,  11250\n",
      "250 Steps done!,  11500\n",
      "250 Steps done!,  11750\n",
      "-------------------------------------------------------------\n",
      "Episode:  11\n",
      "250 Steps done!,  12000\n",
      "250 Steps done!,  12250\n",
      "250 Steps done!,  12500\n",
      "250 Steps done!,  12750\n",
      "-------------------------------------------------------------\n",
      "Episode:  12\n",
      "250 Steps done!,  13000\n",
      "250 Steps done!,  13250\n",
      "250 Steps done!,  13500\n",
      "250 Steps done!,  13750\n",
      "-------------------------------------------------------------\n",
      "Episode:  13\n",
      "250 Steps done!,  14000\n",
      "250 Steps done!,  14250\n",
      "250 Steps done!,  14500\n",
      "250 Steps done!,  14750\n",
      "250 Steps done!,  15000\n",
      "-------------------------------------------------------------\n",
      "Episode:  14\n",
      "250 Steps done!,  15250\n",
      "250 Steps done!,  15500\n",
      "250 Steps done!,  15750\n",
      "250 Steps done!,  16000\n",
      "-------------------------------------------------------------\n",
      "Episode:  15\n",
      "250 Steps done!,  16250\n",
      "250 Steps done!,  16500\n",
      "250 Steps done!,  16750\n",
      "250 Steps done!,  17000\n",
      "-------------------------------------------------------------\n",
      "Episode:  16\n",
      "250 Steps done!,  17250\n",
      "250 Steps done!,  17500\n",
      "250 Steps done!,  17750\n",
      "250 Steps done!,  18000\n",
      "250 Steps done!,  18250\n",
      "-------------------------------------------------------------\n",
      "Episode:  17\n",
      "250 Steps done!,  18500\n",
      "250 Steps done!,  18750\n",
      "250 Steps done!,  19000\n",
      "250 Steps done!,  19250\n",
      "-------------------------------------------------------------\n",
      "Episode:  18\n",
      "250 Steps done!,  19500\n",
      "250 Steps done!,  19750\n",
      "250 Steps done!,  20000\n",
      "250 Steps done!,  20250\n",
      "-------------------------------------------------------------\n",
      "Episode:  19\n",
      "250 Steps done!,  20500\n",
      "250 Steps done!,  20750\n",
      "250 Steps done!,  21000\n",
      "250 Steps done!,  21250\n",
      "250 Steps done!,  21500\n",
      "-------------------------------------------------------------\n",
      "Episode:  20\n",
      "250 Steps done!,  21750\n",
      "250 Steps done!,  22000\n",
      "250 Steps done!,  22250\n",
      "250 Steps done!,  22500\n",
      "-------------------------------------------------------------\n",
      "Episode:  21\n",
      "250 Steps done!,  22750\n",
      "250 Steps done!,  23000\n",
      "250 Steps done!,  23250\n",
      "250 Steps done!,  23500\n",
      "-------------------------------------------------------------\n",
      "Episode:  22\n",
      "250 Steps done!,  23750\n",
      "250 Steps done!,  24000\n",
      "250 Steps done!,  24250\n",
      "250 Steps done!,  24500\n",
      "250 Steps done!,  24750\n",
      "-------------------------------------------------------------\n",
      "Episode:  23\n",
      "250 Steps done!,  25000\n",
      "250 Steps done!,  25250\n",
      "250 Steps done!,  25500\n",
      "250 Steps done!,  25750\n",
      "-------------------------------------------------------------\n",
      "Episode:  24\n",
      "250 Steps done!,  26000\n",
      "250 Steps done!,  26250\n",
      "250 Steps done!,  26500\n",
      "250 Steps done!,  26750\n",
      "-------------------------------------------------------------\n",
      "Episode:  25\n",
      "250 Steps done!,  27000\n",
      "250 Steps done!,  27250\n",
      "250 Steps done!,  27500\n",
      "250 Steps done!,  27750\n",
      "250 Steps done!,  28000\n",
      "-------------------------------------------------------------\n",
      "Episode:  26\n",
      "250 Steps done!,  28250\n",
      "250 Steps done!,  28500\n",
      "250 Steps done!,  28750\n",
      "250 Steps done!,  29000\n",
      "-------------------------------------------------------------\n",
      "Episode:  27\n",
      "250 Steps done!,  29250\n",
      "250 Steps done!,  29500\n",
      "250 Steps done!,  29750\n",
      "250 Steps done!,  30000\n",
      "-------------------------------------------------------------\n",
      "Episode:  28\n",
      "250 Steps done!,  30250\n",
      "250 Steps done!,  30500\n",
      "250 Steps done!,  30750\n",
      "250 Steps done!,  31000\n",
      "250 Steps done!,  31250\n",
      "-------------------------------------------------------------\n",
      "Episode:  29\n",
      "250 Steps done!,  31500\n",
      "250 Steps done!,  31750\n",
      "250 Steps done!,  32000\n",
      "250 Steps done!,  32250\n",
      "-------------------------------------------------------------\n",
      "Episode:  30\n",
      "250 Steps done!,  32500\n",
      "250 Steps done!,  32750\n",
      "250 Steps done!,  33000\n",
      "250 Steps done!,  33250\n",
      "-------------------------------------------------------------\n",
      "Episode:  31\n",
      "250 Steps done!,  33500\n",
      "250 Steps done!,  33750\n",
      "250 Steps done!,  34000\n",
      "250 Steps done!,  34250\n",
      "-------------------------------------------------------------\n",
      "Episode:  32\n",
      "250 Steps done!,  34500\n",
      "250 Steps done!,  34750\n",
      "250 Steps done!,  35000\n",
      "250 Steps done!,  35250\n",
      "250 Steps done!,  35500\n",
      "-------------------------------------------------------------\n",
      "Episode:  33\n",
      "250 Steps done!,  35750\n",
      "250 Steps done!,  36000\n",
      "250 Steps done!,  36250\n",
      "250 Steps done!,  36500\n",
      "-------------------------------------------------------------\n",
      "Episode:  34\n",
      "250 Steps done!,  36750\n",
      "250 Steps done!,  37000\n",
      "250 Steps done!,  37250\n",
      "250 Steps done!,  37500\n",
      "-------------------------------------------------------------\n",
      "Episode:  35\n",
      "250 Steps done!,  37750\n",
      "250 Steps done!,  38000\n",
      "250 Steps done!,  38250\n",
      "250 Steps done!,  38500\n",
      "250 Steps done!,  38750\n",
      "-------------------------------------------------------------\n",
      "Episode:  36\n",
      "250 Steps done!,  39000\n",
      "250 Steps done!,  39250\n",
      "250 Steps done!,  39500\n",
      "250 Steps done!,  39750\n",
      "-------------------------------------------------------------\n",
      "Episode:  37\n",
      "250 Steps done!,  40000\n",
      "250 Steps done!,  40250\n",
      "250 Steps done!,  40500\n",
      "250 Steps done!,  40750\n",
      "-------------------------------------------------------------\n",
      "Episode:  38\n",
      "250 Steps done!,  41000\n",
      "250 Steps done!,  41250\n",
      "250 Steps done!,  41500\n",
      "250 Steps done!,  41750\n",
      "250 Steps done!,  42000\n",
      "-------------------------------------------------------------\n",
      "Episode:  39\n",
      "250 Steps done!,  42250\n",
      "250 Steps done!,  42500\n",
      "250 Steps done!,  42750\n",
      "250 Steps done!,  43000\n",
      "-------------------------------------------------------------\n",
      "Episode:  40\n",
      "250 Steps done!,  43250\n",
      "250 Steps done!,  43500\n",
      "250 Steps done!,  43750\n",
      "250 Steps done!,  44000\n",
      "-------------------------------------------------------------\n",
      "Episode:  41\n",
      "250 Steps done!,  44250\n",
      "250 Steps done!,  44500\n",
      "250 Steps done!,  44750\n",
      "250 Steps done!,  45000\n",
      "250 Steps done!,  45250\n",
      "-------------------------------------------------------------\n",
      "Episode:  42\n",
      "250 Steps done!,  45500\n",
      "250 Steps done!,  45750\n",
      "250 Steps done!,  46000\n",
      "250 Steps done!,  46250\n",
      "-------------------------------------------------------------\n",
      "Episode:  43\n",
      "250 Steps done!,  46500\n",
      "250 Steps done!,  46750\n",
      "250 Steps done!,  47000\n",
      "250 Steps done!,  47250\n",
      "-------------------------------------------------------------\n",
      "Episode:  44\n",
      "250 Steps done!,  47500\n",
      "250 Steps done!,  47750\n",
      "250 Steps done!,  48000\n",
      "250 Steps done!,  48250\n",
      "250 Steps done!,  48500\n",
      "-------------------------------------------------------------\n",
      "Episode:  45\n",
      "250 Steps done!,  48750\n",
      "250 Steps done!,  49000\n",
      "250 Steps done!,  49250\n",
      "250 Steps done!,  49500\n",
      "-------------------------------------------------------------\n",
      "Episode:  46\n",
      "250 Steps done!,  49750\n",
      "250 Steps done!,  50000\n",
      "250 Steps done!,  50250\n",
      "250 Steps done!,  50500\n",
      "-------------------------------------------------------------\n",
      "Episode:  47\n",
      "250 Steps done!,  50750\n",
      "250 Steps done!,  51000\n",
      "250 Steps done!,  51250\n",
      "250 Steps done!,  51500\n",
      "250 Steps done!,  51750\n",
      "-------------------------------------------------------------\n",
      "Episode:  48\n",
      "250 Steps done!,  52000\n",
      "250 Steps done!,  52250\n",
      "250 Steps done!,  52500\n",
      "250 Steps done!,  52750\n",
      "-------------------------------------------------------------\n",
      "Episode:  49\n",
      "250 Steps done!,  53000\n",
      "250 Steps done!,  53250\n",
      "250 Steps done!,  53500\n",
      "250 Steps done!,  53750\n",
      "-------------------------------------------------------------\n",
      "Episode:  50\n",
      "250 Steps done!,  54000\n",
      "250 Steps done!,  54250\n",
      "250 Steps done!,  54500\n",
      "250 Steps done!,  54750\n",
      "250 Steps done!,  55000\n",
      "-------------------------------------------------------------\n",
      "Episode:  51\n",
      "250 Steps done!,  55250\n",
      "250 Steps done!,  55500\n",
      "250 Steps done!,  55750\n",
      "250 Steps done!,  56000\n",
      "-------------------------------------------------------------\n",
      "Episode:  52\n",
      "250 Steps done!,  56250\n",
      "250 Steps done!,  56500\n",
      "250 Steps done!,  56750\n",
      "250 Steps done!,  57000\n",
      "-------------------------------------------------------------\n",
      "Episode:  53\n",
      "250 Steps done!,  57250\n",
      "250 Steps done!,  57500\n",
      "250 Steps done!,  57750\n",
      "250 Steps done!,  58000\n",
      "250 Steps done!,  58250\n",
      "-------------------------------------------------------------\n",
      "Episode:  54\n",
      "250 Steps done!,  58500\n",
      "250 Steps done!,  58750\n",
      "250 Steps done!,  59000\n",
      "250 Steps done!,  59250\n",
      "-------------------------------------------------------------\n",
      "Episode:  55\n",
      "250 Steps done!,  59500\n",
      "250 Steps done!,  59750\n",
      "250 Steps done!,  60000\n",
      "250 Steps done!,  60250\n",
      "-------------------------------------------------------------\n",
      "Episode:  56\n",
      "250 Steps done!,  60500\n",
      "250 Steps done!,  60750\n",
      "250 Steps done!,  61000\n",
      "250 Steps done!,  61250\n",
      "-------------------------------------------------------------\n",
      "Episode:  57\n",
      "250 Steps done!,  61500\n",
      "250 Steps done!,  61750\n",
      "250 Steps done!,  62000\n",
      "250 Steps done!,  62250\n",
      "250 Steps done!,  62500\n",
      "-------------------------------------------------------------\n",
      "Episode:  58\n",
      "250 Steps done!,  62750\n",
      "250 Steps done!,  63000\n",
      "250 Steps done!,  63250\n",
      "250 Steps done!,  63500\n",
      "-------------------------------------------------------------\n",
      "Episode:  59\n",
      "250 Steps done!,  63750\n",
      "250 Steps done!,  64000\n",
      "250 Steps done!,  64250\n",
      "250 Steps done!,  64500\n",
      "-------------------------------------------------------------\n",
      "Episode:  60\n",
      "250 Steps done!,  64750\n",
      "250 Steps done!,  65000\n",
      "250 Steps done!,  65250\n",
      "250 Steps done!,  65500\n",
      "250 Steps done!,  65750\n",
      "-------------------------------------------------------------\n",
      "Episode:  61\n",
      "250 Steps done!,  66000\n",
      "250 Steps done!,  66250\n",
      "250 Steps done!,  66500\n",
      "250 Steps done!,  66750\n",
      "-------------------------------------------------------------\n",
      "Episode:  62\n",
      "250 Steps done!,  67000\n",
      "250 Steps done!,  67250\n",
      "250 Steps done!,  67500\n",
      "250 Steps done!,  67750\n",
      "-------------------------------------------------------------\n",
      "Episode:  63\n",
      "250 Steps done!,  68000\n",
      "250 Steps done!,  68250\n",
      "250 Steps done!,  68500\n",
      "250 Steps done!,  68750\n",
      "250 Steps done!,  69000\n",
      "-------------------------------------------------------------\n",
      "Episode:  64\n",
      "250 Steps done!,  69250\n",
      "250 Steps done!,  69500\n",
      "250 Steps done!,  69750\n",
      "250 Steps done!,  70000\n",
      "-------------------------------------------------------------\n",
      "Episode:  65\n",
      "250 Steps done!,  70250\n",
      "250 Steps done!,  70500\n",
      "250 Steps done!,  70750\n",
      "250 Steps done!,  71000\n",
      "-------------------------------------------------------------\n",
      "Episode:  66\n",
      "250 Steps done!,  71250\n",
      "250 Steps done!,  71500\n",
      "250 Steps done!,  71750\n",
      "250 Steps done!,  72000\n",
      "250 Steps done!,  72250\n",
      "-------------------------------------------------------------\n",
      "Episode:  67\n",
      "250 Steps done!,  72500\n",
      "250 Steps done!,  72750\n",
      "250 Steps done!,  73000\n",
      "250 Steps done!,  73250\n",
      "-------------------------------------------------------------\n",
      "Episode:  68\n",
      "250 Steps done!,  73500\n",
      "250 Steps done!,  73750\n",
      "250 Steps done!,  74000\n",
      "250 Steps done!,  74250\n",
      "-------------------------------------------------------------\n",
      "Episode:  69\n",
      "250 Steps done!,  74500\n",
      "250 Steps done!,  74750\n",
      "250 Steps done!,  75000\n",
      "250 Steps done!,  75250\n",
      "250 Steps done!,  75500\n",
      "-------------------------------------------------------------\n",
      "Episode:  70\n",
      "250 Steps done!,  75750\n",
      "250 Steps done!,  76000\n",
      "250 Steps done!,  76250\n",
      "250 Steps done!,  76500\n",
      "-------------------------------------------------------------\n",
      "Episode:  71\n",
      "250 Steps done!,  76750\n",
      "250 Steps done!,  77000\n",
      "250 Steps done!,  77250\n",
      "250 Steps done!,  77500\n",
      "-------------------------------------------------------------\n",
      "Episode:  72\n",
      "250 Steps done!,  77750\n",
      "250 Steps done!,  78000\n",
      "250 Steps done!,  78250\n",
      "250 Steps done!,  78500\n",
      "250 Steps done!,  78750\n",
      "-------------------------------------------------------------\n",
      "Episode:  73\n",
      "250 Steps done!,  79000\n",
      "250 Steps done!,  79250\n",
      "250 Steps done!,  79500\n",
      "250 Steps done!,  79750\n",
      "-------------------------------------------------------------\n",
      "Episode:  74\n",
      "250 Steps done!,  80000\n",
      "250 Steps done!,  80250\n",
      "250 Steps done!,  80500\n",
      "250 Steps done!,  80750\n",
      "-------------------------------------------------------------\n",
      "Episode:  75\n",
      "250 Steps done!,  81000\n",
      "250 Steps done!,  81250\n",
      "250 Steps done!,  81500\n",
      "250 Steps done!,  81750\n",
      "250 Steps done!,  82000\n",
      "-------------------------------------------------------------\n",
      "Episode:  76\n",
      "250 Steps done!,  82250\n",
      "250 Steps done!,  82500\n",
      "250 Steps done!,  82750\n",
      "250 Steps done!,  83000\n",
      "-------------------------------------------------------------\n",
      "Episode:  77\n",
      "250 Steps done!,  83250\n",
      "250 Steps done!,  83500\n",
      "250 Steps done!,  83750\n",
      "250 Steps done!,  84000\n",
      "-------------------------------------------------------------\n",
      "Episode:  78\n",
      "250 Steps done!,  84250\n",
      "250 Steps done!,  84500\n",
      "250 Steps done!,  84750\n",
      "250 Steps done!,  85000\n",
      "250 Steps done!,  85250\n",
      "-------------------------------------------------------------\n",
      "Episode:  79\n",
      "250 Steps done!,  85500\n",
      "250 Steps done!,  85750\n",
      "250 Steps done!,  86000\n",
      "250 Steps done!,  86250\n",
      "-------------------------------------------------------------\n",
      "Episode:  80\n",
      "250 Steps done!,  86500\n",
      "250 Steps done!,  86750\n",
      "250 Steps done!,  87000\n",
      "250 Steps done!,  87250\n",
      "-------------------------------------------------------------\n",
      "Episode:  81\n",
      "250 Steps done!,  87500\n",
      "250 Steps done!,  87750\n",
      "250 Steps done!,  88000\n",
      "250 Steps done!,  88250\n",
      "-------------------------------------------------------------\n",
      "Episode:  82\n",
      "250 Steps done!,  88500\n",
      "250 Steps done!,  88750\n",
      "250 Steps done!,  89000\n",
      "250 Steps done!,  89250\n",
      "250 Steps done!,  89500\n",
      "-------------------------------------------------------------\n",
      "Episode:  83\n",
      "250 Steps done!,  89750\n",
      "250 Steps done!,  90000\n",
      "250 Steps done!,  90250\n",
      "250 Steps done!,  90500\n",
      "-------------------------------------------------------------\n",
      "Episode:  84\n",
      "250 Steps done!,  90750\n",
      "250 Steps done!,  91000\n",
      "250 Steps done!,  91250\n",
      "250 Steps done!,  91500\n",
      "-------------------------------------------------------------\n",
      "Episode:  85\n",
      "250 Steps done!,  91750\n",
      "250 Steps done!,  92000\n",
      "250 Steps done!,  92250\n",
      "250 Steps done!,  92500\n",
      "250 Steps done!,  92750\n",
      "-------------------------------------------------------------\n",
      "Episode:  86\n",
      "250 Steps done!,  93000\n",
      "250 Steps done!,  93250\n",
      "250 Steps done!,  93500\n",
      "250 Steps done!,  93750\n",
      "-------------------------------------------------------------\n",
      "Episode:  87\n",
      "250 Steps done!,  94000\n",
      "250 Steps done!,  94250\n",
      "250 Steps done!,  94500\n",
      "250 Steps done!,  94750\n",
      "-------------------------------------------------------------\n",
      "Episode:  88\n",
      "250 Steps done!,  95000\n",
      "250 Steps done!,  95250\n",
      "250 Steps done!,  95500\n",
      "250 Steps done!,  95750\n",
      "250 Steps done!,  96000\n",
      "-------------------------------------------------------------\n",
      "Episode:  89\n",
      "250 Steps done!,  96250\n",
      "250 Steps done!,  96500\n",
      "250 Steps done!,  96750\n",
      "250 Steps done!,  97000\n",
      "-------------------------------------------------------------\n",
      "Episode:  90\n",
      "250 Steps done!,  97250\n",
      "250 Steps done!,  97500\n",
      "250 Steps done!,  97750\n",
      "250 Steps done!,  98000\n",
      "-------------------------------------------------------------\n",
      "Episode:  91\n",
      "250 Steps done!,  98250\n",
      "250 Steps done!,  98500\n",
      "250 Steps done!,  98750\n",
      "250 Steps done!,  99000\n",
      "250 Steps done!,  99250\n",
      "-------------------------------------------------------------\n",
      "Episode:  92\n",
      "250 Steps done!,  99500\n",
      "250 Steps done!,  99750\n",
      "250 Steps done!,  100000\n",
      "250 Steps done!,  100250\n",
      "-------------------------------------------------------------\n",
      "Episode:  93\n",
      "250 Steps done!,  100500\n",
      "250 Steps done!,  100750\n",
      "250 Steps done!,  101000\n",
      "250 Steps done!,  101250\n",
      "-------------------------------------------------------------\n",
      "Episode:  94\n",
      "250 Steps done!,  101500\n",
      "250 Steps done!,  101750\n",
      "250 Steps done!,  102000\n",
      "250 Steps done!,  102250\n",
      "250 Steps done!,  102500\n",
      "-------------------------------------------------------------\n",
      "Episode:  95\n",
      "250 Steps done!,  102750\n",
      "250 Steps done!,  103000\n",
      "250 Steps done!,  103250\n",
      "250 Steps done!,  103500\n",
      "-------------------------------------------------------------\n",
      "Episode:  96\n",
      "250 Steps done!,  103750\n",
      "250 Steps done!,  104000\n",
      "250 Steps done!,  104250\n",
      "250 Steps done!,  104500\n",
      "-------------------------------------------------------------\n",
      "Episode:  97\n",
      "250 Steps done!,  104750\n",
      "250 Steps done!,  105000\n",
      "250 Steps done!,  105250\n",
      "250 Steps done!,  105500\n",
      "250 Steps done!,  105750\n",
      "-------------------------------------------------------------\n",
      "Episode:  98\n",
      "250 Steps done!,  106000\n",
      "250 Steps done!,  106250\n",
      "250 Steps done!,  106500\n",
      "250 Steps done!,  106750\n",
      "-------------------------------------------------------------\n",
      "Episode:  99\n",
      "250 Steps done!,  107000\n",
      "250 Steps done!,  107250\n",
      "250 Steps done!,  107500\n",
      "250 Steps done!,  107750\n",
      "-------------------------------------------------------------\n",
      "Episode:  100\n",
      "250 Steps done!,  108000\n",
      "250 Steps done!,  108250\n",
      "250 Steps done!,  108500\n",
      "250 Steps done!,  108750\n",
      "250 Steps done!,  109000\n",
      "-------------------------------------------------------------\n",
      "Episode:  101\n",
      "250 Steps done!,  109250\n",
      "250 Steps done!,  109500\n",
      "250 Steps done!,  109750\n",
      "250 Steps done!,  110000\n",
      "-------------------------------------------------------------\n",
      "Episode:  102\n",
      "250 Steps done!,  110250\n",
      "250 Steps done!,  110500\n",
      "250 Steps done!,  110750\n",
      "250 Steps done!,  111000\n",
      "-------------------------------------------------------------\n",
      "Episode:  103\n",
      "250 Steps done!,  111250\n",
      "250 Steps done!,  111500\n",
      "250 Steps done!,  111750\n",
      "250 Steps done!,  112000\n",
      "250 Steps done!,  112250\n",
      "-------------------------------------------------------------\n",
      "Episode:  104\n",
      "250 Steps done!,  112500\n",
      "250 Steps done!,  112750\n",
      "250 Steps done!,  113000\n",
      "250 Steps done!,  113250\n",
      "-------------------------------------------------------------\n",
      "Episode:  105\n",
      "250 Steps done!,  113500\n",
      "250 Steps done!,  113750\n",
      "250 Steps done!,  114000\n",
      "250 Steps done!,  114250\n",
      "-------------------------------------------------------------\n",
      "Episode:  106\n",
      "250 Steps done!,  114500\n",
      "250 Steps done!,  114750\n",
      "250 Steps done!,  115000\n",
      "250 Steps done!,  115250\n",
      "-------------------------------------------------------------\n",
      "Episode:  107\n",
      "250 Steps done!,  115500\n",
      "250 Steps done!,  115750\n",
      "250 Steps done!,  116000\n",
      "250 Steps done!,  116250\n",
      "250 Steps done!,  116500\n",
      "-------------------------------------------------------------\n",
      "Episode:  108\n",
      "250 Steps done!,  116750\n",
      "250 Steps done!,  117000\n",
      "250 Steps done!,  117250\n",
      "250 Steps done!,  117500\n",
      "-------------------------------------------------------------\n",
      "Episode:  109\n",
      "250 Steps done!,  117750\n",
      "250 Steps done!,  118000\n",
      "250 Steps done!,  118250\n",
      "250 Steps done!,  118500\n",
      "-------------------------------------------------------------\n",
      "Episode:  110\n",
      "250 Steps done!,  118750\n",
      "250 Steps done!,  119000\n",
      "250 Steps done!,  119250\n",
      "250 Steps done!,  119500\n",
      "250 Steps done!,  119750\n",
      "-------------------------------------------------------------\n",
      "Episode:  111\n",
      "250 Steps done!,  120000\n",
      "250 Steps done!,  120250\n",
      "250 Steps done!,  120500\n",
      "250 Steps done!,  120750\n",
      "-------------------------------------------------------------\n",
      "Episode:  112\n",
      "250 Steps done!,  121000\n",
      "250 Steps done!,  121250\n",
      "250 Steps done!,  121500\n",
      "250 Steps done!,  121750\n",
      "-------------------------------------------------------------\n",
      "Episode:  113\n",
      "250 Steps done!,  122000\n",
      "250 Steps done!,  122250\n",
      "250 Steps done!,  122500\n",
      "250 Steps done!,  122750\n",
      "250 Steps done!,  123000\n",
      "-------------------------------------------------------------\n",
      "Episode:  114\n",
      "250 Steps done!,  123250\n",
      "250 Steps done!,  123500\n",
      "250 Steps done!,  123750\n",
      "250 Steps done!,  124000\n",
      "-------------------------------------------------------------\n",
      "Episode:  115\n",
      "250 Steps done!,  124250\n",
      "250 Steps done!,  124500\n",
      "250 Steps done!,  124750\n",
      "250 Steps done!,  125000\n",
      "-------------------------------------------------------------\n",
      "Episode:  116\n",
      "250 Steps done!,  125250\n",
      "250 Steps done!,  125500\n",
      "250 Steps done!,  125750\n",
      "250 Steps done!,  126000\n",
      "250 Steps done!,  126250\n",
      "-------------------------------------------------------------\n",
      "Episode:  117\n",
      "250 Steps done!,  126500\n",
      "250 Steps done!,  126750\n",
      "250 Steps done!,  127000\n",
      "250 Steps done!,  127250\n",
      "-------------------------------------------------------------\n",
      "Episode:  118\n",
      "250 Steps done!,  127500\n",
      "250 Steps done!,  127750\n",
      "250 Steps done!,  128000\n",
      "250 Steps done!,  128250\n",
      "-------------------------------------------------------------\n",
      "Episode:  119\n",
      "250 Steps done!,  128500\n",
      "250 Steps done!,  128750\n",
      "250 Steps done!,  129000\n",
      "250 Steps done!,  129250\n",
      "250 Steps done!,  129500\n",
      "-------------------------------------------------------------\n",
      "Episode:  120\n",
      "250 Steps done!,  129750\n",
      "250 Steps done!,  130000\n",
      "250 Steps done!,  130250\n",
      "250 Steps done!,  130500\n",
      "-------------------------------------------------------------\n",
      "Episode:  121\n",
      "250 Steps done!,  130750\n",
      "250 Steps done!,  131000\n",
      "250 Steps done!,  131250\n",
      "250 Steps done!,  131500\n",
      "-------------------------------------------------------------\n",
      "Episode:  122\n",
      "250 Steps done!,  131750\n",
      "250 Steps done!,  132000\n",
      "250 Steps done!,  132250\n",
      "250 Steps done!,  132500\n",
      "250 Steps done!,  132750\n",
      "-------------------------------------------------------------\n",
      "Episode:  123\n",
      "250 Steps done!,  133000\n",
      "250 Steps done!,  133250\n",
      "250 Steps done!,  133500\n",
      "250 Steps done!,  133750\n",
      "-------------------------------------------------------------\n",
      "Episode:  124\n",
      "250 Steps done!,  134000\n",
      "250 Steps done!,  134250\n",
      "250 Steps done!,  134500\n",
      "250 Steps done!,  134750\n",
      "-------------------------------------------------------------\n",
      "Episode:  125\n",
      "250 Steps done!,  135000\n",
      "250 Steps done!,  135250\n",
      "250 Steps done!,  135500\n",
      "250 Steps done!,  135750\n",
      "250 Steps done!,  136000\n",
      "-------------------------------------------------------------\n",
      "Episode:  126\n",
      "250 Steps done!,  136250\n",
      "250 Steps done!,  136500\n",
      "250 Steps done!,  136750\n",
      "250 Steps done!,  137000\n",
      "-------------------------------------------------------------\n",
      "Episode:  127\n",
      "250 Steps done!,  137250\n",
      "250 Steps done!,  137500\n",
      "250 Steps done!,  137750\n",
      "250 Steps done!,  138000\n",
      "-------------------------------------------------------------\n",
      "Episode:  128\n",
      "250 Steps done!,  138250\n",
      "250 Steps done!,  138500\n",
      "250 Steps done!,  138750\n",
      "250 Steps done!,  139000\n",
      "250 Steps done!,  139250\n",
      "-------------------------------------------------------------\n",
      "Episode:  129\n",
      "250 Steps done!,  139500\n",
      "250 Steps done!,  139750\n",
      "250 Steps done!,  140000\n",
      "250 Steps done!,  140250\n",
      "-------------------------------------------------------------\n",
      "Episode:  130\n",
      "250 Steps done!,  140500\n",
      "250 Steps done!,  140750\n",
      "250 Steps done!,  141000\n",
      "250 Steps done!,  141250\n",
      "-------------------------------------------------------------\n",
      "Episode:  131\n",
      "250 Steps done!,  141500\n",
      "250 Steps done!,  141750\n",
      "250 Steps done!,  142000\n",
      "250 Steps done!,  142250\n",
      "-------------------------------------------------------------\n",
      "Episode:  132\n",
      "250 Steps done!,  142500\n",
      "250 Steps done!,  142750\n",
      "250 Steps done!,  143000\n",
      "250 Steps done!,  143250\n",
      "250 Steps done!,  143500\n",
      "-------------------------------------------------------------\n",
      "Episode:  133\n",
      "250 Steps done!,  143750\n",
      "250 Steps done!,  144000\n",
      "250 Steps done!,  144250\n",
      "250 Steps done!,  144500\n",
      "-------------------------------------------------------------\n",
      "Episode:  134\n",
      "250 Steps done!,  144750\n",
      "250 Steps done!,  145000\n",
      "250 Steps done!,  145250\n",
      "250 Steps done!,  145500\n",
      "-------------------------------------------------------------\n",
      "Episode:  135\n",
      "250 Steps done!,  145750\n",
      "250 Steps done!,  146000\n",
      "250 Steps done!,  146250\n",
      "250 Steps done!,  146500\n",
      "250 Steps done!,  146750\n",
      "-------------------------------------------------------------\n",
      "Episode:  136\n",
      "250 Steps done!,  147000\n",
      "250 Steps done!,  147250\n",
      "250 Steps done!,  147500\n",
      "250 Steps done!,  147750\n",
      "-------------------------------------------------------------\n",
      "Episode:  137\n",
      "250 Steps done!,  148000\n",
      "250 Steps done!,  148250\n",
      "250 Steps done!,  148500\n",
      "250 Steps done!,  148750\n",
      "-------------------------------------------------------------\n",
      "Episode:  138\n",
      "250 Steps done!,  149000\n",
      "250 Steps done!,  149250\n",
      "250 Steps done!,  149500\n",
      "250 Steps done!,  149750\n",
      "250 Steps done!,  150000\n",
      "-------------------------------------------------------------\n",
      "Episode:  139\n",
      "250 Steps done!,  150250\n",
      "250 Steps done!,  150500\n",
      "250 Steps done!,  150750\n",
      "250 Steps done!,  151000\n",
      "-------------------------------------------------------------\n",
      "Episode:  140\n",
      "250 Steps done!,  151250\n",
      "250 Steps done!,  151500\n",
      "250 Steps done!,  151750\n",
      "250 Steps done!,  152000\n",
      "-------------------------------------------------------------\n",
      "Episode:  141\n",
      "250 Steps done!,  152250\n",
      "250 Steps done!,  152500\n",
      "250 Steps done!,  152750\n",
      "250 Steps done!,  153000\n",
      "250 Steps done!,  153250\n",
      "-------------------------------------------------------------\n",
      "Episode:  142\n",
      "250 Steps done!,  153500\n",
      "250 Steps done!,  153750\n",
      "250 Steps done!,  154000\n",
      "250 Steps done!,  154250\n",
      "-------------------------------------------------------------\n",
      "Episode:  143\n",
      "250 Steps done!,  154500\n",
      "250 Steps done!,  154750\n",
      "250 Steps done!,  155000\n",
      "250 Steps done!,  155250\n",
      "-------------------------------------------------------------\n",
      "Episode:  144\n",
      "250 Steps done!,  155500\n",
      "250 Steps done!,  155750\n",
      "250 Steps done!,  156000\n",
      "250 Steps done!,  156250\n",
      "250 Steps done!,  156500\n",
      "-------------------------------------------------------------\n",
      "Episode:  145\n",
      "250 Steps done!,  156750\n",
      "250 Steps done!,  157000\n",
      "250 Steps done!,  157250\n",
      "250 Steps done!,  157500\n",
      "-------------------------------------------------------------\n",
      "Episode:  146\n",
      "250 Steps done!,  157750\n",
      "250 Steps done!,  158000\n",
      "250 Steps done!,  158250\n",
      "250 Steps done!,  158500\n",
      "-------------------------------------------------------------\n",
      "Episode:  147\n",
      "250 Steps done!,  158750\n",
      "250 Steps done!,  159000\n",
      "250 Steps done!,  159250\n",
      "250 Steps done!,  159500\n",
      "250 Steps done!,  159750\n",
      "-------------------------------------------------------------\n",
      "Episode:  148\n",
      "250 Steps done!,  160000\n",
      "250 Steps done!,  160250\n",
      "250 Steps done!,  160500\n",
      "250 Steps done!,  160750\n",
      "-------------------------------------------------------------\n",
      "Episode:  149\n",
      "250 Steps done!,  161000\n",
      "250 Steps done!,  161250\n",
      "250 Steps done!,  161500\n",
      "250 Steps done!,  161750\n",
      "-------------------------------------------------------------\n",
      "Episode:  150\n",
      "250 Steps done!,  162000\n",
      "250 Steps done!,  162250\n",
      "250 Steps done!,  162500\n",
      "250 Steps done!,  162750\n",
      "250 Steps done!,  163000\n",
      "-------------------------------------------------------------\n",
      "Episode:  151\n",
      "250 Steps done!,  163250\n",
      "250 Steps done!,  163500\n",
      "250 Steps done!,  163750\n",
      "250 Steps done!,  164000\n",
      "-------------------------------------------------------------\n",
      "Episode:  152\n",
      "250 Steps done!,  164250\n",
      "250 Steps done!,  164500\n",
      "250 Steps done!,  164750\n",
      "250 Steps done!,  165000\n",
      "-------------------------------------------------------------\n",
      "Episode:  153\n",
      "250 Steps done!,  165250\n",
      "250 Steps done!,  165500\n",
      "250 Steps done!,  165750\n",
      "250 Steps done!,  166000\n",
      "250 Steps done!,  166250\n",
      "-------------------------------------------------------------\n",
      "Episode:  154\n",
      "250 Steps done!,  166500\n",
      "250 Steps done!,  166750\n",
      "250 Steps done!,  167000\n",
      "250 Steps done!,  167250\n",
      "-------------------------------------------------------------\n",
      "Episode:  155\n",
      "250 Steps done!,  167500\n",
      "250 Steps done!,  167750\n",
      "250 Steps done!,  168000\n",
      "250 Steps done!,  168250\n",
      "-------------------------------------------------------------\n",
      "Episode:  156\n",
      "250 Steps done!,  168500\n",
      "250 Steps done!,  168750\n",
      "250 Steps done!,  169000\n",
      "250 Steps done!,  169250\n",
      "-------------------------------------------------------------\n",
      "Episode:  157\n",
      "250 Steps done!,  169500\n",
      "250 Steps done!,  169750\n",
      "250 Steps done!,  170000\n",
      "250 Steps done!,  170250\n",
      "250 Steps done!,  170500\n",
      "-------------------------------------------------------------\n",
      "Episode:  158\n",
      "250 Steps done!,  170750\n",
      "250 Steps done!,  171000\n",
      "250 Steps done!,  171250\n",
      "250 Steps done!,  171500\n",
      "-------------------------------------------------------------\n",
      "Episode:  159\n",
      "250 Steps done!,  171750\n",
      "250 Steps done!,  172000\n",
      "250 Steps done!,  172250\n",
      "250 Steps done!,  172500\n",
      "-------------------------------------------------------------\n",
      "Episode:  160\n",
      "250 Steps done!,  172750\n",
      "250 Steps done!,  173000\n",
      "250 Steps done!,  173250\n",
      "250 Steps done!,  173500\n",
      "250 Steps done!,  173750\n",
      "-------------------------------------------------------------\n",
      "Episode:  161\n",
      "250 Steps done!,  174000\n",
      "250 Steps done!,  174250\n",
      "250 Steps done!,  174500\n",
      "250 Steps done!,  174750\n",
      "-------------------------------------------------------------\n",
      "Episode:  162\n",
      "250 Steps done!,  175000\n",
      "250 Steps done!,  175250\n",
      "250 Steps done!,  175500\n",
      "250 Steps done!,  175750\n",
      "-------------------------------------------------------------\n",
      "Episode:  163\n",
      "250 Steps done!,  176000\n",
      "250 Steps done!,  176250\n",
      "250 Steps done!,  176500\n",
      "250 Steps done!,  176750\n",
      "250 Steps done!,  177000\n",
      "-------------------------------------------------------------\n",
      "Episode:  164\n",
      "250 Steps done!,  177250\n",
      "250 Steps done!,  177500\n",
      "250 Steps done!,  177750\n",
      "250 Steps done!,  178000\n",
      "-------------------------------------------------------------\n",
      "Episode:  165\n",
      "250 Steps done!,  178250\n",
      "250 Steps done!,  178500\n",
      "250 Steps done!,  178750\n",
      "250 Steps done!,  179000\n",
      "-------------------------------------------------------------\n",
      "Episode:  166\n",
      "250 Steps done!,  179250\n",
      "250 Steps done!,  179500\n",
      "250 Steps done!,  179750\n",
      "250 Steps done!,  180000\n",
      "250 Steps done!,  180250\n",
      "-------------------------------------------------------------\n",
      "Episode:  167\n",
      "250 Steps done!,  180500\n",
      "250 Steps done!,  180750\n",
      "250 Steps done!,  181000\n",
      "250 Steps done!,  181250\n",
      "-------------------------------------------------------------\n",
      "Episode:  168\n",
      "250 Steps done!,  181500\n",
      "250 Steps done!,  181750\n",
      "250 Steps done!,  182000\n",
      "250 Steps done!,  182250\n",
      "-------------------------------------------------------------\n",
      "Episode:  169\n",
      "250 Steps done!,  182500\n",
      "250 Steps done!,  182750\n",
      "250 Steps done!,  183000\n",
      "250 Steps done!,  183250\n",
      "250 Steps done!,  183500\n",
      "-------------------------------------------------------------\n",
      "Episode:  170\n",
      "250 Steps done!,  183750\n",
      "250 Steps done!,  184000\n",
      "250 Steps done!,  184250\n",
      "250 Steps done!,  184500\n",
      "-------------------------------------------------------------\n",
      "Episode:  171\n",
      "250 Steps done!,  184750\n",
      "250 Steps done!,  185000\n",
      "250 Steps done!,  185250\n",
      "250 Steps done!,  185500\n",
      "-------------------------------------------------------------\n",
      "Episode:  172\n",
      "250 Steps done!,  185750\n",
      "250 Steps done!,  186000\n",
      "250 Steps done!,  186250\n",
      "250 Steps done!,  186500\n",
      "250 Steps done!,  186750\n",
      "-------------------------------------------------------------\n",
      "Episode:  173\n",
      "250 Steps done!,  187000\n",
      "250 Steps done!,  187250\n",
      "250 Steps done!,  187500\n",
      "250 Steps done!,  187750\n",
      "-------------------------------------------------------------\n",
      "Episode:  174\n",
      "250 Steps done!,  188000\n",
      "250 Steps done!,  188250\n",
      "250 Steps done!,  188500\n",
      "250 Steps done!,  188750\n",
      "-------------------------------------------------------------\n",
      "Episode:  175\n",
      "250 Steps done!,  189000\n",
      "250 Steps done!,  189250\n",
      "250 Steps done!,  189500\n",
      "250 Steps done!,  189750\n",
      "250 Steps done!,  190000\n",
      "-------------------------------------------------------------\n",
      "Episode:  176\n",
      "250 Steps done!,  190250\n",
      "250 Steps done!,  190500\n",
      "250 Steps done!,  190750\n",
      "250 Steps done!,  191000\n",
      "-------------------------------------------------------------\n",
      "Episode:  177\n",
      "250 Steps done!,  191250\n",
      "250 Steps done!,  191500\n",
      "250 Steps done!,  191750\n",
      "250 Steps done!,  192000\n",
      "-------------------------------------------------------------\n",
      "Episode:  178\n",
      "250 Steps done!,  192250\n",
      "250 Steps done!,  192500\n",
      "250 Steps done!,  192750\n",
      "250 Steps done!,  193000\n",
      "250 Steps done!,  193250\n",
      "-------------------------------------------------------------\n",
      "Episode:  179\n",
      "250 Steps done!,  193500\n",
      "250 Steps done!,  193750\n",
      "250 Steps done!,  194000\n",
      "250 Steps done!,  194250\n",
      "-------------------------------------------------------------\n",
      "Episode:  180\n",
      "250 Steps done!,  194500\n",
      "250 Steps done!,  194750\n",
      "250 Steps done!,  195000\n",
      "250 Steps done!,  195250\n",
      "-------------------------------------------------------------\n",
      "Episode:  181\n",
      "250 Steps done!,  195500\n",
      "250 Steps done!,  195750\n",
      "250 Steps done!,  196000\n",
      "250 Steps done!,  196250\n",
      "-------------------------------------------------------------\n",
      "Episode:  182\n",
      "250 Steps done!,  196500\n",
      "250 Steps done!,  196750\n",
      "250 Steps done!,  197000\n",
      "250 Steps done!,  197250\n",
      "250 Steps done!,  197500\n",
      "-------------------------------------------------------------\n",
      "Episode:  183\n",
      "250 Steps done!,  197750\n",
      "250 Steps done!,  198000\n",
      "250 Steps done!,  198250\n",
      "250 Steps done!,  198500\n",
      "-------------------------------------------------------------\n",
      "Episode:  184\n",
      "250 Steps done!,  198750\n",
      "250 Steps done!,  199000\n",
      "250 Steps done!,  199250\n",
      "250 Steps done!,  199500\n",
      "-------------------------------------------------------------\n",
      "Episode:  185\n",
      "250 Steps done!,  199750\n",
      "250 Steps done!,  200000\n",
      "250 Steps done!,  200250\n",
      "250 Steps done!,  200500\n",
      "250 Steps done!,  200750\n",
      "-------------------------------------------------------------\n",
      "Episode:  186\n",
      "250 Steps done!,  201000\n",
      "250 Steps done!,  201250\n",
      "250 Steps done!,  201500\n",
      "250 Steps done!,  201750\n",
      "-------------------------------------------------------------\n",
      "Episode:  187\n",
      "250 Steps done!,  202000\n",
      "250 Steps done!,  202250\n",
      "250 Steps done!,  202500\n",
      "250 Steps done!,  202750\n",
      "-------------------------------------------------------------\n",
      "Episode:  188\n",
      "250 Steps done!,  203000\n",
      "250 Steps done!,  203250\n",
      "250 Steps done!,  203500\n",
      "250 Steps done!,  203750\n",
      "250 Steps done!,  204000\n",
      "-------------------------------------------------------------\n",
      "Episode:  189\n",
      "250 Steps done!,  204250\n",
      "250 Steps done!,  204500\n",
      "250 Steps done!,  204750\n",
      "250 Steps done!,  205000\n",
      "-------------------------------------------------------------\n",
      "Episode:  190\n",
      "250 Steps done!,  205250\n",
      "250 Steps done!,  205500\n",
      "250 Steps done!,  205750\n",
      "250 Steps done!,  206000\n",
      "-------------------------------------------------------------\n",
      "Episode:  191\n",
      "250 Steps done!,  206250\n",
      "250 Steps done!,  206500\n",
      "250 Steps done!,  206750\n",
      "250 Steps done!,  207000\n",
      "250 Steps done!,  207250\n",
      "-------------------------------------------------------------\n",
      "Episode:  192\n",
      "250 Steps done!,  207500\n",
      "250 Steps done!,  207750\n",
      "250 Steps done!,  208000\n",
      "250 Steps done!,  208250\n",
      "-------------------------------------------------------------\n",
      "Episode:  193\n",
      "250 Steps done!,  208500\n",
      "250 Steps done!,  208750\n",
      "250 Steps done!,  209000\n",
      "250 Steps done!,  209250\n",
      "-------------------------------------------------------------\n",
      "Episode:  194\n",
      "250 Steps done!,  209500\n",
      "250 Steps done!,  209750\n",
      "250 Steps done!,  210000\n",
      "250 Steps done!,  210250\n",
      "250 Steps done!,  210500\n",
      "-------------------------------------------------------------\n",
      "Episode:  195\n",
      "250 Steps done!,  210750\n",
      "250 Steps done!,  211000\n",
      "250 Steps done!,  211250\n",
      "250 Steps done!,  211500\n",
      "-------------------------------------------------------------\n",
      "Episode:  196\n",
      "250 Steps done!,  211750\n",
      "250 Steps done!,  212000\n",
      "250 Steps done!,  212250\n",
      "250 Steps done!,  212500\n",
      "-------------------------------------------------------------\n",
      "Episode:  197\n",
      "250 Steps done!,  212750\n",
      "250 Steps done!,  213000\n",
      "250 Steps done!,  213250\n",
      "250 Steps done!,  213500\n",
      "250 Steps done!,  213750\n",
      "-------------------------------------------------------------\n",
      "Episode:  198\n",
      "250 Steps done!,  214000\n",
      "250 Steps done!,  214250\n",
      "250 Steps done!,  214500\n",
      "250 Steps done!,  214750\n",
      "-------------------------------------------------------------\n",
      "Episode:  199\n",
      "250 Steps done!,  215000\n",
      "250 Steps done!,  215250\n",
      "250 Steps done!,  215500\n",
      "250 Steps done!,  215750\n",
      "-------------------------------------------------------------\n",
      "Episode:  200\n",
      "250 Steps done!,  216000\n",
      "250 Steps done!,  216250\n",
      "250 Steps done!,  216500\n",
      "250 Steps done!,  216750\n",
      "250 Steps done!,  217000\n",
      "-------------------------------------------------------------\n",
      "Episode:  201\n",
      "250 Steps done!,  217250\n",
      "250 Steps done!,  217500\n",
      "250 Steps done!,  217750\n",
      "250 Steps done!,  218000\n",
      "-------------------------------------------------------------\n",
      "Episode:  202\n",
      "250 Steps done!,  218250\n",
      "250 Steps done!,  218500\n",
      "250 Steps done!,  218750\n",
      "250 Steps done!,  219000\n",
      "-------------------------------------------------------------\n",
      "Episode:  203\n",
      "250 Steps done!,  219250\n",
      "250 Steps done!,  219500\n",
      "250 Steps done!,  219750\n",
      "250 Steps done!,  220000\n",
      "250 Steps done!,  220250\n",
      "-------------------------------------------------------------\n",
      "Episode:  204\n",
      "250 Steps done!,  220500\n",
      "250 Steps done!,  220750\n",
      "250 Steps done!,  221000\n",
      "250 Steps done!,  221250\n",
      "-------------------------------------------------------------\n",
      "Episode:  205\n",
      "250 Steps done!,  221500\n",
      "250 Steps done!,  221750\n",
      "250 Steps done!,  222000\n",
      "250 Steps done!,  222250\n",
      "-------------------------------------------------------------\n",
      "Episode:  206\n",
      "250 Steps done!,  222500\n",
      "250 Steps done!,  222750\n",
      "250 Steps done!,  223000\n",
      "250 Steps done!,  223250\n",
      "-------------------------------------------------------------\n",
      "Episode:  207\n",
      "250 Steps done!,  223500\n",
      "250 Steps done!,  223750\n",
      "250 Steps done!,  224000\n",
      "250 Steps done!,  224250\n",
      "250 Steps done!,  224500\n",
      "-------------------------------------------------------------\n",
      "Episode:  208\n",
      "250 Steps done!,  224750\n",
      "250 Steps done!,  225000\n",
      "250 Steps done!,  225250\n",
      "250 Steps done!,  225500\n",
      "-------------------------------------------------------------\n",
      "Episode:  209\n",
      "250 Steps done!,  225750\n",
      "250 Steps done!,  226000\n",
      "250 Steps done!,  226250\n",
      "250 Steps done!,  226500\n",
      "-------------------------------------------------------------\n",
      "Episode:  210\n",
      "250 Steps done!,  226750\n",
      "250 Steps done!,  227000\n",
      "250 Steps done!,  227250\n",
      "250 Steps done!,  227500\n",
      "250 Steps done!,  227750\n",
      "-------------------------------------------------------------\n",
      "Episode:  211\n",
      "250 Steps done!,  228000\n",
      "250 Steps done!,  228250\n",
      "250 Steps done!,  228500\n",
      "250 Steps done!,  228750\n",
      "-------------------------------------------------------------\n",
      "Episode:  212\n",
      "250 Steps done!,  229000\n",
      "250 Steps done!,  229250\n",
      "250 Steps done!,  229500\n",
      "250 Steps done!,  229750\n",
      "-------------------------------------------------------------\n",
      "Episode:  213\n",
      "250 Steps done!,  230000\n",
      "250 Steps done!,  230250\n",
      "250 Steps done!,  230500\n",
      "250 Steps done!,  230750\n",
      "250 Steps done!,  231000\n",
      "-------------------------------------------------------------\n",
      "Episode:  214\n",
      "250 Steps done!,  231250\n",
      "250 Steps done!,  231500\n",
      "250 Steps done!,  231750\n",
      "250 Steps done!,  232000\n",
      "-------------------------------------------------------------\n",
      "Episode:  215\n",
      "250 Steps done!,  232250\n",
      "250 Steps done!,  232500\n",
      "250 Steps done!,  232750\n",
      "250 Steps done!,  233000\n",
      "-------------------------------------------------------------\n",
      "Episode:  216\n",
      "250 Steps done!,  233250\n",
      "250 Steps done!,  233500\n",
      "250 Steps done!,  233750\n",
      "250 Steps done!,  234000\n",
      "250 Steps done!,  234250\n",
      "-------------------------------------------------------------\n",
      "Episode:  217\n",
      "250 Steps done!,  234500\n",
      "250 Steps done!,  234750\n",
      "250 Steps done!,  235000\n",
      "250 Steps done!,  235250\n",
      "-------------------------------------------------------------\n",
      "Episode:  218\n",
      "250 Steps done!,  235500\n",
      "250 Steps done!,  235750\n",
      "250 Steps done!,  236000\n",
      "250 Steps done!,  236250\n",
      "-------------------------------------------------------------\n",
      "Episode:  219\n",
      "250 Steps done!,  236500\n",
      "250 Steps done!,  236750\n",
      "250 Steps done!,  237000\n",
      "250 Steps done!,  237250\n",
      "250 Steps done!,  237500\n",
      "-------------------------------------------------------------\n",
      "Episode:  220\n",
      "250 Steps done!,  237750\n",
      "250 Steps done!,  238000\n",
      "250 Steps done!,  238250\n",
      "250 Steps done!,  238500\n",
      "-------------------------------------------------------------\n",
      "Episode:  221\n",
      "250 Steps done!,  238750\n",
      "250 Steps done!,  239000\n",
      "250 Steps done!,  239250\n",
      "250 Steps done!,  239500\n",
      "-------------------------------------------------------------\n",
      "Episode:  222\n",
      "250 Steps done!,  239750\n",
      "250 Steps done!,  240000\n",
      "250 Steps done!,  240250\n",
      "250 Steps done!,  240500\n",
      "250 Steps done!,  240750\n",
      "-------------------------------------------------------------\n",
      "Episode:  223\n",
      "250 Steps done!,  241000\n",
      "250 Steps done!,  241250\n",
      "250 Steps done!,  241500\n",
      "250 Steps done!,  241750\n",
      "-------------------------------------------------------------\n",
      "Episode:  224\n",
      "250 Steps done!,  242000\n",
      "250 Steps done!,  242250\n",
      "250 Steps done!,  242500\n",
      "250 Steps done!,  242750\n",
      "-------------------------------------------------------------\n",
      "Episode:  225\n",
      "250 Steps done!,  243000\n",
      "250 Steps done!,  243250\n",
      "250 Steps done!,  243500\n",
      "250 Steps done!,  243750\n",
      "250 Steps done!,  244000\n",
      "-------------------------------------------------------------\n",
      "Episode:  226\n",
      "250 Steps done!,  244250\n",
      "250 Steps done!,  244500\n",
      "250 Steps done!,  244750\n",
      "250 Steps done!,  245000\n",
      "-------------------------------------------------------------\n",
      "Episode:  227\n",
      "250 Steps done!,  245250\n",
      "250 Steps done!,  245500\n",
      "250 Steps done!,  245750\n",
      "250 Steps done!,  246000\n",
      "-------------------------------------------------------------\n",
      "Episode:  228\n",
      "250 Steps done!,  246250\n",
      "250 Steps done!,  246500\n",
      "250 Steps done!,  246750\n",
      "250 Steps done!,  247000\n",
      "250 Steps done!,  247250\n",
      "-------------------------------------------------------------\n",
      "Episode:  229\n",
      "250 Steps done!,  247500\n",
      "250 Steps done!,  247750\n",
      "250 Steps done!,  248000\n",
      "250 Steps done!,  248250\n",
      "-------------------------------------------------------------\n",
      "Episode:  230\n",
      "250 Steps done!,  248500\n",
      "250 Steps done!,  248750\n",
      "250 Steps done!,  249000\n",
      "250 Steps done!,  249250\n",
      "-------------------------------------------------------------\n",
      "Episode:  231\n",
      "250 Steps done!,  249500\n",
      "250 Steps done!,  249750\n",
      "250 Steps done!,  250000\n",
      "250 Steps done!,  250250\n",
      "-------------------------------------------------------------\n",
      "Episode:  232\n",
      "250 Steps done!,  250500\n",
      "250 Steps done!,  250750\n",
      "250 Steps done!,  251000\n",
      "250 Steps done!,  251250\n",
      "250 Steps done!,  251500\n",
      "-------------------------------------------------------------\n",
      "Episode:  233\n",
      "250 Steps done!,  251750\n",
      "250 Steps done!,  252000\n",
      "250 Steps done!,  252250\n",
      "250 Steps done!,  252500\n",
      "-------------------------------------------------------------\n",
      "Episode:  234\n",
      "250 Steps done!,  252750\n",
      "250 Steps done!,  253000\n",
      "250 Steps done!,  253250\n",
      "250 Steps done!,  253500\n",
      "-------------------------------------------------------------\n",
      "Episode:  235\n",
      "250 Steps done!,  253750\n",
      "250 Steps done!,  254000\n",
      "250 Steps done!,  254250\n",
      "250 Steps done!,  254500\n",
      "250 Steps done!,  254750\n",
      "-------------------------------------------------------------\n",
      "Episode:  236\n",
      "250 Steps done!,  255000\n",
      "250 Steps done!,  255250\n",
      "250 Steps done!,  255500\n",
      "250 Steps done!,  255750\n",
      "-------------------------------------------------------------\n",
      "Episode:  237\n",
      "250 Steps done!,  256000\n",
      "250 Steps done!,  256250\n",
      "250 Steps done!,  256500\n",
      "250 Steps done!,  256750\n",
      "-------------------------------------------------------------\n",
      "Episode:  238\n",
      "250 Steps done!,  257000\n",
      "250 Steps done!,  257250\n",
      "250 Steps done!,  257500\n",
      "250 Steps done!,  257750\n",
      "250 Steps done!,  258000\n",
      "-------------------------------------------------------------\n",
      "Episode:  239\n",
      "250 Steps done!,  258250\n",
      "250 Steps done!,  258500\n",
      "250 Steps done!,  258750\n",
      "250 Steps done!,  259000\n",
      "-------------------------------------------------------------\n",
      "Episode:  240\n",
      "250 Steps done!,  259250\n",
      "250 Steps done!,  259500\n",
      "250 Steps done!,  259750\n",
      "250 Steps done!,  260000\n",
      "-------------------------------------------------------------\n",
      "Episode:  241\n",
      "250 Steps done!,  260250\n",
      "250 Steps done!,  260500\n",
      "250 Steps done!,  260750\n",
      "250 Steps done!,  261000\n",
      "250 Steps done!,  261250\n",
      "-------------------------------------------------------------\n",
      "Episode:  242\n",
      "250 Steps done!,  261500\n",
      "250 Steps done!,  261750\n",
      "250 Steps done!,  262000\n",
      "250 Steps done!,  262250\n",
      "-------------------------------------------------------------\n",
      "Episode:  243\n",
      "250 Steps done!,  262500\n",
      "250 Steps done!,  262750\n",
      "250 Steps done!,  263000\n",
      "250 Steps done!,  263250\n",
      "-------------------------------------------------------------\n",
      "Episode:  244\n",
      "250 Steps done!,  263500\n",
      "250 Steps done!,  263750\n",
      "250 Steps done!,  264000\n",
      "250 Steps done!,  264250\n",
      "250 Steps done!,  264500\n",
      "-------------------------------------------------------------\n",
      "Episode:  245\n",
      "250 Steps done!,  264750\n",
      "250 Steps done!,  265000\n",
      "250 Steps done!,  265250\n",
      "250 Steps done!,  265500\n",
      "-------------------------------------------------------------\n",
      "Episode:  246\n",
      "250 Steps done!,  265750\n",
      "250 Steps done!,  266000\n",
      "250 Steps done!,  266250\n",
      "250 Steps done!,  266500\n",
      "-------------------------------------------------------------\n",
      "Episode:  247\n",
      "250 Steps done!,  266750\n",
      "250 Steps done!,  267000\n",
      "250 Steps done!,  267250\n",
      "250 Steps done!,  267500\n",
      "250 Steps done!,  267750\n",
      "-------------------------------------------------------------\n",
      "Episode:  248\n",
      "250 Steps done!,  268000\n",
      "250 Steps done!,  268250\n",
      "250 Steps done!,  268500\n",
      "250 Steps done!,  268750\n",
      "-------------------------------------------------------------\n",
      "Episode:  249\n",
      "250 Steps done!,  269000\n",
      "250 Steps done!,  269250\n",
      "250 Steps done!,  269500\n",
      "250 Steps done!,  269750\n",
      "-------------------------------------------------------------\n",
      "Episode:  250\n",
      "250 Steps done!,  270000\n",
      "250 Steps done!,  270250\n",
      "250 Steps done!,  270500\n",
      "250 Steps done!,  270750\n",
      "250 Steps done!,  271000\n",
      "-------------------------------------------------------------\n",
      "Episode:  251\n",
      "250 Steps done!,  271250\n",
      "250 Steps done!,  271500\n",
      "250 Steps done!,  271750\n",
      "250 Steps done!,  272000\n",
      "-------------------------------------------------------------\n",
      "Episode:  252\n",
      "250 Steps done!,  272250\n",
      "250 Steps done!,  272500\n",
      "250 Steps done!,  272750\n",
      "250 Steps done!,  273000\n",
      "-------------------------------------------------------------\n",
      "Episode:  253\n",
      "250 Steps done!,  273250\n",
      "250 Steps done!,  273500\n",
      "250 Steps done!,  273750\n",
      "250 Steps done!,  274000\n",
      "250 Steps done!,  274250\n",
      "-------------------------------------------------------------\n",
      "Episode:  254\n",
      "250 Steps done!,  274500\n",
      "250 Steps done!,  274750\n",
      "250 Steps done!,  275000\n",
      "250 Steps done!,  275250\n",
      "-------------------------------------------------------------\n",
      "Episode:  255\n",
      "250 Steps done!,  275500\n",
      "250 Steps done!,  275750\n",
      "250 Steps done!,  276000\n",
      "250 Steps done!,  276250\n",
      "-------------------------------------------------------------\n",
      "Episode:  256\n",
      "250 Steps done!,  276500\n",
      "250 Steps done!,  276750\n",
      "250 Steps done!,  277000\n",
      "250 Steps done!,  277250\n",
      "-------------------------------------------------------------\n",
      "Episode:  257\n",
      "250 Steps done!,  277500\n",
      "250 Steps done!,  277750\n",
      "250 Steps done!,  278000\n",
      "250 Steps done!,  278250\n",
      "250 Steps done!,  278500\n",
      "-------------------------------------------------------------\n",
      "Episode:  258\n",
      "250 Steps done!,  278750\n",
      "250 Steps done!,  279000\n",
      "250 Steps done!,  279250\n",
      "250 Steps done!,  279500\n",
      "-------------------------------------------------------------\n",
      "Episode:  259\n",
      "250 Steps done!,  279750\n",
      "250 Steps done!,  280000\n",
      "250 Steps done!,  280250\n",
      "250 Steps done!,  280500\n",
      "-------------------------------------------------------------\n",
      "Episode:  260\n",
      "250 Steps done!,  280750\n",
      "250 Steps done!,  281000\n",
      "250 Steps done!,  281250\n",
      "250 Steps done!,  281500\n",
      "250 Steps done!,  281750\n",
      "-------------------------------------------------------------\n",
      "Episode:  261\n",
      "250 Steps done!,  282000\n",
      "250 Steps done!,  282250\n",
      "250 Steps done!,  282500\n",
      "250 Steps done!,  282750\n",
      "-------------------------------------------------------------\n",
      "Episode:  262\n",
      "250 Steps done!,  283000\n",
      "250 Steps done!,  283250\n",
      "250 Steps done!,  283500\n",
      "250 Steps done!,  283750\n",
      "-------------------------------------------------------------\n",
      "Episode:  263\n",
      "250 Steps done!,  284000\n",
      "250 Steps done!,  284250\n",
      "250 Steps done!,  284500\n",
      "250 Steps done!,  284750\n",
      "250 Steps done!,  285000\n",
      "-------------------------------------------------------------\n",
      "Episode:  264\n",
      "250 Steps done!,  285250\n",
      "250 Steps done!,  285500\n",
      "250 Steps done!,  285750\n",
      "250 Steps done!,  286000\n",
      "-------------------------------------------------------------\n",
      "Episode:  265\n",
      "250 Steps done!,  286250\n",
      "250 Steps done!,  286500\n",
      "250 Steps done!,  286750\n",
      "250 Steps done!,  287000\n",
      "-------------------------------------------------------------\n",
      "Episode:  266\n",
      "250 Steps done!,  287250\n",
      "250 Steps done!,  287500\n",
      "250 Steps done!,  287750\n",
      "250 Steps done!,  288000\n",
      "250 Steps done!,  288250\n",
      "-------------------------------------------------------------\n",
      "Episode:  267\n",
      "250 Steps done!,  288500\n",
      "250 Steps done!,  288750\n",
      "250 Steps done!,  289000\n",
      "250 Steps done!,  289250\n",
      "-------------------------------------------------------------\n",
      "Episode:  268\n",
      "250 Steps done!,  289500\n",
      "250 Steps done!,  289750\n",
      "250 Steps done!,  290000\n",
      "250 Steps done!,  290250\n",
      "-------------------------------------------------------------\n",
      "Episode:  269\n",
      "250 Steps done!,  290500\n",
      "250 Steps done!,  290750\n",
      "250 Steps done!,  291000\n",
      "250 Steps done!,  291250\n",
      "250 Steps done!,  291500\n",
      "-------------------------------------------------------------\n",
      "Episode:  270\n",
      "250 Steps done!,  291750\n",
      "250 Steps done!,  292000\n",
      "250 Steps done!,  292250\n",
      "250 Steps done!,  292500\n",
      "-------------------------------------------------------------\n",
      "Episode:  271\n",
      "250 Steps done!,  292750\n",
      "250 Steps done!,  293000\n",
      "250 Steps done!,  293250\n",
      "250 Steps done!,  293500\n",
      "-------------------------------------------------------------\n",
      "Episode:  272\n",
      "250 Steps done!,  293750\n",
      "250 Steps done!,  294000\n",
      "250 Steps done!,  294250\n",
      "250 Steps done!,  294500\n",
      "250 Steps done!,  294750\n",
      "-------------------------------------------------------------\n",
      "Episode:  273\n",
      "250 Steps done!,  295000\n",
      "250 Steps done!,  295250\n",
      "250 Steps done!,  295500\n",
      "250 Steps done!,  295750\n",
      "-------------------------------------------------------------\n",
      "Episode:  274\n",
      "250 Steps done!,  296000\n",
      "250 Steps done!,  296250\n",
      "250 Steps done!,  296500\n",
      "250 Steps done!,  296750\n",
      "-------------------------------------------------------------\n",
      "Episode:  275\n",
      "250 Steps done!,  297000\n",
      "250 Steps done!,  297250\n",
      "250 Steps done!,  297500\n",
      "250 Steps done!,  297750\n",
      "250 Steps done!,  298000\n",
      "-------------------------------------------------------------\n",
      "Episode:  276\n",
      "250 Steps done!,  298250\n",
      "250 Steps done!,  298500\n",
      "250 Steps done!,  298750\n",
      "250 Steps done!,  299000\n",
      "-------------------------------------------------------------\n",
      "Episode:  277\n",
      "250 Steps done!,  299250\n",
      "250 Steps done!,  299500\n",
      "250 Steps done!,  299750\n",
      "250 Steps done!,  300000\n",
      "-------------------------------------------------------------\n",
      "Episode:  278\n",
      "250 Steps done!,  300250\n",
      "250 Steps done!,  300500\n",
      "250 Steps done!,  300750\n",
      "250 Steps done!,  301000\n",
      "250 Steps done!,  301250\n",
      "-------------------------------------------------------------\n",
      "Episode:  279\n",
      "250 Steps done!,  301500\n",
      "250 Steps done!,  301750\n",
      "250 Steps done!,  302000\n",
      "250 Steps done!,  302250\n",
      "-------------------------------------------------------------\n",
      "Episode:  280\n",
      "250 Steps done!,  302500\n",
      "250 Steps done!,  302750\n",
      "250 Steps done!,  303000\n",
      "250 Steps done!,  303250\n",
      "-------------------------------------------------------------\n",
      "Episode:  281\n",
      "250 Steps done!,  303500\n",
      "250 Steps done!,  303750\n",
      "250 Steps done!,  304000\n",
      "250 Steps done!,  304250\n",
      "-------------------------------------------------------------\n",
      "Episode:  282\n",
      "250 Steps done!,  304500\n",
      "250 Steps done!,  304750\n",
      "250 Steps done!,  305000\n",
      "250 Steps done!,  305250\n",
      "250 Steps done!,  305500\n",
      "-------------------------------------------------------------\n",
      "Episode:  283\n",
      "250 Steps done!,  305750\n",
      "250 Steps done!,  306000\n",
      "250 Steps done!,  306250\n",
      "250 Steps done!,  306500\n",
      "-------------------------------------------------------------\n",
      "Episode:  284\n",
      "250 Steps done!,  306750\n",
      "250 Steps done!,  307000\n",
      "250 Steps done!,  307250\n",
      "250 Steps done!,  307500\n",
      "-------------------------------------------------------------\n",
      "Episode:  285\n",
      "250 Steps done!,  307750\n",
      "250 Steps done!,  308000\n",
      "250 Steps done!,  308250\n",
      "250 Steps done!,  308500\n",
      "250 Steps done!,  308750\n",
      "-------------------------------------------------------------\n",
      "Episode:  286\n",
      "250 Steps done!,  309000\n",
      "250 Steps done!,  309250\n",
      "250 Steps done!,  309500\n",
      "250 Steps done!,  309750\n",
      "-------------------------------------------------------------\n",
      "Episode:  287\n",
      "250 Steps done!,  310000\n",
      "250 Steps done!,  310250\n",
      "250 Steps done!,  310500\n",
      "250 Steps done!,  310750\n",
      "-------------------------------------------------------------\n",
      "Episode:  288\n",
      "250 Steps done!,  311000\n",
      "250 Steps done!,  311250\n",
      "250 Steps done!,  311500\n",
      "250 Steps done!,  311750\n",
      "250 Steps done!,  312000\n",
      "-------------------------------------------------------------\n",
      "Episode:  289\n",
      "250 Steps done!,  312250\n",
      "250 Steps done!,  312500\n",
      "250 Steps done!,  312750\n",
      "250 Steps done!,  313000\n",
      "-------------------------------------------------------------\n",
      "Episode:  290\n",
      "250 Steps done!,  313250\n",
      "250 Steps done!,  313500\n",
      "250 Steps done!,  313750\n",
      "250 Steps done!,  314000\n",
      "-------------------------------------------------------------\n",
      "Episode:  291\n",
      "250 Steps done!,  314250\n",
      "250 Steps done!,  314500\n",
      "250 Steps done!,  314750\n",
      "250 Steps done!,  315000\n",
      "250 Steps done!,  315250\n",
      "-------------------------------------------------------------\n",
      "Episode:  292\n",
      "250 Steps done!,  315500\n",
      "250 Steps done!,  315750\n",
      "250 Steps done!,  316000\n",
      "250 Steps done!,  316250\n",
      "-------------------------------------------------------------\n",
      "Episode:  293\n",
      "250 Steps done!,  316500\n",
      "250 Steps done!,  316750\n",
      "250 Steps done!,  317000\n",
      "250 Steps done!,  317250\n",
      "-------------------------------------------------------------\n",
      "Episode:  294\n",
      "250 Steps done!,  317500\n",
      "250 Steps done!,  317750\n",
      "250 Steps done!,  318000\n",
      "250 Steps done!,  318250\n",
      "250 Steps done!,  318500\n",
      "-------------------------------------------------------------\n",
      "Episode:  295\n",
      "250 Steps done!,  318750\n",
      "250 Steps done!,  319000\n",
      "250 Steps done!,  319250\n",
      "250 Steps done!,  319500\n",
      "-------------------------------------------------------------\n",
      "Episode:  296\n",
      "250 Steps done!,  319750\n",
      "250 Steps done!,  320000\n",
      "250 Steps done!,  320250\n",
      "250 Steps done!,  320500\n",
      "-------------------------------------------------------------\n",
      "Episode:  297\n",
      "250 Steps done!,  320750\n",
      "250 Steps done!,  321000\n",
      "250 Steps done!,  321250\n",
      "250 Steps done!,  321500\n",
      "250 Steps done!,  321750\n",
      "-------------------------------------------------------------\n",
      "Episode:  298\n",
      "250 Steps done!,  322000\n",
      "250 Steps done!,  322250\n",
      "250 Steps done!,  322500\n",
      "250 Steps done!,  322750\n",
      "-------------------------------------------------------------\n",
      "Episode:  299\n",
      "250 Steps done!,  323000\n",
      "250 Steps done!,  323250\n",
      "250 Steps done!,  323500\n",
      "250 Steps done!,  323750\n",
      "-------------------------------------------------------------\n",
      "Episode:  300\n",
      "250 Steps done!,  324000\n",
      "250 Steps done!,  324250\n",
      "250 Steps done!,  324500\n",
      "250 Steps done!,  324750\n",
      "250 Steps done!,  325000\n",
      "-------------------------------------------------------------\n",
      "Episode:  301\n",
      "250 Steps done!,  325250\n",
      "250 Steps done!,  325500\n",
      "250 Steps done!,  325750\n",
      "250 Steps done!,  326000\n",
      "-------------------------------------------------------------\n",
      "Episode:  302\n",
      "250 Steps done!,  326250\n",
      "250 Steps done!,  326500\n",
      "250 Steps done!,  326750\n",
      "250 Steps done!,  327000\n",
      "-------------------------------------------------------------\n",
      "Episode:  303\n",
      "250 Steps done!,  327250\n",
      "250 Steps done!,  327500\n",
      "250 Steps done!,  327750\n",
      "250 Steps done!,  328000\n",
      "250 Steps done!,  328250\n",
      "-------------------------------------------------------------\n",
      "Episode:  304\n",
      "250 Steps done!,  328500\n",
      "250 Steps done!,  328750\n",
      "250 Steps done!,  329000\n",
      "250 Steps done!,  329250\n",
      "-------------------------------------------------------------\n",
      "Episode:  305\n",
      "250 Steps done!,  329500\n",
      "250 Steps done!,  329750\n",
      "250 Steps done!,  330000\n",
      "250 Steps done!,  330250\n",
      "-------------------------------------------------------------\n",
      "Episode:  306\n",
      "250 Steps done!,  330500\n",
      "250 Steps done!,  330750\n",
      "250 Steps done!,  331000\n",
      "250 Steps done!,  331250\n",
      "-------------------------------------------------------------\n",
      "Episode:  307\n",
      "250 Steps done!,  331500\n",
      "250 Steps done!,  331750\n",
      "250 Steps done!,  332000\n",
      "250 Steps done!,  332250\n",
      "250 Steps done!,  332500\n",
      "-------------------------------------------------------------\n",
      "Episode:  308\n",
      "250 Steps done!,  332750\n",
      "250 Steps done!,  333000\n",
      "250 Steps done!,  333250\n",
      "250 Steps done!,  333500\n",
      "-------------------------------------------------------------\n",
      "Episode:  309\n",
      "250 Steps done!,  333750\n",
      "250 Steps done!,  334000\n",
      "250 Steps done!,  334250\n",
      "250 Steps done!,  334500\n",
      "-------------------------------------------------------------\n",
      "Episode:  310\n",
      "250 Steps done!,  334750\n",
      "250 Steps done!,  335000\n",
      "250 Steps done!,  335250\n",
      "250 Steps done!,  335500\n",
      "250 Steps done!,  335750\n",
      "-------------------------------------------------------------\n",
      "Episode:  311\n",
      "250 Steps done!,  336000\n",
      "250 Steps done!,  336250\n",
      "250 Steps done!,  336500\n",
      "250 Steps done!,  336750\n",
      "-------------------------------------------------------------\n",
      "Episode:  312\n",
      "250 Steps done!,  337000\n",
      "250 Steps done!,  337250\n",
      "250 Steps done!,  337500\n",
      "250 Steps done!,  337750\n",
      "-------------------------------------------------------------\n",
      "Episode:  313\n",
      "250 Steps done!,  338000\n",
      "250 Steps done!,  338250\n",
      "250 Steps done!,  338500\n",
      "250 Steps done!,  338750\n",
      "250 Steps done!,  339000\n",
      "-------------------------------------------------------------\n",
      "Episode:  314\n",
      "250 Steps done!,  339250\n",
      "250 Steps done!,  339500\n",
      "250 Steps done!,  339750\n",
      "250 Steps done!,  340000\n",
      "-------------------------------------------------------------\n",
      "Episode:  315\n",
      "250 Steps done!,  340250\n",
      "250 Steps done!,  340500\n",
      "250 Steps done!,  340750\n",
      "250 Steps done!,  341000\n",
      "-------------------------------------------------------------\n",
      "Episode:  316\n",
      "250 Steps done!,  341250\n",
      "250 Steps done!,  341500\n",
      "250 Steps done!,  341750\n",
      "250 Steps done!,  342000\n",
      "250 Steps done!,  342250\n",
      "-------------------------------------------------------------\n",
      "Episode:  317\n",
      "250 Steps done!,  342500\n",
      "250 Steps done!,  342750\n",
      "250 Steps done!,  343000\n",
      "250 Steps done!,  343250\n",
      "-------------------------------------------------------------\n",
      "Episode:  318\n",
      "250 Steps done!,  343500\n",
      "250 Steps done!,  343750\n",
      "250 Steps done!,  344000\n",
      "250 Steps done!,  344250\n",
      "-------------------------------------------------------------\n",
      "Episode:  319\n",
      "250 Steps done!,  344500\n",
      "250 Steps done!,  344750\n",
      "250 Steps done!,  345000\n",
      "250 Steps done!,  345250\n",
      "250 Steps done!,  345500\n",
      "-------------------------------------------------------------\n",
      "Episode:  320\n",
      "250 Steps done!,  345750\n",
      "250 Steps done!,  346000\n",
      "250 Steps done!,  346250\n",
      "250 Steps done!,  346500\n",
      "-------------------------------------------------------------\n",
      "Episode:  321\n",
      "250 Steps done!,  346750\n",
      "250 Steps done!,  347000\n",
      "250 Steps done!,  347250\n",
      "250 Steps done!,  347500\n",
      "-------------------------------------------------------------\n",
      "Episode:  322\n",
      "250 Steps done!,  347750\n",
      "250 Steps done!,  348000\n",
      "250 Steps done!,  348250\n",
      "250 Steps done!,  348500\n",
      "250 Steps done!,  348750\n",
      "-------------------------------------------------------------\n",
      "Episode:  323\n",
      "250 Steps done!,  349000\n",
      "250 Steps done!,  349250\n",
      "250 Steps done!,  349500\n",
      "250 Steps done!,  349750\n",
      "-------------------------------------------------------------\n",
      "Episode:  324\n",
      "250 Steps done!,  350000\n",
      "250 Steps done!,  350250\n",
      "250 Steps done!,  350500\n",
      "250 Steps done!,  350750\n",
      "-------------------------------------------------------------\n",
      "Episode:  325\n",
      "250 Steps done!,  351000\n",
      "250 Steps done!,  351250\n",
      "250 Steps done!,  351500\n",
      "250 Steps done!,  351750\n",
      "250 Steps done!,  352000\n",
      "-------------------------------------------------------------\n",
      "Episode:  326\n",
      "250 Steps done!,  352250\n",
      "250 Steps done!,  352500\n",
      "250 Steps done!,  352750\n",
      "250 Steps done!,  353000\n",
      "-------------------------------------------------------------\n",
      "Episode:  327\n",
      "250 Steps done!,  353250\n",
      "250 Steps done!,  353500\n",
      "250 Steps done!,  353750\n",
      "250 Steps done!,  354000\n",
      "-------------------------------------------------------------\n",
      "Episode:  328\n",
      "250 Steps done!,  354250\n",
      "250 Steps done!,  354500\n",
      "250 Steps done!,  354750\n",
      "250 Steps done!,  355000\n",
      "250 Steps done!,  355250\n",
      "-------------------------------------------------------------\n",
      "Episode:  329\n",
      "250 Steps done!,  355500\n",
      "250 Steps done!,  355750\n",
      "250 Steps done!,  356000\n",
      "250 Steps done!,  356250\n",
      "-------------------------------------------------------------\n",
      "Episode:  330\n",
      "250 Steps done!,  356500\n",
      "250 Steps done!,  356750\n",
      "250 Steps done!,  357000\n",
      "250 Steps done!,  357250\n",
      "-------------------------------------------------------------\n",
      "Episode:  331\n",
      "250 Steps done!,  357500\n",
      "250 Steps done!,  357750\n",
      "250 Steps done!,  358000\n",
      "250 Steps done!,  358250\n",
      "-------------------------------------------------------------\n",
      "Episode:  332\n",
      "250 Steps done!,  358500\n",
      "250 Steps done!,  358750\n",
      "250 Steps done!,  359000\n",
      "250 Steps done!,  359250\n",
      "250 Steps done!,  359500\n",
      "-------------------------------------------------------------\n",
      "Episode:  333\n",
      "250 Steps done!,  359750\n",
      "250 Steps done!,  360000\n",
      "250 Steps done!,  360250\n",
      "250 Steps done!,  360500\n",
      "-------------------------------------------------------------\n",
      "Episode:  334\n",
      "250 Steps done!,  360750\n",
      "250 Steps done!,  361000\n",
      "250 Steps done!,  361250\n",
      "250 Steps done!,  361500\n",
      "-------------------------------------------------------------\n",
      "Episode:  335\n",
      "250 Steps done!,  361750\n",
      "250 Steps done!,  362000\n",
      "250 Steps done!,  362250\n",
      "250 Steps done!,  362500\n",
      "250 Steps done!,  362750\n",
      "-------------------------------------------------------------\n",
      "Episode:  336\n",
      "250 Steps done!,  363000\n",
      "250 Steps done!,  363250\n",
      "250 Steps done!,  363500\n",
      "250 Steps done!,  363750\n",
      "-------------------------------------------------------------\n",
      "Episode:  337\n",
      "250 Steps done!,  364000\n",
      "250 Steps done!,  364250\n",
      "250 Steps done!,  364500\n",
      "250 Steps done!,  364750\n",
      "-------------------------------------------------------------\n",
      "Episode:  338\n",
      "250 Steps done!,  365000\n",
      "250 Steps done!,  365250\n",
      "250 Steps done!,  365500\n",
      "250 Steps done!,  365750\n",
      "250 Steps done!,  366000\n",
      "-------------------------------------------------------------\n",
      "Episode:  339\n",
      "250 Steps done!,  366250\n",
      "250 Steps done!,  366500\n",
      "250 Steps done!,  366750\n",
      "250 Steps done!,  367000\n",
      "-------------------------------------------------------------\n",
      "Episode:  340\n",
      "250 Steps done!,  367250\n",
      "250 Steps done!,  367500\n",
      "250 Steps done!,  367750\n",
      "250 Steps done!,  368000\n",
      "-------------------------------------------------------------\n",
      "Episode:  341\n",
      "250 Steps done!,  368250\n",
      "250 Steps done!,  368500\n",
      "250 Steps done!,  368750\n",
      "250 Steps done!,  369000\n",
      "250 Steps done!,  369250\n",
      "-------------------------------------------------------------\n",
      "Episode:  342\n",
      "250 Steps done!,  369500\n",
      "250 Steps done!,  369750\n",
      "250 Steps done!,  370000\n",
      "250 Steps done!,  370250\n",
      "-------------------------------------------------------------\n",
      "Episode:  343\n",
      "250 Steps done!,  370500\n",
      "250 Steps done!,  370750\n",
      "250 Steps done!,  371000\n",
      "250 Steps done!,  371250\n",
      "-------------------------------------------------------------\n",
      "Episode:  344\n",
      "250 Steps done!,  371500\n",
      "250 Steps done!,  371750\n",
      "250 Steps done!,  372000\n",
      "250 Steps done!,  372250\n",
      "250 Steps done!,  372500\n",
      "-------------------------------------------------------------\n",
      "Episode:  345\n",
      "250 Steps done!,  372750\n",
      "250 Steps done!,  373000\n",
      "250 Steps done!,  373250\n",
      "250 Steps done!,  373500\n",
      "-------------------------------------------------------------\n",
      "Episode:  346\n",
      "250 Steps done!,  373750\n",
      "250 Steps done!,  374000\n",
      "250 Steps done!,  374250\n",
      "250 Steps done!,  374500\n",
      "-------------------------------------------------------------\n",
      "Episode:  347\n",
      "250 Steps done!,  374750\n",
      "250 Steps done!,  375000\n",
      "250 Steps done!,  375250\n",
      "250 Steps done!,  375500\n",
      "250 Steps done!,  375750\n",
      "-------------------------------------------------------------\n",
      "Episode:  348\n",
      "250 Steps done!,  376000\n",
      "250 Steps done!,  376250\n",
      "250 Steps done!,  376500\n",
      "250 Steps done!,  376750\n",
      "-------------------------------------------------------------\n",
      "Episode:  349\n",
      "250 Steps done!,  377000\n",
      "250 Steps done!,  377250\n",
      "250 Steps done!,  377500\n",
      "250 Steps done!,  377750\n",
      "-------------------------------------------------------------\n",
      "Episode:  350\n",
      "250 Steps done!,  378000\n",
      "250 Steps done!,  378250\n",
      "250 Steps done!,  378500\n",
      "250 Steps done!,  378750\n",
      "250 Steps done!,  379000\n",
      "-------------------------------------------------------------\n",
      "Episode:  351\n",
      "250 Steps done!,  379250\n",
      "250 Steps done!,  379500\n",
      "250 Steps done!,  379750\n",
      "250 Steps done!,  380000\n",
      "-------------------------------------------------------------\n",
      "Episode:  352\n",
      "250 Steps done!,  380250\n",
      "250 Steps done!,  380500\n",
      "250 Steps done!,  380750\n",
      "250 Steps done!,  381000\n",
      "-------------------------------------------------------------\n",
      "Episode:  353\n",
      "250 Steps done!,  381250\n",
      "250 Steps done!,  381500\n",
      "250 Steps done!,  381750\n",
      "250 Steps done!,  382000\n",
      "250 Steps done!,  382250\n",
      "-------------------------------------------------------------\n",
      "Episode:  354\n",
      "250 Steps done!,  382500\n",
      "250 Steps done!,  382750\n",
      "250 Steps done!,  383000\n",
      "250 Steps done!,  383250\n",
      "-------------------------------------------------------------\n",
      "Episode:  355\n",
      "250 Steps done!,  383500\n",
      "250 Steps done!,  383750\n",
      "250 Steps done!,  384000\n",
      "250 Steps done!,  384250\n",
      "-------------------------------------------------------------\n",
      "Episode:  356\n",
      "250 Steps done!,  384500\n",
      "250 Steps done!,  384750\n",
      "250 Steps done!,  385000\n",
      "250 Steps done!,  385250\n",
      "-------------------------------------------------------------\n",
      "Episode:  357\n",
      "250 Steps done!,  385500\n",
      "250 Steps done!,  385750\n",
      "250 Steps done!,  386000\n",
      "250 Steps done!,  386250\n",
      "250 Steps done!,  386500\n",
      "-------------------------------------------------------------\n",
      "Episode:  358\n",
      "250 Steps done!,  386750\n",
      "250 Steps done!,  387000\n",
      "250 Steps done!,  387250\n",
      "250 Steps done!,  387500\n",
      "-------------------------------------------------------------\n",
      "Episode:  359\n",
      "250 Steps done!,  387750\n",
      "250 Steps done!,  388000\n",
      "250 Steps done!,  388250\n",
      "250 Steps done!,  388500\n",
      "-------------------------------------------------------------\n",
      "Episode:  360\n",
      "250 Steps done!,  388750\n",
      "250 Steps done!,  389000\n",
      "250 Steps done!,  389250\n",
      "250 Steps done!,  389500\n",
      "250 Steps done!,  389750\n",
      "-------------------------------------------------------------\n",
      "Episode:  361\n",
      "250 Steps done!,  390000\n",
      "250 Steps done!,  390250\n",
      "250 Steps done!,  390500\n",
      "250 Steps done!,  390750\n",
      "-------------------------------------------------------------\n",
      "Episode:  362\n",
      "250 Steps done!,  391000\n",
      "250 Steps done!,  391250\n",
      "250 Steps done!,  391500\n",
      "250 Steps done!,  391750\n",
      "-------------------------------------------------------------\n",
      "Episode:  363\n",
      "250 Steps done!,  392000\n",
      "250 Steps done!,  392250\n",
      "250 Steps done!,  392500\n",
      "250 Steps done!,  392750\n",
      "250 Steps done!,  393000\n",
      "-------------------------------------------------------------\n",
      "Episode:  364\n",
      "250 Steps done!,  393250\n",
      "250 Steps done!,  393500\n",
      "250 Steps done!,  393750\n",
      "250 Steps done!,  394000\n",
      "-------------------------------------------------------------\n",
      "Episode:  365\n",
      "250 Steps done!,  394250\n",
      "250 Steps done!,  394500\n",
      "250 Steps done!,  394750\n",
      "250 Steps done!,  395000\n",
      "-------------------------------------------------------------\n",
      "Episode:  366\n",
      "250 Steps done!,  395250\n",
      "250 Steps done!,  395500\n",
      "250 Steps done!,  395750\n",
      "250 Steps done!,  396000\n",
      "250 Steps done!,  396250\n",
      "-------------------------------------------------------------\n",
      "Episode:  367\n",
      "250 Steps done!,  396500\n",
      "250 Steps done!,  396750\n",
      "250 Steps done!,  397000\n",
      "250 Steps done!,  397250\n",
      "-------------------------------------------------------------\n",
      "Episode:  368\n",
      "250 Steps done!,  397500\n",
      "250 Steps done!,  397750\n",
      "250 Steps done!,  398000\n",
      "250 Steps done!,  398250\n",
      "-------------------------------------------------------------\n",
      "Episode:  369\n",
      "250 Steps done!,  398500\n",
      "250 Steps done!,  398750\n",
      "250 Steps done!,  399000\n",
      "250 Steps done!,  399250\n",
      "250 Steps done!,  399500\n",
      "-------------------------------------------------------------\n",
      "Episode:  370\n",
      "250 Steps done!,  399750\n",
      "250 Steps done!,  400000\n",
      "250 Steps done!,  400250\n",
      "250 Steps done!,  400500\n",
      "-------------------------------------------------------------\n",
      "Episode:  371\n",
      "250 Steps done!,  400750\n",
      "250 Steps done!,  401000\n",
      "250 Steps done!,  401250\n",
      "250 Steps done!,  401500\n",
      "-------------------------------------------------------------\n",
      "Episode:  372\n",
      "250 Steps done!,  401750\n",
      "250 Steps done!,  402000\n",
      "250 Steps done!,  402250\n",
      "250 Steps done!,  402500\n",
      "250 Steps done!,  402750\n",
      "-------------------------------------------------------------\n",
      "Episode:  373\n",
      "250 Steps done!,  403000\n",
      "250 Steps done!,  403250\n",
      "250 Steps done!,  403500\n",
      "250 Steps done!,  403750\n",
      "-------------------------------------------------------------\n",
      "Episode:  374\n",
      "250 Steps done!,  404000\n",
      "250 Steps done!,  404250\n",
      "250 Steps done!,  404500\n",
      "250 Steps done!,  404750\n",
      "-------------------------------------------------------------\n",
      "Episode:  375\n",
      "250 Steps done!,  405000\n",
      "250 Steps done!,  405250\n",
      "250 Steps done!,  405500\n",
      "250 Steps done!,  405750\n",
      "250 Steps done!,  406000\n",
      "-------------------------------------------------------------\n",
      "Episode:  376\n",
      "250 Steps done!,  406250\n",
      "250 Steps done!,  406500\n",
      "250 Steps done!,  406750\n",
      "250 Steps done!,  407000\n",
      "-------------------------------------------------------------\n",
      "Episode:  377\n",
      "250 Steps done!,  407250\n",
      "250 Steps done!,  407500\n",
      "250 Steps done!,  407750\n",
      "250 Steps done!,  408000\n",
      "-------------------------------------------------------------\n",
      "Episode:  378\n",
      "250 Steps done!,  408250\n",
      "250 Steps done!,  408500\n",
      "250 Steps done!,  408750\n",
      "250 Steps done!,  409000\n",
      "250 Steps done!,  409250\n",
      "-------------------------------------------------------------\n",
      "Episode:  379\n",
      "250 Steps done!,  409500\n",
      "250 Steps done!,  409750\n",
      "250 Steps done!,  410000\n",
      "250 Steps done!,  410250\n",
      "-------------------------------------------------------------\n",
      "Episode:  380\n",
      "250 Steps done!,  410500\n",
      "250 Steps done!,  410750\n",
      "250 Steps done!,  411000\n",
      "250 Steps done!,  411250\n",
      "-------------------------------------------------------------\n",
      "Episode:  381\n",
      "250 Steps done!,  411500\n",
      "250 Steps done!,  411750\n",
      "250 Steps done!,  412000\n",
      "250 Steps done!,  412250\n",
      "-------------------------------------------------------------\n",
      "Episode:  382\n",
      "250 Steps done!,  412500\n",
      "250 Steps done!,  412750\n",
      "250 Steps done!,  413000\n",
      "250 Steps done!,  413250\n",
      "250 Steps done!,  413500\n",
      "-------------------------------------------------------------\n",
      "Episode:  383\n",
      "250 Steps done!,  413750\n",
      "250 Steps done!,  414000\n",
      "250 Steps done!,  414250\n",
      "250 Steps done!,  414500\n",
      "-------------------------------------------------------------\n",
      "Episode:  384\n",
      "250 Steps done!,  414750\n",
      "250 Steps done!,  415000\n",
      "250 Steps done!,  415250\n",
      "250 Steps done!,  415500\n",
      "-------------------------------------------------------------\n",
      "Episode:  385\n",
      "250 Steps done!,  415750\n",
      "250 Steps done!,  416000\n",
      "250 Steps done!,  416250\n",
      "250 Steps done!,  416500\n",
      "250 Steps done!,  416750\n",
      "-------------------------------------------------------------\n",
      "Episode:  386\n",
      "250 Steps done!,  417000\n",
      "250 Steps done!,  417250\n",
      "250 Steps done!,  417500\n",
      "250 Steps done!,  417750\n",
      "-------------------------------------------------------------\n",
      "Episode:  387\n",
      "250 Steps done!,  418000\n",
      "250 Steps done!,  418250\n",
      "250 Steps done!,  418500\n",
      "250 Steps done!,  418750\n",
      "-------------------------------------------------------------\n",
      "Episode:  388\n",
      "250 Steps done!,  419000\n",
      "250 Steps done!,  419250\n",
      "250 Steps done!,  419500\n",
      "250 Steps done!,  419750\n",
      "250 Steps done!,  420000\n",
      "-------------------------------------------------------------\n",
      "Episode:  389\n",
      "250 Steps done!,  420250\n",
      "250 Steps done!,  420500\n",
      "250 Steps done!,  420750\n",
      "250 Steps done!,  421000\n",
      "-------------------------------------------------------------\n",
      "Episode:  390\n",
      "250 Steps done!,  421250\n",
      "250 Steps done!,  421500\n",
      "250 Steps done!,  421750\n",
      "250 Steps done!,  422000\n",
      "-------------------------------------------------------------\n",
      "Episode:  391\n",
      "250 Steps done!,  422250\n",
      "250 Steps done!,  422500\n",
      "250 Steps done!,  422750\n",
      "250 Steps done!,  423000\n",
      "250 Steps done!,  423250\n",
      "-------------------------------------------------------------\n",
      "Episode:  392\n",
      "250 Steps done!,  423500\n",
      "250 Steps done!,  423750\n",
      "250 Steps done!,  424000\n",
      "250 Steps done!,  424250\n",
      "-------------------------------------------------------------\n",
      "Episode:  393\n",
      "250 Steps done!,  424500\n",
      "250 Steps done!,  424750\n",
      "250 Steps done!,  425000\n",
      "250 Steps done!,  425250\n",
      "-------------------------------------------------------------\n",
      "Episode:  394\n",
      "250 Steps done!,  425500\n",
      "250 Steps done!,  425750\n",
      "250 Steps done!,  426000\n",
      "250 Steps done!,  426250\n",
      "250 Steps done!,  426500\n",
      "-------------------------------------------------------------\n",
      "Episode:  395\n",
      "250 Steps done!,  426750\n",
      "250 Steps done!,  427000\n",
      "250 Steps done!,  427250\n",
      "250 Steps done!,  427500\n",
      "-------------------------------------------------------------\n",
      "Episode:  396\n",
      "250 Steps done!,  427750\n",
      "250 Steps done!,  428000\n",
      "250 Steps done!,  428250\n",
      "250 Steps done!,  428500\n",
      "-------------------------------------------------------------\n",
      "Episode:  397\n",
      "250 Steps done!,  428750\n",
      "250 Steps done!,  429000\n",
      "250 Steps done!,  429250\n",
      "250 Steps done!,  429500\n",
      "250 Steps done!,  429750\n",
      "-------------------------------------------------------------\n",
      "Episode:  398\n",
      "250 Steps done!,  430000\n",
      "250 Steps done!,  430250\n",
      "250 Steps done!,  430500\n",
      "250 Steps done!,  430750\n",
      "-------------------------------------------------------------\n",
      "Episode:  399\n",
      "250 Steps done!,  431000\n",
      "250 Steps done!,  431250\n",
      "250 Steps done!,  431500\n",
      "250 Steps done!,  431750\n",
      "-------------------------------------------------------------\n",
      "Episode:  400\n",
      "250 Steps done!,  432000\n",
      "250 Steps done!,  432250\n",
      "250 Steps done!,  432500\n",
      "250 Steps done!,  432750\n",
      "250 Steps done!,  433000\n",
      "-------------------------------------------------------------\n",
      "Episode:  401\n",
      "250 Steps done!,  433250\n",
      "250 Steps done!,  433500\n",
      "250 Steps done!,  433750\n",
      "250 Steps done!,  434000\n",
      "-------------------------------------------------------------\n",
      "Episode:  402\n",
      "250 Steps done!,  434250\n",
      "250 Steps done!,  434500\n",
      "250 Steps done!,  434750\n",
      "250 Steps done!,  435000\n",
      "-------------------------------------------------------------\n",
      "Episode:  403\n",
      "250 Steps done!,  435250\n",
      "250 Steps done!,  435500\n",
      "250 Steps done!,  435750\n",
      "250 Steps done!,  436000\n",
      "250 Steps done!,  436250\n",
      "-------------------------------------------------------------\n",
      "Episode:  404\n",
      "250 Steps done!,  436500\n",
      "250 Steps done!,  436750\n",
      "250 Steps done!,  437000\n",
      "250 Steps done!,  437250\n",
      "-------------------------------------------------------------\n",
      "Episode:  405\n",
      "250 Steps done!,  437500\n",
      "250 Steps done!,  437750\n",
      "250 Steps done!,  438000\n",
      "250 Steps done!,  438250\n",
      "-------------------------------------------------------------\n",
      "Episode:  406\n",
      "250 Steps done!,  438500\n",
      "250 Steps done!,  438750\n",
      "250 Steps done!,  439000\n",
      "250 Steps done!,  439250\n",
      "-------------------------------------------------------------\n",
      "Episode:  407\n",
      "250 Steps done!,  439500\n",
      "250 Steps done!,  439750\n",
      "250 Steps done!,  440000\n",
      "250 Steps done!,  440250\n",
      "250 Steps done!,  440500\n",
      "-------------------------------------------------------------\n",
      "Episode:  408\n",
      "250 Steps done!,  440750\n",
      "250 Steps done!,  441000\n",
      "250 Steps done!,  441250\n",
      "250 Steps done!,  441500\n",
      "-------------------------------------------------------------\n",
      "Episode:  409\n",
      "250 Steps done!,  441750\n",
      "250 Steps done!,  442000\n",
      "250 Steps done!,  442250\n",
      "250 Steps done!,  442500\n",
      "-------------------------------------------------------------\n",
      "Episode:  410\n",
      "250 Steps done!,  442750\n",
      "250 Steps done!,  443000\n",
      "250 Steps done!,  443250\n",
      "250 Steps done!,  443500\n",
      "250 Steps done!,  443750\n",
      "-------------------------------------------------------------\n",
      "Episode:  411\n",
      "250 Steps done!,  444000\n",
      "250 Steps done!,  444250\n",
      "250 Steps done!,  444500\n",
      "250 Steps done!,  444750\n",
      "-------------------------------------------------------------\n",
      "Episode:  412\n",
      "250 Steps done!,  445000\n",
      "250 Steps done!,  445250\n",
      "250 Steps done!,  445500\n",
      "250 Steps done!,  445750\n",
      "-------------------------------------------------------------\n",
      "Episode:  413\n",
      "250 Steps done!,  446000\n",
      "250 Steps done!,  446250\n",
      "250 Steps done!,  446500\n",
      "250 Steps done!,  446750\n",
      "250 Steps done!,  447000\n",
      "-------------------------------------------------------------\n",
      "Episode:  414\n",
      "250 Steps done!,  447250\n",
      "250 Steps done!,  447500\n",
      "250 Steps done!,  447750\n",
      "250 Steps done!,  448000\n",
      "-------------------------------------------------------------\n",
      "Episode:  415\n",
      "250 Steps done!,  448250\n",
      "250 Steps done!,  448500\n",
      "250 Steps done!,  448750\n",
      "250 Steps done!,  449000\n",
      "-------------------------------------------------------------\n",
      "Episode:  416\n",
      "250 Steps done!,  449250\n",
      "250 Steps done!,  449500\n",
      "250 Steps done!,  449750\n",
      "250 Steps done!,  450000\n",
      "250 Steps done!,  450250\n",
      "-------------------------------------------------------------\n",
      "Episode:  417\n",
      "250 Steps done!,  450500\n",
      "250 Steps done!,  450750\n",
      "250 Steps done!,  451000\n",
      "250 Steps done!,  451250\n",
      "-------------------------------------------------------------\n",
      "Episode:  418\n",
      "250 Steps done!,  451500\n",
      "250 Steps done!,  451750\n",
      "250 Steps done!,  452000\n",
      "250 Steps done!,  452250\n",
      "-------------------------------------------------------------\n",
      "Episode:  419\n",
      "250 Steps done!,  452500\n",
      "250 Steps done!,  452750\n",
      "250 Steps done!,  453000\n",
      "250 Steps done!,  453250\n",
      "250 Steps done!,  453500\n",
      "-------------------------------------------------------------\n",
      "Episode:  420\n",
      "250 Steps done!,  453750\n",
      "250 Steps done!,  454000\n",
      "250 Steps done!,  454250\n",
      "250 Steps done!,  454500\n",
      "-------------------------------------------------------------\n",
      "Episode:  421\n",
      "250 Steps done!,  454750\n",
      "250 Steps done!,  455000\n",
      "250 Steps done!,  455250\n",
      "250 Steps done!,  455500\n",
      "-------------------------------------------------------------\n",
      "Episode:  422\n",
      "250 Steps done!,  455750\n",
      "250 Steps done!,  456000\n",
      "250 Steps done!,  456250\n",
      "250 Steps done!,  456500\n",
      "250 Steps done!,  456750\n",
      "-------------------------------------------------------------\n",
      "Episode:  423\n",
      "250 Steps done!,  457000\n",
      "250 Steps done!,  457250\n",
      "250 Steps done!,  457500\n",
      "250 Steps done!,  457750\n",
      "-------------------------------------------------------------\n",
      "Episode:  424\n",
      "250 Steps done!,  458000\n",
      "250 Steps done!,  458250\n",
      "250 Steps done!,  458500\n",
      "250 Steps done!,  458750\n",
      "-------------------------------------------------------------\n",
      "Episode:  425\n",
      "250 Steps done!,  459000\n",
      "250 Steps done!,  459250\n",
      "250 Steps done!,  459500\n",
      "250 Steps done!,  459750\n",
      "250 Steps done!,  460000\n",
      "-------------------------------------------------------------\n",
      "Episode:  426\n",
      "250 Steps done!,  460250\n",
      "250 Steps done!,  460500\n",
      "250 Steps done!,  460750\n",
      "250 Steps done!,  461000\n",
      "-------------------------------------------------------------\n",
      "Episode:  427\n",
      "250 Steps done!,  461250\n",
      "250 Steps done!,  461500\n",
      "250 Steps done!,  461750\n",
      "250 Steps done!,  462000\n",
      "-------------------------------------------------------------\n",
      "Episode:  428\n",
      "250 Steps done!,  462250\n",
      "250 Steps done!,  462500\n",
      "250 Steps done!,  462750\n",
      "250 Steps done!,  463000\n",
      "250 Steps done!,  463250\n",
      "-------------------------------------------------------------\n",
      "Episode:  429\n",
      "250 Steps done!,  463500\n",
      "250 Steps done!,  463750\n",
      "250 Steps done!,  464000\n",
      "250 Steps done!,  464250\n",
      "-------------------------------------------------------------\n",
      "Episode:  430\n",
      "250 Steps done!,  464500\n",
      "250 Steps done!,  464750\n",
      "250 Steps done!,  465000\n",
      "250 Steps done!,  465250\n",
      "-------------------------------------------------------------\n",
      "Episode:  431\n",
      "250 Steps done!,  465500\n",
      "250 Steps done!,  465750\n",
      "250 Steps done!,  466000\n",
      "250 Steps done!,  466250\n",
      "-------------------------------------------------------------\n",
      "Episode:  432\n",
      "250 Steps done!,  466500\n",
      "250 Steps done!,  466750\n",
      "250 Steps done!,  467000\n",
      "250 Steps done!,  467250\n",
      "250 Steps done!,  467500\n",
      "-------------------------------------------------------------\n",
      "Episode:  433\n",
      "250 Steps done!,  467750\n",
      "250 Steps done!,  468000\n",
      "250 Steps done!,  468250\n",
      "250 Steps done!,  468500\n",
      "-------------------------------------------------------------\n",
      "Episode:  434\n",
      "250 Steps done!,  468750\n",
      "250 Steps done!,  469000\n",
      "250 Steps done!,  469250\n",
      "250 Steps done!,  469500\n",
      "-------------------------------------------------------------\n",
      "Episode:  435\n",
      "250 Steps done!,  469750\n",
      "250 Steps done!,  470000\n",
      "250 Steps done!,  470250\n",
      "250 Steps done!,  470500\n",
      "250 Steps done!,  470750\n",
      "-------------------------------------------------------------\n",
      "Episode:  436\n",
      "250 Steps done!,  471000\n",
      "250 Steps done!,  471250\n",
      "250 Steps done!,  471500\n",
      "250 Steps done!,  471750\n",
      "-------------------------------------------------------------\n",
      "Episode:  437\n",
      "250 Steps done!,  472000\n",
      "250 Steps done!,  472250\n",
      "250 Steps done!,  472500\n",
      "250 Steps done!,  472750\n",
      "-------------------------------------------------------------\n",
      "Episode:  438\n",
      "250 Steps done!,  473000\n",
      "250 Steps done!,  473250\n",
      "250 Steps done!,  473500\n",
      "250 Steps done!,  473750\n",
      "250 Steps done!,  474000\n",
      "-------------------------------------------------------------\n",
      "Episode:  439\n",
      "250 Steps done!,  474250\n",
      "250 Steps done!,  474500\n",
      "250 Steps done!,  474750\n",
      "250 Steps done!,  475000\n",
      "-------------------------------------------------------------\n",
      "Episode:  440\n",
      "250 Steps done!,  475250\n",
      "250 Steps done!,  475500\n",
      "250 Steps done!,  475750\n",
      "250 Steps done!,  476000\n",
      "-------------------------------------------------------------\n",
      "Episode:  441\n",
      "250 Steps done!,  476250\n",
      "250 Steps done!,  476500\n",
      "250 Steps done!,  476750\n",
      "250 Steps done!,  477000\n",
      "250 Steps done!,  477250\n",
      "-------------------------------------------------------------\n",
      "Episode:  442\n",
      "250 Steps done!,  477500\n",
      "250 Steps done!,  477750\n",
      "250 Steps done!,  478000\n",
      "250 Steps done!,  478250\n",
      "-------------------------------------------------------------\n",
      "Episode:  443\n",
      "250 Steps done!,  478500\n",
      "250 Steps done!,  478750\n",
      "250 Steps done!,  479000\n",
      "250 Steps done!,  479250\n",
      "-------------------------------------------------------------\n",
      "Episode:  444\n",
      "250 Steps done!,  479500\n",
      "250 Steps done!,  479750\n",
      "250 Steps done!,  480000\n",
      "250 Steps done!,  480250\n",
      "250 Steps done!,  480500\n",
      "-------------------------------------------------------------\n",
      "Episode:  445\n",
      "250 Steps done!,  480750\n",
      "250 Steps done!,  481000\n",
      "250 Steps done!,  481250\n",
      "250 Steps done!,  481500\n",
      "-------------------------------------------------------------\n",
      "Episode:  446\n",
      "250 Steps done!,  481750\n",
      "250 Steps done!,  482000\n",
      "250 Steps done!,  482250\n",
      "250 Steps done!,  482500\n",
      "-------------------------------------------------------------\n",
      "Episode:  447\n",
      "250 Steps done!,  482750\n",
      "250 Steps done!,  483000\n",
      "250 Steps done!,  483250\n",
      "250 Steps done!,  483500\n",
      "250 Steps done!,  483750\n",
      "-------------------------------------------------------------\n",
      "Episode:  448\n",
      "250 Steps done!,  484000\n",
      "250 Steps done!,  484250\n",
      "250 Steps done!,  484500\n",
      "250 Steps done!,  484750\n",
      "-------------------------------------------------------------\n",
      "Episode:  449\n",
      "250 Steps done!,  485000\n",
      "250 Steps done!,  485250\n",
      "250 Steps done!,  485500\n",
      "250 Steps done!,  485750\n",
      "-------------------------------------------------------------\n",
      "Episode:  450\n",
      "250 Steps done!,  486000\n",
      "250 Steps done!,  486250\n",
      "250 Steps done!,  486500\n",
      "250 Steps done!,  486750\n",
      "250 Steps done!,  487000\n",
      "-------------------------------------------------------------\n",
      "Episode:  451\n",
      "250 Steps done!,  487250\n",
      "250 Steps done!,  487500\n",
      "250 Steps done!,  487750\n",
      "250 Steps done!,  488000\n",
      "-------------------------------------------------------------\n",
      "Episode:  452\n",
      "250 Steps done!,  488250\n",
      "250 Steps done!,  488500\n",
      "250 Steps done!,  488750\n",
      "250 Steps done!,  489000\n",
      "-------------------------------------------------------------\n",
      "Episode:  453\n",
      "250 Steps done!,  489250\n",
      "250 Steps done!,  489500\n",
      "250 Steps done!,  489750\n",
      "250 Steps done!,  490000\n",
      "250 Steps done!,  490250\n",
      "-------------------------------------------------------------\n",
      "Episode:  454\n",
      "250 Steps done!,  490500\n",
      "250 Steps done!,  490750\n",
      "250 Steps done!,  491000\n",
      "250 Steps done!,  491250\n",
      "-------------------------------------------------------------\n",
      "Episode:  455\n",
      "250 Steps done!,  491500\n",
      "250 Steps done!,  491750\n",
      "250 Steps done!,  492000\n",
      "250 Steps done!,  492250\n",
      "-------------------------------------------------------------\n",
      "Episode:  456\n",
      "250 Steps done!,  492500\n",
      "250 Steps done!,  492750\n",
      "250 Steps done!,  493000\n",
      "250 Steps done!,  493250\n",
      "-------------------------------------------------------------\n",
      "Episode:  457\n",
      "250 Steps done!,  493500\n",
      "250 Steps done!,  493750\n",
      "250 Steps done!,  494000\n",
      "250 Steps done!,  494250\n",
      "250 Steps done!,  494500\n",
      "-------------------------------------------------------------\n",
      "Episode:  458\n",
      "250 Steps done!,  494750\n",
      "250 Steps done!,  495000\n",
      "250 Steps done!,  495250\n",
      "250 Steps done!,  495500\n",
      "-------------------------------------------------------------\n",
      "Episode:  459\n",
      "250 Steps done!,  495750\n",
      "250 Steps done!,  496000\n",
      "250 Steps done!,  496250\n",
      "250 Steps done!,  496500\n",
      "-------------------------------------------------------------\n",
      "Episode:  460\n",
      "250 Steps done!,  496750\n",
      "250 Steps done!,  497000\n",
      "250 Steps done!,  497250\n",
      "250 Steps done!,  497500\n",
      "250 Steps done!,  497750\n",
      "-------------------------------------------------------------\n",
      "Episode:  461\n",
      "250 Steps done!,  498000\n",
      "250 Steps done!,  498250\n",
      "250 Steps done!,  498500\n",
      "250 Steps done!,  498750\n",
      "-------------------------------------------------------------\n",
      "Episode:  462\n",
      "250 Steps done!,  499000\n",
      "250 Steps done!,  499250\n",
      "250 Steps done!,  499500\n",
      "250 Steps done!,  499750\n",
      "-------------------------------------------------------------\n",
      "Episode:  463\n",
      "250 Steps done!,  500000\n",
      "250 Steps done!,  500250\n",
      "250 Steps done!,  500500\n",
      "250 Steps done!,  500750\n",
      "250 Steps done!,  501000\n",
      "-------------------------------------------------------------\n",
      "Episode:  464\n",
      "250 Steps done!,  501250\n",
      "250 Steps done!,  501500\n",
      "250 Steps done!,  501750\n",
      "250 Steps done!,  502000\n",
      "-------------------------------------------------------------\n",
      "Episode:  465\n",
      "250 Steps done!,  502250\n",
      "250 Steps done!,  502500\n",
      "250 Steps done!,  502750\n",
      "250 Steps done!,  503000\n",
      "-------------------------------------------------------------\n",
      "Episode:  466\n",
      "250 Steps done!,  503250\n",
      "250 Steps done!,  503500\n",
      "250 Steps done!,  503750\n",
      "250 Steps done!,  504000\n",
      "250 Steps done!,  504250\n",
      "-------------------------------------------------------------\n",
      "Episode:  467\n",
      "250 Steps done!,  504500\n",
      "250 Steps done!,  504750\n",
      "250 Steps done!,  505000\n",
      "250 Steps done!,  505250\n",
      "-------------------------------------------------------------\n",
      "Episode:  468\n",
      "250 Steps done!,  505500\n",
      "250 Steps done!,  505750\n",
      "250 Steps done!,  506000\n",
      "250 Steps done!,  506250\n",
      "-------------------------------------------------------------\n",
      "Episode:  469\n",
      "250 Steps done!,  506500\n",
      "250 Steps done!,  506750\n",
      "250 Steps done!,  507000\n",
      "250 Steps done!,  507250\n",
      "250 Steps done!,  507500\n",
      "-------------------------------------------------------------\n",
      "Episode:  470\n",
      "250 Steps done!,  507750\n",
      "250 Steps done!,  508000\n",
      "250 Steps done!,  508250\n",
      "250 Steps done!,  508500\n",
      "-------------------------------------------------------------\n",
      "Episode:  471\n",
      "250 Steps done!,  508750\n",
      "250 Steps done!,  509000\n",
      "250 Steps done!,  509250\n",
      "250 Steps done!,  509500\n",
      "-------------------------------------------------------------\n",
      "Episode:  472\n",
      "250 Steps done!,  509750\n",
      "250 Steps done!,  510000\n",
      "250 Steps done!,  510250\n",
      "250 Steps done!,  510500\n",
      "250 Steps done!,  510750\n",
      "-------------------------------------------------------------\n",
      "Episode:  473\n",
      "250 Steps done!,  511000\n",
      "250 Steps done!,  511250\n",
      "250 Steps done!,  511500\n",
      "250 Steps done!,  511750\n",
      "-------------------------------------------------------------\n",
      "Episode:  474\n",
      "250 Steps done!,  512000\n",
      "250 Steps done!,  512250\n",
      "250 Steps done!,  512500\n",
      "250 Steps done!,  512750\n",
      "-------------------------------------------------------------\n",
      "Episode:  475\n",
      "250 Steps done!,  513000\n",
      "250 Steps done!,  513250\n",
      "250 Steps done!,  513500\n",
      "250 Steps done!,  513750\n",
      "250 Steps done!,  514000\n",
      "-------------------------------------------------------------\n",
      "Episode:  476\n",
      "250 Steps done!,  514250\n",
      "250 Steps done!,  514500\n",
      "250 Steps done!,  514750\n",
      "250 Steps done!,  515000\n",
      "-------------------------------------------------------------\n",
      "Episode:  477\n",
      "250 Steps done!,  515250\n",
      "250 Steps done!,  515500\n",
      "250 Steps done!,  515750\n",
      "250 Steps done!,  516000\n",
      "-------------------------------------------------------------\n",
      "Episode:  478\n",
      "250 Steps done!,  516250\n",
      "250 Steps done!,  516500\n",
      "250 Steps done!,  516750\n",
      "250 Steps done!,  517000\n",
      "250 Steps done!,  517250\n",
      "-------------------------------------------------------------\n",
      "Episode:  479\n",
      "250 Steps done!,  517500\n",
      "250 Steps done!,  517750\n",
      "250 Steps done!,  518000\n",
      "250 Steps done!,  518250\n",
      "-------------------------------------------------------------\n",
      "Episode:  480\n",
      "250 Steps done!,  518500\n",
      "250 Steps done!,  518750\n",
      "250 Steps done!,  519000\n",
      "250 Steps done!,  519250\n",
      "-------------------------------------------------------------\n",
      "Episode:  481\n",
      "250 Steps done!,  519500\n",
      "250 Steps done!,  519750\n",
      "250 Steps done!,  520000\n",
      "250 Steps done!,  520250\n",
      "-------------------------------------------------------------\n",
      "Episode:  482\n",
      "250 Steps done!,  520500\n",
      "250 Steps done!,  520750\n",
      "250 Steps done!,  521000\n",
      "250 Steps done!,  521250\n",
      "250 Steps done!,  521500\n",
      "-------------------------------------------------------------\n",
      "Episode:  483\n",
      "250 Steps done!,  521750\n",
      "250 Steps done!,  522000\n",
      "250 Steps done!,  522250\n",
      "250 Steps done!,  522500\n",
      "-------------------------------------------------------------\n",
      "Episode:  484\n",
      "250 Steps done!,  522750\n",
      "250 Steps done!,  523000\n",
      "250 Steps done!,  523250\n",
      "250 Steps done!,  523500\n",
      "-------------------------------------------------------------\n",
      "Episode:  485\n",
      "250 Steps done!,  523750\n",
      "250 Steps done!,  524000\n",
      "250 Steps done!,  524250\n",
      "250 Steps done!,  524500\n",
      "250 Steps done!,  524750\n",
      "-------------------------------------------------------------\n",
      "Episode:  486\n",
      "250 Steps done!,  525000\n",
      "250 Steps done!,  525250\n",
      "250 Steps done!,  525500\n",
      "250 Steps done!,  525750\n",
      "-------------------------------------------------------------\n",
      "Episode:  487\n",
      "250 Steps done!,  526000\n",
      "250 Steps done!,  526250\n",
      "250 Steps done!,  526500\n",
      "250 Steps done!,  526750\n",
      "-------------------------------------------------------------\n",
      "Episode:  488\n",
      "250 Steps done!,  527000\n",
      "250 Steps done!,  527250\n",
      "250 Steps done!,  527500\n",
      "250 Steps done!,  527750\n",
      "250 Steps done!,  528000\n",
      "-------------------------------------------------------------\n",
      "Episode:  489\n",
      "250 Steps done!,  528250\n",
      "250 Steps done!,  528500\n",
      "250 Steps done!,  528750\n",
      "250 Steps done!,  529000\n",
      "-------------------------------------------------------------\n",
      "Episode:  490\n",
      "250 Steps done!,  529250\n",
      "250 Steps done!,  529500\n",
      "250 Steps done!,  529750\n",
      "250 Steps done!,  530000\n",
      "-------------------------------------------------------------\n",
      "Episode:  491\n",
      "250 Steps done!,  530250\n",
      "250 Steps done!,  530500\n",
      "250 Steps done!,  530750\n",
      "250 Steps done!,  531000\n",
      "250 Steps done!,  531250\n",
      "-------------------------------------------------------------\n",
      "Episode:  492\n",
      "250 Steps done!,  531500\n",
      "250 Steps done!,  531750\n",
      "250 Steps done!,  532000\n",
      "250 Steps done!,  532250\n",
      "-------------------------------------------------------------\n",
      "Episode:  493\n",
      "250 Steps done!,  532500\n",
      "250 Steps done!,  532750\n",
      "250 Steps done!,  533000\n",
      "250 Steps done!,  533250\n",
      "-------------------------------------------------------------\n",
      "Episode:  494\n",
      "250 Steps done!,  533500\n",
      "250 Steps done!,  533750\n",
      "250 Steps done!,  534000\n",
      "250 Steps done!,  534250\n",
      "250 Steps done!,  534500\n",
      "-------------------------------------------------------------\n",
      "Episode:  495\n",
      "250 Steps done!,  534750\n",
      "250 Steps done!,  535000\n",
      "250 Steps done!,  535250\n",
      "250 Steps done!,  535500\n",
      "-------------------------------------------------------------\n",
      "Episode:  496\n",
      "250 Steps done!,  535750\n",
      "250 Steps done!,  536000\n",
      "250 Steps done!,  536250\n",
      "250 Steps done!,  536500\n",
      "-------------------------------------------------------------\n",
      "Episode:  497\n",
      "250 Steps done!,  536750\n",
      "250 Steps done!,  537000\n",
      "250 Steps done!,  537250\n",
      "250 Steps done!,  537500\n",
      "250 Steps done!,  537750\n",
      "-------------------------------------------------------------\n",
      "Episode:  498\n",
      "250 Steps done!,  538000\n",
      "250 Steps done!,  538250\n",
      "250 Steps done!,  538500\n",
      "250 Steps done!,  538750\n",
      "-------------------------------------------------------------\n",
      "Episode:  499\n",
      "250 Steps done!,  539000\n",
      "250 Steps done!,  539250\n",
      "250 Steps done!,  539500\n",
      "250 Steps done!,  539750\n"
     ]
    }
   ],
   "source": [
    "episodeRewards = [] # to store the episode rewards for logging/plotting purposes\n",
    "stepsDone = 0\n",
    "# creating a loop for training\n",
    "for ep in range(PARAM_episodes):\n",
    "    print('-------------------------------------------------------------')\n",
    "    print(\"Episode: \", ep)\n",
    "    # reset the environment\n",
    "    state = env.reset()\n",
    "    epReward = []\n",
    "    done = False\n",
    "\n",
    "    # training in the episode\n",
    "    while not done:\n",
    "        if stepsDone%250==0:\n",
    "            print(\"250 Steps done!, \", stepsDone)\n",
    "        # select an action and perform it\n",
    "        action = selectAction(state, INUSE_epsilon)\n",
    "        nextSt, r, done = env.take_action(action)\n",
    "        # debug\n",
    "        # if env.numSteps == 10800:\n",
    "        #     print(\"\\n!!!10800 steps done!!!\\n\")\n",
    "\n",
    "        # store current state, action, reward, next state, done in replay memory\n",
    "        memory.insert(state, action, r, nextSt, done)\n",
    "\n",
    "        # update current state to next state\n",
    "        state = nextSt\n",
    "        epReward.append(r)\n",
    "\n",
    "        # update model parameters\n",
    "        # if memory does not have enough tuples to sample (batch_size), we cant sample so we continue the loop\n",
    "        if memory.size() < PARAM_batch_size:\n",
    "            continue\n",
    "        # else, we train the networks\n",
    "        # take a sample\n",
    "        stBatch, aBatch, rBatch, nstBatch, doneBatch = memory.sample(PARAM_batch_size)\n",
    "        stBatch = torch.tensor(stBatch, device=PARAM_device, dtype=torch.float32)\n",
    "        aBatch = torch.tensor(aBatch, device=PARAM_device).reshape(-1, aBatch.shape[0])\n",
    "        rBatch = torch.tensor(rBatch, device=PARAM_device)\n",
    "        nstBatch = torch.tensor(nstBatch, device=PARAM_device, dtype=torch.float32)\n",
    "        # getting Q values for current states\n",
    "        QVals = policyNet(stBatch).gather(1, aBatch)\n",
    "\n",
    "        # Updating q values using target network\n",
    "        with torch.no_grad():\n",
    "            # next state q values using target network\n",
    "            nextQVals = targetNet(nstBatch).max(1)[0]\n",
    "            # updating the q values for policy network\n",
    "            targetQVals = rBatch + PARAM_gamma * nextQVals\n",
    "\n",
    "        # calculating the loss\n",
    "        lossFunction = nn.MSELoss()\n",
    "        loss = lossFunction(QVals, targetQVals.unsqueeze(0))\n",
    "\n",
    "        # updating model weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # updaing the target network periodically\n",
    "        if ep%PARAM_target_update_freq==0:\n",
    "            # targetNetStateDict = targetNet.state_dict()\n",
    "            # policyNetStateDict = policyNet.state_dict()\n",
    "            # for key in policyNetStateDict:\n",
    "            #     targetNetStateDict[key] = policyNetStateDict*PARAM_tau\n",
    "            targetNet.load_state_dict(policyNet.state_dict())\n",
    "        \n",
    "        stepsDone+=1\n",
    "\n",
    "    # decay epsilon after episode is complete\n",
    "    epsilon = PARAM_epsilon_min + (PARAM_epsilon - PARAM_epsilon_min) * math.exp(-1. * stepsDone/PARAM_epsilon_decay)\n",
    "    episodeRewards.append(epReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4599442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to save everything\n",
    "import pickle\n",
    "# saving the models\n",
    "torch.save(targetNet, \"models/TargetNetv2.pth\")\n",
    "torch.save(policyNet, \"models/PolicyNetv2.pth\")\n",
    "\n",
    "# saving the lists\n",
    "with open('models/rewardListv2.pkl', 'wb') as f:\n",
    "    pickle.dump(episodeRewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2ffb4",
   "metadata": {},
   "source": [
    "# Loading models for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "766c54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityFlowEnv(10800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9cfb0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "policyNet = torch.load(os.path.join('models', 'PolicyNet.pth'), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f701c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetNet = torch.load(os.path.join('models', 'TargetNet.pth'), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e6f8ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6992938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1\n",
      "QVals:  tensor([-2509.2739, -2511.6631, -2511.0339, -2510.9023], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  2\n",
      "QVals:  tensor([-2540.1638, -2543.0718, -2542.4177, -2542.1948], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  3\n",
      "QVals:  tensor([-2540.4495, -2543.3518, -2542.7126, -2542.4890], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  4\n",
      "QVals:  tensor([-2541.4441, -2544.3401, -2543.7344, -2543.5154], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  5\n",
      "QVals:  tensor([-2541.1831, -2544.0750, -2543.4758, -2543.2632], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  6\n",
      "QVals:  tensor([-2541.3130, -2544.1824, -2543.6167, -2543.4387], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  7\n",
      "QVals:  tensor([-2541.5862, -2544.4421, -2543.8784, -2543.6929], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  8\n",
      "QVals:  tensor([-2541.6316, -2544.4839, -2543.9192, -2543.7473], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  9\n",
      "QVals:  tensor([-2541.6907, -2544.5388, -2543.9729, -2543.8162], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  10\n",
      "QVals:  tensor([-2542.2388, -2545.0667, -2544.5232, -2544.3936], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  11\n",
      "QVals:  tensor([-2542.0247, -2544.8616, -2544.3147, -2544.2085], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  12\n",
      "QVals:  tensor([-2542.2520, -2545.0764, -2544.5303, -2544.4321], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  13\n",
      "QVals:  tensor([-2542.3274, -2545.1487, -2544.6033, -2544.4971], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  14\n",
      "QVals:  tensor([-2544.5173, -2547.3120, -2546.7939, -2546.7227], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  15\n",
      "QVals:  tensor([-2545.1956, -2547.9905, -2547.4729, -2547.4048], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  16\n",
      "QVals:  tensor([-2548.9172, -2551.7175, -2551.1968, -2551.1262], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  17\n",
      "QVals:  tensor([-2550.0667, -2552.8792, -2552.3511, -2552.2820], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  18\n",
      "QVals:  tensor([-2568.1458, -2570.9912, -2570.4558, -2570.3594], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n",
      "Step:  19\n",
      "QVals:  tensor([-2568.8936, -2571.7122, -2571.1943, -2571.0959], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  tensor(0, device='cuda:0')\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = torch.tensor(env._getState(), device=PARAM_device, dtype=torch.float32) # starting state\n",
    "rewards = []\n",
    "step = 1\n",
    "while not done:\n",
    "    print(\"Step: \", step)\n",
    "    # get the prediction from the model\n",
    "    QVals = policyNet(state)\n",
    "    print(\"QVals: \", QVals)\n",
    "    # get the best action from QVals\n",
    "    bestAction = torch.argmax(QVals)\n",
    "    print(\"best Action: \", bestAction)\n",
    "    print('------------------------------')\n",
    "    # perform the action\n",
    "    nextState, r, done = env.take_action(bestAction)\n",
    "\n",
    "    # update the current state\n",
    "    state = torch.tensor(nextState, device=PARAM_device, dtype=torch.float32)\n",
    "    rewards.append(r)\n",
    "    step+=1\n",
    "    if step==20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d7395c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2568.8936, -2571.7122, -2571.1943, -2571.0959], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75e6d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93a8ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/rewardList.pkl', 'rb') as f:\n",
    "    rewardsList = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55da76a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-12,\n",
       " -13,\n",
       " -16,\n",
       " -16,\n",
       " -23,\n",
       " -21,\n",
       " -21,\n",
       " -19,\n",
       " -19,\n",
       " -21,\n",
       " -18,\n",
       " -16,\n",
       " -24,\n",
       " -22,\n",
       " -25,\n",
       " -27,\n",
       " -30,\n",
       " -27,\n",
       " -27,\n",
       " -24,\n",
       " -26,\n",
       " -26,\n",
       " -24,\n",
       " -21,\n",
       " -26,\n",
       " -26,\n",
       " -27,\n",
       " -29,\n",
       " -35,\n",
       " -32,\n",
       " -33,\n",
       " -36,\n",
       " -42,\n",
       " -47,\n",
       " -41,\n",
       " -32,\n",
       " -33,\n",
       " -30,\n",
       " -29,\n",
       " -26,\n",
       " -35,\n",
       " -34,\n",
       " -35,\n",
       " -32,\n",
       " -37,\n",
       " -42,\n",
       " -38,\n",
       " -35,\n",
       " -38,\n",
       " -36,\n",
       " -32,\n",
       " -29,\n",
       " -32,\n",
       " -28,\n",
       " -22,\n",
       " -22,\n",
       " -25,\n",
       " -28,\n",
       " -27,\n",
       " -26,\n",
       " -34,\n",
       " -36,\n",
       " -39,\n",
       " -38,\n",
       " -40,\n",
       " -37,\n",
       " -38,\n",
       " -34,\n",
       " -35,\n",
       " -38,\n",
       " -34,\n",
       " -26,\n",
       " -31,\n",
       " -28,\n",
       " -28,\n",
       " -30,\n",
       " -36,\n",
       " -35,\n",
       " -31,\n",
       " -31,\n",
       " -36,\n",
       " -41,\n",
       " -34,\n",
       " -26,\n",
       " -33,\n",
       " -31,\n",
       " -28,\n",
       " -25,\n",
       " -28,\n",
       " -22,\n",
       " -20,\n",
       " -19,\n",
       " -25,\n",
       " -30,\n",
       " -30,\n",
       " -29,\n",
       " -33,\n",
       " -34,\n",
       " -37,\n",
       " -39,\n",
       " -44,\n",
       " -41,\n",
       " -42,\n",
       " -38,\n",
       " -44,\n",
       " -44,\n",
       " -39,\n",
       " -37,\n",
       " -44,\n",
       " -38,\n",
       " -40,\n",
       " -37,\n",
       " -42,\n",
       " -33,\n",
       " -32,\n",
       " -28,\n",
       " -30,\n",
       " -33,\n",
       " -29,\n",
       " -21,\n",
       " -29,\n",
       " -30,\n",
       " -27,\n",
       " -27,\n",
       " -36,\n",
       " -35,\n",
       " -32,\n",
       " -30,\n",
       " -33,\n",
       " -30,\n",
       " -27,\n",
       " -22,\n",
       " -26,\n",
       " -24,\n",
       " -22,\n",
       " -23,\n",
       " -24,\n",
       " -20,\n",
       " -19,\n",
       " -21,\n",
       " -23,\n",
       " -22,\n",
       " -22,\n",
       " -21,\n",
       " -25,\n",
       " -21,\n",
       " -22,\n",
       " -21,\n",
       " -29,\n",
       " -28,\n",
       " -23,\n",
       " -26,\n",
       " -29,\n",
       " -27,\n",
       " -25,\n",
       " -22,\n",
       " -24,\n",
       " -25,\n",
       " -28,\n",
       " -24,\n",
       " -29,\n",
       " -23,\n",
       " -23,\n",
       " -24,\n",
       " -28,\n",
       " -33,\n",
       " -30,\n",
       " -21,\n",
       " -25,\n",
       " -26,\n",
       " -29,\n",
       " -24,\n",
       " -31,\n",
       " -23,\n",
       " -24,\n",
       " -23,\n",
       " -26,\n",
       " -31,\n",
       " -31,\n",
       " -22,\n",
       " -30,\n",
       " -27,\n",
       " -26,\n",
       " -21,\n",
       " -29,\n",
       " -22,\n",
       " -18,\n",
       " -19,\n",
       " -22,\n",
       " -21,\n",
       " -17,\n",
       " -16,\n",
       " -24,\n",
       " -26,\n",
       " -29,\n",
       " -24,\n",
       " -26,\n",
       " -22,\n",
       " -20,\n",
       " -20,\n",
       " -22,\n",
       " -21,\n",
       " -19,\n",
       " -18,\n",
       " -27,\n",
       " -29,\n",
       " -26,\n",
       " -27,\n",
       " -31,\n",
       " -29,\n",
       " -26,\n",
       " -30,\n",
       " -35,\n",
       " -33,\n",
       " -31,\n",
       " -28,\n",
       " -37,\n",
       " -31,\n",
       " -28,\n",
       " -28,\n",
       " -37,\n",
       " -29,\n",
       " -26,\n",
       " -25,\n",
       " -25,\n",
       " -27,\n",
       " -23,\n",
       " -22,\n",
       " -31,\n",
       " -29,\n",
       " -32,\n",
       " -34,\n",
       " -40,\n",
       " -30,\n",
       " -31,\n",
       " -32,\n",
       " -32,\n",
       " -33,\n",
       " -29,\n",
       " -26,\n",
       " -32,\n",
       " -26,\n",
       " -28,\n",
       " -30,\n",
       " -34,\n",
       " -28,\n",
       " -27,\n",
       " -26,\n",
       " -30,\n",
       " -35,\n",
       " -33,\n",
       " -24,\n",
       " -28,\n",
       " -27,\n",
       " -26,\n",
       " -26,\n",
       " -33,\n",
       " -31,\n",
       " -27,\n",
       " -27,\n",
       " -30,\n",
       " -29,\n",
       " -28,\n",
       " -24,\n",
       " -27,\n",
       " -27,\n",
       " -26,\n",
       " -28,\n",
       " -31,\n",
       " -25,\n",
       " -24,\n",
       " -26,\n",
       " -32,\n",
       " -30,\n",
       " -27,\n",
       " -19,\n",
       " -23,\n",
       " -24,\n",
       " -27,\n",
       " -29,\n",
       " -38,\n",
       " -29,\n",
       " -28,\n",
       " -29,\n",
       " -33,\n",
       " -32,\n",
       " -26,\n",
       " -21,\n",
       " -25,\n",
       " -22,\n",
       " -22,\n",
       " -22,\n",
       " -27,\n",
       " -25,\n",
       " -24,\n",
       " -26,\n",
       " -30,\n",
       " -33,\n",
       " -31,\n",
       " -26,\n",
       " -34,\n",
       " -35,\n",
       " -31,\n",
       " -30,\n",
       " -34,\n",
       " -30,\n",
       " -31,\n",
       " -32,\n",
       " -38,\n",
       " -39,\n",
       " -36,\n",
       " -33,\n",
       " -42,\n",
       " -40,\n",
       " -43,\n",
       " -43,\n",
       " -44,\n",
       " -40,\n",
       " -35,\n",
       " -32,\n",
       " -32,\n",
       " -32,\n",
       " -30,\n",
       " -23,\n",
       " -27,\n",
       " -27,\n",
       " -26,\n",
       " -27,\n",
       " -32,\n",
       " -31,\n",
       " -30,\n",
       " -32,\n",
       " -37,\n",
       " -42,\n",
       " -37,\n",
       " -35,\n",
       " -44,\n",
       " -38,\n",
       " -35,\n",
       " -33,\n",
       " -42,\n",
       " -41,\n",
       " -38,\n",
       " -35,\n",
       " -40,\n",
       " -39,\n",
       " -38,\n",
       " -29,\n",
       " -33,\n",
       " -31,\n",
       " -33,\n",
       " -35,\n",
       " -39,\n",
       " -37,\n",
       " -35,\n",
       " -36,\n",
       " -42,\n",
       " -43,\n",
       " -41,\n",
       " -37,\n",
       " -45,\n",
       " -46,\n",
       " -42,\n",
       " -38,\n",
       " -44,\n",
       " -41,\n",
       " -43,\n",
       " -39,\n",
       " -42,\n",
       " -43,\n",
       " -38,\n",
       " -30,\n",
       " -36,\n",
       " -36,\n",
       " -39,\n",
       " -36,\n",
       " -45,\n",
       " -38,\n",
       " -35,\n",
       " -35,\n",
       " -41,\n",
       " -43,\n",
       " -38,\n",
       " -34,\n",
       " -43,\n",
       " -38,\n",
       " -39,\n",
       " -41,\n",
       " -48,\n",
       " -40,\n",
       " -38,\n",
       " -34,\n",
       " -36,\n",
       " -35,\n",
       " -31,\n",
       " -24,\n",
       " -31,\n",
       " -30,\n",
       " -31,\n",
       " -33,\n",
       " -40,\n",
       " -35,\n",
       " -35,\n",
       " -38,\n",
       " -44,\n",
       " -41,\n",
       " -35,\n",
       " -32,\n",
       " -40,\n",
       " -41,\n",
       " -41,\n",
       " -37,\n",
       " -43,\n",
       " -40,\n",
       " -39,\n",
       " -35,\n",
       " -37,\n",
       " -40,\n",
       " -40,\n",
       " -36,\n",
       " -41,\n",
       " -42,\n",
       " -43,\n",
       " -45,\n",
       " -53,\n",
       " -44,\n",
       " -40,\n",
       " -41,\n",
       " -42,\n",
       " -42,\n",
       " -42,\n",
       " -36,\n",
       " -45,\n",
       " -47,\n",
       " -47,\n",
       " -42,\n",
       " -44,\n",
       " -41,\n",
       " -38,\n",
       " -35,\n",
       " -40,\n",
       " -45,\n",
       " -45,\n",
       " -44,\n",
       " -48,\n",
       " -44,\n",
       " -41,\n",
       " -38,\n",
       " -43,\n",
       " -35,\n",
       " -35,\n",
       " -34,\n",
       " -34,\n",
       " -38,\n",
       " -34,\n",
       " -28,\n",
       " -30,\n",
       " -28,\n",
       " -31,\n",
       " -27,\n",
       " -32,\n",
       " -25,\n",
       " -23,\n",
       " -20,\n",
       " -22,\n",
       " -27,\n",
       " -27,\n",
       " -25,\n",
       " -33,\n",
       " -29,\n",
       " -27,\n",
       " -24,\n",
       " -28,\n",
       " -24,\n",
       " -23,\n",
       " -24,\n",
       " -30,\n",
       " -27,\n",
       " -24,\n",
       " -21,\n",
       " -26,\n",
       " -27,\n",
       " -25,\n",
       " -20,\n",
       " -25,\n",
       " -21,\n",
       " -17,\n",
       " -17,\n",
       " -23,\n",
       " -28,\n",
       " -28,\n",
       " -19,\n",
       " -26,\n",
       " -23,\n",
       " -25,\n",
       " -19,\n",
       " -27,\n",
       " -20,\n",
       " -20,\n",
       " -22,\n",
       " -24,\n",
       " -25,\n",
       " -23,\n",
       " -20,\n",
       " -26,\n",
       " -26,\n",
       " -29,\n",
       " -28,\n",
       " -35,\n",
       " -26,\n",
       " -25,\n",
       " -27,\n",
       " -31,\n",
       " -34,\n",
       " -29,\n",
       " -26,\n",
       " -35,\n",
       " -36,\n",
       " -35,\n",
       " -35,\n",
       " -44,\n",
       " -35,\n",
       " -33,\n",
       " -35,\n",
       " -34,\n",
       " -36,\n",
       " -30,\n",
       " -27,\n",
       " -35,\n",
       " -31,\n",
       " -34,\n",
       " -36,\n",
       " -38,\n",
       " -30,\n",
       " -30,\n",
       " -34,\n",
       " -35,\n",
       " -40,\n",
       " -35,\n",
       " -27,\n",
       " -34,\n",
       " -32,\n",
       " -34,\n",
       " -30,\n",
       " -38,\n",
       " -31,\n",
       " -31,\n",
       " -33,\n",
       " -33,\n",
       " -30,\n",
       " -29,\n",
       " -28,\n",
       " -33,\n",
       " -34,\n",
       " -35,\n",
       " -31,\n",
       " -39,\n",
       " -37,\n",
       " -39,\n",
       " -35,\n",
       " -38,\n",
       " -39,\n",
       " -34,\n",
       " -32,\n",
       " -35,\n",
       " -30,\n",
       " -32,\n",
       " -29,\n",
       " -34,\n",
       " -27,\n",
       " -26,\n",
       " -25,\n",
       " -30,\n",
       " -29,\n",
       " -27,\n",
       " -26,\n",
       " -30,\n",
       " -30,\n",
       " -29,\n",
       " -24,\n",
       " -32,\n",
       " -25,\n",
       " -21,\n",
       " -20,\n",
       " -23,\n",
       " -26,\n",
       " -25,\n",
       " -22,\n",
       " -31,\n",
       " -33,\n",
       " -36,\n",
       " -34,\n",
       " -35,\n",
       " -31,\n",
       " -26,\n",
       " -25,\n",
       " -28,\n",
       " -29,\n",
       " -25,\n",
       " -17,\n",
       " -25,\n",
       " -24,\n",
       " -25,\n",
       " -25,\n",
       " -27,\n",
       " -20,\n",
       " -19,\n",
       " -19,\n",
       " -22,\n",
       " -26,\n",
       " -21,\n",
       " -19,\n",
       " -22,\n",
       " -22,\n",
       " -23,\n",
       " -20,\n",
       " -29,\n",
       " -25,\n",
       " -25,\n",
       " -21,\n",
       " -25,\n",
       " -29,\n",
       " -25,\n",
       " -22,\n",
       " -25,\n",
       " -23,\n",
       " -23,\n",
       " -18,\n",
       " -24,\n",
       " -18,\n",
       " -15,\n",
       " -16,\n",
       " -18,\n",
       " -21,\n",
       " -19,\n",
       " -10,\n",
       " -19,\n",
       " -19,\n",
       " -18,\n",
       " -19,\n",
       " -24,\n",
       " -15,\n",
       " -17,\n",
       " -18,\n",
       " -21,\n",
       " -25,\n",
       " -19,\n",
       " -13,\n",
       " -20,\n",
       " -21,\n",
       " -20,\n",
       " -15,\n",
       " -22,\n",
       " -18,\n",
       " -17,\n",
       " -18,\n",
       " -24,\n",
       " -24,\n",
       " -25,\n",
       " -23,\n",
       " -26,\n",
       " -23,\n",
       " -25,\n",
       " -27,\n",
       " -31,\n",
       " -28,\n",
       " -24,\n",
       " -24,\n",
       " -24,\n",
       " -25,\n",
       " -20,\n",
       " -13,\n",
       " -21,\n",
       " -22,\n",
       " -23,\n",
       " -21,\n",
       " -29,\n",
       " -22,\n",
       " -24,\n",
       " -25,\n",
       " -31,\n",
       " -36,\n",
       " -30,\n",
       " -29,\n",
       " -31,\n",
       " -32,\n",
       " -28,\n",
       " -24,\n",
       " -28,\n",
       " -24,\n",
       " -23,\n",
       " -24,\n",
       " -30,\n",
       " -33,\n",
       " -28,\n",
       " -24,\n",
       " -27,\n",
       " -25,\n",
       " -25,\n",
       " -23,\n",
       " -32,\n",
       " -24,\n",
       " -24,\n",
       " -26,\n",
       " -30,\n",
       " -34,\n",
       " -29,\n",
       " -24,\n",
       " -33,\n",
       " -33,\n",
       " -29,\n",
       " -30,\n",
       " -35,\n",
       " -33,\n",
       " -33,\n",
       " -34,\n",
       " -40,\n",
       " -37,\n",
       " -35,\n",
       " -31,\n",
       " -34,\n",
       " -34,\n",
       " -33,\n",
       " -34,\n",
       " -36,\n",
       " -34,\n",
       " -31,\n",
       " -32,\n",
       " -38,\n",
       " -36,\n",
       " -36,\n",
       " -31,\n",
       " -40,\n",
       " -37,\n",
       " -39,\n",
       " -39,\n",
       " -48,\n",
       " -39,\n",
       " -39,\n",
       " -37,\n",
       " -36,\n",
       " -39,\n",
       " -35,\n",
       " -28,\n",
       " -31,\n",
       " -30,\n",
       " -30,\n",
       " -31,\n",
       " -36,\n",
       " -34,\n",
       " -35,\n",
       " -33,\n",
       " -39,\n",
       " -41,\n",
       " -41,\n",
       " -37,\n",
       " -46,\n",
       " -42,\n",
       " -45,\n",
       " -39,\n",
       " -44,\n",
       " -38,\n",
       " -39,\n",
       " -42,\n",
       " -46,\n",
       " -50,\n",
       " -50,\n",
       " -49,\n",
       " -54,\n",
       " -48,\n",
       " -44,\n",
       " -42,\n",
       " -47,\n",
       " -44,\n",
       " -42,\n",
       " -42,\n",
       " -44,\n",
       " -46,\n",
       " -47,\n",
       " -45,\n",
       " -53,\n",
       " -47,\n",
       " -46,\n",
       " -41,\n",
       " -46,\n",
       " -39,\n",
       " -38,\n",
       " -37,\n",
       " -39,\n",
       " -39,\n",
       " -36,\n",
       " -26,\n",
       " -32,\n",
       " -31,\n",
       " -29,\n",
       " -25,\n",
       " -30,\n",
       " -25,\n",
       " -22,\n",
       " -22,\n",
       " -27,\n",
       " -27,\n",
       " -28,\n",
       " -23,\n",
       " -32,\n",
       " -31,\n",
       " -30,\n",
       " -30,\n",
       " -32,\n",
       " -24,\n",
       " -22,\n",
       " -20,\n",
       " -23,\n",
       " -28,\n",
       " -26,\n",
       " -25,\n",
       " -34,\n",
       " -36,\n",
       " -33,\n",
       " -33,\n",
       " -40,\n",
       " -34,\n",
       " -34,\n",
       " -33,\n",
       " -39,\n",
       " -36,\n",
       " -33,\n",
       " -28,\n",
       " -37,\n",
       " -38,\n",
       " -37,\n",
       " -37,\n",
       " -46,\n",
       " -43,\n",
       " -38,\n",
       " -38,\n",
       " -44,\n",
       " -49,\n",
       " -41,\n",
       " -34,\n",
       " -39,\n",
       " -37,\n",
       " -34,\n",
       " -31,\n",
       " -35,\n",
       " -34,\n",
       " -31,\n",
       " -29,\n",
       " -27,\n",
       " -28,\n",
       " -23,\n",
       " -20,\n",
       " -25,\n",
       " -21,\n",
       " -20,\n",
       " -17,\n",
       " -23,\n",
       " -22,\n",
       " -18,\n",
       " -17,\n",
       " -23,\n",
       " -26,\n",
       " -23,\n",
       " -22,\n",
       " -24,\n",
       " -20,\n",
       " -20,\n",
       " -18,\n",
       " -27,\n",
       " -21,\n",
       " -22,\n",
       " -26,\n",
       " -27,\n",
       " -31,\n",
       " -28,\n",
       " -19,\n",
       " -27,\n",
       " -23,\n",
       " -24,\n",
       " -26,\n",
       " -30,\n",
       " -26,\n",
       " -28,\n",
       " -26,\n",
       " -31,\n",
       " -31,\n",
       " -24,\n",
       " -21,\n",
       " -24,\n",
       " -24,\n",
       " -25,\n",
       " -27,\n",
       " -30,\n",
       " -29,\n",
       " -28,\n",
       " -30,\n",
       " -30,\n",
       " -33,\n",
       " -33,\n",
       " -31,\n",
       " -33,\n",
       " -31,\n",
       " -30,\n",
       " -25,\n",
       " -29,\n",
       " -21,\n",
       " -20,\n",
       " -21,\n",
       " -24,\n",
       " -27,\n",
       " -27,\n",
       " -23,\n",
       " -28,\n",
       " -29,\n",
       " -31,\n",
       " -25,\n",
       " -30,\n",
       " -29,\n",
       " -25,\n",
       " -25,\n",
       " -25,\n",
       " -28,\n",
       " -28,\n",
       " -24,\n",
       " -33,\n",
       " -28,\n",
       " -30,\n",
       " -25,\n",
       " -28,\n",
       " -22,\n",
       " -23,\n",
       " -24,\n",
       " -30,\n",
       " -29,\n",
       " -27,\n",
       " -18,\n",
       " -23,\n",
       " -23,\n",
       " -25,\n",
       " -21,\n",
       " -26,\n",
       " -19,\n",
       " -19,\n",
       " -20,\n",
       " -26,\n",
       " -31,\n",
       " -23,\n",
       " -19,\n",
       " -25,\n",
       " -26,\n",
       " -22,\n",
       " -18,\n",
       " -22,\n",
       " -19,\n",
       " -21,\n",
       " -20,\n",
       " -22,\n",
       " -27,\n",
       " -27,\n",
       " -21,\n",
       " -30,\n",
       " -31,\n",
       " -32,\n",
       " -34,\n",
       " -43,\n",
       " -33,\n",
       " -28,\n",
       " -27,\n",
       " -29,\n",
       " -29,\n",
       " -29,\n",
       " -27,\n",
       " -35,\n",
       " -31,\n",
       " -32,\n",
       " -26,\n",
       " -32,\n",
       " -25,\n",
       " -25,\n",
       " -22,\n",
       " -25,\n",
       " -27,\n",
       " -23,\n",
       " -20,\n",
       " -27,\n",
       " -28,\n",
       " -24,\n",
       " -20,\n",
       " -28,\n",
       " -20,\n",
       " -21,\n",
       " -20,\n",
       " -26,\n",
       " -25,\n",
       " -19,\n",
       " -14,\n",
       " -21,\n",
       " -20,\n",
       " -20,\n",
       " -19,\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewardsList[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e446bb2",
   "metadata": {},
   "source": [
    "# THE MODEL PERFORMED EXCEPTIONALLY BAD\n",
    "Need to check the reason. It only chooses one action always, even if the traffic is pretty much stuck for all lanes except one.\n",
    "Im thinking these can be the following reasons:\n",
    "1. Just the setting of the environment, specifically the car spawning rates, because I feel that after a point, it just becomes the same input for every step, for example when the 3 lanes are full. There could also be other reasons for this. I think I should check CoLight's paper and use their metrics, maybe thatll help.\n",
    "2. Need to check the epsilon decay, if its fast enough, because I have a hunch that right now, the way flows are, random actions can also work. Most probably because the change in flow rates isnt extreme enough, or is more or less similar.\n",
    "3. Just some straightforward problem/bug in code.\n",
    "___\n",
    "\n",
    "Checking for epsilon decay first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdb259f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "10000\n",
      "25000\n",
      "35000\n",
      "50000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "INUSE_epsilon = copy.deepcopy(PARAM_epsilon)\n",
    "epsilonVals = {}\n",
    "stepsDone = 0\n",
    "decayList = [1000, 10000, 25000, 35000, 50000, 100000]\n",
    "for d in decayList:\n",
    "    print(d)\n",
    "    epsilons = []\n",
    "    stepsDone = 0\n",
    "    for ep in range(PARAM_episodes):\n",
    "        for i in range(1080):\n",
    "            stepsDone+=1\n",
    "        # decaying epsilon\n",
    "        epsilon = PARAM_epsilon_min + (PARAM_epsilon - PARAM_epsilon_min) * math.exp(-1. * stepsDone/d)\n",
    "        # print('Epsilon: ', epsilon, '|| Episode: ', ep)\n",
    "        epsilons.append(epsilon)\n",
    "    epsilonVals[d] = epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0be596be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.969917938684749,\n",
       " 0.9407499484937146,\n",
       " 0.9124682545343663,\n",
       " 0.8850459258798506,\n",
       " 0.858456849924317,\n",
       " 0.8326757075174807,\n",
       " 0.8076779488547449,\n",
       " 0.7834397700999245,\n",
       " 0.7599380907183086,\n",
       " 0.73715053149848,\n",
       " 0.7150553932419601,\n",
       " 0.6936316361003912,\n",
       " 0.6728588595405757,\n",
       " 0.652717282918296,\n",
       " 0.6331877266424174,\n",
       " 0.6142515939113375,\n",
       " 0.5958908530043889,\n",
       " 0.5780880201113355,\n",
       " 0.5608261426836095,\n",
       " 0.5440887832914363,\n",
       " 0.5278600039714758,\n",
       " 0.5121243510500757,\n",
       " 0.49686684042768275,\n",
       " 0.48207294331040185,\n",
       " 0.46772857237511434,\n",
       " 0.4538200683549825,\n",
       " 0.4403341870325648,\n",
       " 0.42725808662815845,\n",
       " 0.4145793155713578,\n",
       " 0.4022858006441862,\n",
       " 0.39036583548450865,\n",
       " 0.3788080694387797,\n",
       " 0.3676014967535102,\n",
       " 0.3567354460951621,\n",
       " 0.34619957038848975,\n",
       " 0.3359838369636538,\n",
       " 0.3260785180027231,\n",
       " 0.31647418127646887,\n",
       " 0.3071616811626304,\n",
       " 0.29813214993709775,\n",
       " 0.2893769893297211,\n",
       " 0.280887862336704,\n",
       " 0.2726566852817847,\n",
       " 0.2646756201186461,\n",
       " 0.2569370669672232,\n",
       " 0.2494336568768027,\n",
       " 0.24215824480902215,\n",
       " 0.23510390283408672,\n",
       " 0.22826391353372577,\n",
       " 0.22163176360460637,\n",
       " 0.21520113765611298,\n",
       " 0.20896591219658725,\n",
       " 0.2029201498023018,\n",
       " 0.19705809346361464,\n",
       " 0.1913741611029213,\n",
       " 0.18586294025918362,\n",
       " 0.18051918293397443,\n",
       " 0.17533780059412962,\n",
       " 0.17031385932624948,\n",
       " 0.16544257513843455,\n",
       " 0.16071930940478313,\n",
       " 0.15613956444831142,\n",
       " 0.15169897925809103,\n",
       " 0.14739332533652497,\n",
       " 0.14321850267280822,\n",
       " 0.13917053583873817,\n",
       " 0.13524557020315758,\n",
       " 0.13143986826142529,\n",
       " 0.12774980607641906,\n",
       " 0.12417186982768189,\n",
       " 0.12070265246542615,\n",
       " 0.11733885046620811,\n",
       " 0.1140772606871848,\n",
       " 0.11091477731595728,\n",
       " 0.10784838891309509,\n",
       " 0.10487517554452717,\n",
       " 0.1019923060010669,\n",
       " 0.09919703510242503,\n",
       " 0.09648670108314246,\n",
       " 0.0938587230579537,\n",
       " 0.0913105985641678,\n",
       " 0.08883990117872632,\n",
       " 0.08644427820766899,\n",
       " 0.08412144844580714,\n",
       " 0.08186920000447181,\n",
       " 0.07968538820526717,\n",
       " 0.07756793353782485,\n",
       " 0.07551481967961307,\n",
       " 0.07352409157591636,\n",
       " 0.07159385357815642,\n",
       " 0.06972226763878199,\n",
       " 0.06790755156100857,\n",
       " 0.06614797730174159,\n",
       " 0.06444186932606652,\n",
       " 0.06278760301173962,\n",
       " 0.061183603102159555,\n",
       " 0.05962834220634677,\n",
       " 0.058120339344502746,\n",
       " 0.05665815853776334,\n",
       " 0.05524040744080404,\n",
       " 0.05386573601599474,\n",
       " 0.05253283524784144,\n",
       " 0.05124043589649089,\n",
       " 0.04998730728911118,\n",
       " 0.0487722561479972,\n",
       " 0.04759412545428541,\n",
       " 0.046451793346195466,\n",
       " 0.045344172050749885,\n",
       " 0.0442702068479545,\n",
       " 0.04322887506645298,\n",
       " 0.04221918510969958,\n",
       " 0.041240175511722404,\n",
       " 0.04029091402157813,\n",
       " 0.03937049671562649,\n",
       " 0.038478047136779166,\n",
       " 0.0376127154599032,\n",
       " 0.036773677682584625,\n",
       " 0.03596013484048132,\n",
       " 0.035171312246518135,\n",
       " 0.034406458753199864,\n",
       " 0.03366484603733936,\n",
       " 0.03294576790651994,\n",
       " 0.03224853962663161,\n",
       " 0.03157249726984058,\n",
       " 0.030916997082371445,\n",
       " 0.030281414871499907,\n",
       " 0.02966514541117212,\n",
       " 0.029067601865685043,\n",
       " 0.028488215230878647,\n",
       " 0.027926433792308088,\n",
       " 0.027381722599879803,\n",
       " 0.02685356295845126,\n",
       " 0.026341451933909257,\n",
       " 0.025844901874256636,\n",
       " 0.025363439945251066,\n",
       " 0.024896607680153872,\n",
       " 0.024443960543160302,\n",
       " 0.024005067506095244,\n",
       " 0.023579510637971417,\n",
       " 0.02316688470701933,\n",
       " 0.022766796794809845,\n",
       " 0.02237886592210194,\n",
       " 0.022002722686059573,\n",
       " 0.021638008908491903,\n",
       " 0.02128437729478211,\n",
       " 0.020941491103180027,\n",
       " 0.02060902382414353,\n",
       " 0.020286658869423484,\n",
       " 0.019974089270596136,\n",
       " 0.01967101738675587,\n",
       " 0.019377154621089968,\n",
       " 0.0190922211460655,\n",
       " 0.018815945636966747,\n",
       " 0.018548065013529222,\n",
       " 0.018288324189424436,\n",
       " 0.018036475829356812,\n",
       " 0.017792280113541416,\n",
       " 0.017555504509338217,\n",
       " 0.017325923549825522,\n",
       " 0.017103318619101592,\n",
       " 0.016887477744110098,\n",
       " 0.01667819539279116,\n",
       " 0.01647527227836574,\n",
       " 0.016278515169567007,\n",
       " 0.016087736706638066,\n",
       " 0.015902755222920704,\n",
       " 0.015723394571865328,\n",
       " 0.015549483959297422,\n",
       " 0.015380857780780669,\n",
       " 0.015217355463921991,\n",
       " 0.01505882131546829,\n",
       " 0.014905104373049283,\n",
       " 0.014756058261425271,\n",
       " 0.014611541053102944,\n",
       " 0.01447141513318654,\n",
       " 0.014335547068335574,\n",
       " 0.014203807479704433,\n",
       " 0.014076070919742837,\n",
       " 0.013952215752739793,\n",
       " 0.013832124038997348,\n",
       " 0.013715681422523846,\n",
       " 0.013602777022139705,\n",
       " 0.013493303325892042,\n",
       " 0.013387156088677642,\n",
       " 0.013284234232976708,\n",
       " 0.013184439752602918,\n",
       " 0.013087677619378145,\n",
       " 0.012993855692642933,\n",
       " 0.01290288463151657,\n",
       " 0.012814677809823255,\n",
       " 0.012729151233603275,\n",
       " 0.012646223461130705,\n",
       " 0.01256581552536142,\n",
       " 0.012487850858737637,\n",
       " 0.012412255220277288,\n",
       " 0.012338956624878888,\n",
       " 0.012267885274774525,\n",
       " 0.012198973493065715,\n",
       " 0.012132155659278832,\n",
       " 0.012067368146878744,\n",
       " 0.012004549262681167,\n",
       " 0.011943639188105998,\n",
       " 0.011884579922215767,\n",
       " 0.011827315226484872,\n",
       " 0.011771790571247083,\n",
       " 0.011717953083770277,\n",
       " 0.011665751497908963,\n",
       " 0.01161513610528667,\n",
       " 0.01156605870796171,\n",
       " 0.011518472572531219,\n",
       " 0.011472332385629794,\n",
       " 0.011427594210780354,\n",
       " 0.01138421544655612,\n",
       " 0.011342154786013879,\n",
       " 0.011301372177359912,\n",
       " 0.01126182878581112,\n",
       " 0.011223486956615042,\n",
       " 0.011186310179193523,\n",
       " 0.011150263052375941,\n",
       " 0.011115311250688828,\n",
       " 0.011081421491669827,\n",
       " 0.011048561504174836,\n",
       " 0.01101669999764817,\n",
       " 0.010985806632326485,\n",
       " 0.010955851990348075,\n",
       " 0.01092680754774004,\n",
       " 0.010898645647256651,\n",
       " 0.010871339472043058,\n",
       " 0.01084486302009922,\n",
       " 0.010819191079519815,\n",
       " 0.01079429920448646,\n",
       " 0.010770163691989472,\n",
       " 0.01074676155925694,\n",
       " 0.010724070521869626,\n",
       " 0.01070206897254089,\n",
       " 0.010680735960541386,\n",
       " 0.010660051171748959,\n",
       " 0.010639994909304762,\n",
       " 0.010620548074857131,\n",
       " 0.0106016921503754,\n",
       " 0.010583409180516312,\n",
       " 0.01056568175552624,\n",
       " 0.010548492994662947,\n",
       " 0.010531826530121092,\n",
       " 0.010515666491446163,\n",
       " 0.010499997490422018,\n",
       " 0.010484804606417627,\n",
       " 0.01047007337217907,\n",
       " 0.010455789760053255,\n",
       " 0.010441940168630238,\n",
       " 0.010428511409791444,\n",
       " 0.010415490696151412,\n",
       " 0.010402865628881167,\n",
       " 0.010390624185901559,\n",
       " 0.010378754710435388,\n",
       " 0.010367245899907349,\n",
       " 0.010356086795181301,\n",
       " 0.01034526677012454,\n",
       " 0.010334775521489181,\n",
       " 0.010324603059101016,\n",
       " 0.010314739696346475,\n",
       " 0.010305176040948659,\n",
       " 0.010295902986023646,\n",
       " 0.010286911701408566,\n",
       " 0.010278193625253177,\n",
       " 0.010269740455866936,\n",
       " 0.010261544143813812,\n",
       " 0.010253596884247295,\n",
       " 0.01024589110947832,\n",
       " 0.010238419481769026,\n",
       " 0.010231174886345465,\n",
       " 0.010224150424622646,\n",
       " 0.010217339407635436,\n",
       " 0.010210735349669063,\n",
       " 0.010204331962083169,\n",
       " 0.01019812314732352,\n",
       " 0.010192102993115685,\n",
       " 0.01018626576683513,\n",
       " 0.010180605910048397,\n",
       " 0.01017511803322014,\n",
       " 0.010169796910581015,\n",
       " 0.010164637475151481,\n",
       " 0.010159634813916841,\n",
       " 0.010154784163148866,\n",
       " 0.0101500809038696,\n",
       " 0.01014552055745298,\n",
       " 0.01014109878136012,\n",
       " 0.010136811365004179,\n",
       " 0.010132654225740867,\n",
       " 0.010128623404980802,\n",
       " 0.010124715064419986,\n",
       " 0.010120925482384815,\n",
       " 0.010117251050288174,\n",
       " 0.010113688269193177,\n",
       " 0.010110233746481364,\n",
       " 0.01010688419262211,\n",
       " 0.010103636418040202,\n",
       " 0.010100487330078607,\n",
       " 0.010097433930053527,\n",
       " 0.010094473310398923,\n",
       " 0.010091602651897838,\n",
       " 0.010088819220997807,\n",
       " 0.01008612036720788,\n",
       " 0.010083503520574707,\n",
       " 0.010080966189235346,\n",
       " 0.010078505957044397,\n",
       " 0.010076120481273263,\n",
       " 0.010073807490379316,\n",
       " 0.010071564781842835,\n",
       " 0.010069390220069696,\n",
       " 0.010067281734357761,\n",
       " 0.010065237316925088,\n",
       " 0.010063255020998033,\n",
       " 0.010061332958957468,\n",
       " 0.010059469300541302,\n",
       " 0.010057662271101647,\n",
       " 0.010055910149914924,\n",
       " 0.010054211268543322,\n",
       " 0.010052564009246052,\n",
       " 0.010050966803438864,\n",
       " 0.01004941813020039,\n",
       " 0.01004791651482385,\n",
       " 0.010046460527412796,\n",
       " 0.010045048781519492,\n",
       " 0.010043679932824697,\n",
       " 0.010042352677857547,\n",
       " 0.01004106575275434,\n",
       " 0.010039817932055033,\n",
       " 0.01003860802753632,\n",
       " 0.01003743488708015,\n",
       " 0.010036297393576641,\n",
       " 0.01003519446386032,\n",
       " 0.010034125047678701,\n",
       " 0.01003308812669218,\n",
       " 0.01003208271350434,\n",
       " 0.010031107850721716,\n",
       " 0.010030162610042125,\n",
       " 0.010029246091370694,\n",
       " 0.010028357421962772,\n",
       " 0.010027495755592846,\n",
       " 0.010026660271748752,\n",
       " 0.01002585017485034,\n",
       " 0.010025064693491898,\n",
       " 0.010024303079707583,\n",
       " 0.010023564608259187,\n",
       " 0.010022848575945527,\n",
       " 0.010022154300932841,\n",
       " 0.010021481122105508,\n",
       " 0.01002082839843652,\n",
       " 0.01002019550837706,\n",
       " 0.010019581849264644,\n",
       " 0.010018986836749245,\n",
       " 0.010018409904236848,\n",
       " 0.010017850502349919,\n",
       " 0.010017308098404263,\n",
       " 0.010016782175901792,\n",
       " 0.010016272234038679,\n",
       " 0.010015777787228489,\n",
       " 0.010015298364639776,\n",
       " 0.01001483350974774,\n",
       " 0.010014382779899505,\n",
       " 0.010013945745892615,\n",
       " 0.010013521991566323,\n",
       " 0.010013111113405311,\n",
       " 0.010012712720155442,\n",
       " 0.0100123264324512,\n",
       " 0.010011951882454437,\n",
       " 0.010011588713504107,\n",
       " 0.010011236579776638,\n",
       " 0.010010895145956622,\n",
       " 0.010010564086917527,\n",
       " 0.01001024308741208,\n",
       " 0.010009931841772091,\n",
       " 0.010009630053617383,\n",
       " 0.010009337435573556,\n",
       " 0.010009053708998353,\n",
       " 0.010008778603716314,\n",
       " 0.01000851185776151,\n",
       " 0.010008253217128088,\n",
       " 0.010008002435528395,\n",
       " 0.01000775927415846,\n",
       " 0.010007523501470585,\n",
       " 0.010007294892952864,\n",
       " 0.010007073230915393,\n",
       " 0.010006858304282974,\n",
       " 0.010006649908394127,\n",
       " 0.010006447844806195,\n",
       " 0.010006251921106386,\n",
       " 0.010006061950728546,\n",
       " 0.01000587775277551,\n",
       " 0.010005699151846835,\n",
       " 0.010005525977871783,\n",
       " 0.010005358065947373,\n",
       " 0.010005195256181354,\n",
       " 0.010005037393539943,\n",
       " 0.010004884327700208,\n",
       " 0.010004735912906914,\n",
       " 0.010004592007833732,\n",
       " 0.010004452475448667,\n",
       " 0.010004317182883565,\n",
       " 0.010004186001307593,\n",
       " 0.010004058805804562,\n",
       " 0.010003935475253975,\n",
       " 0.010003815892215698,\n",
       " 0.010003699942818118,\n",
       " 0.010003587516649716,\n",
       " 0.010003478506653932,\n",
       " 0.010003372809027215,\n",
       " 0.010003270323120185,\n",
       " 0.010003170951341778,\n",
       " 0.010003074599066332,\n",
       " 0.010002981174543471,\n",
       " 0.010002890588810735,\n",
       " 0.010002802755608876,\n",
       " 0.010002717591299705,\n",
       " 0.010002635014786465,\n",
       " 0.010002554947436593,\n",
       " 0.010002477313006852,\n",
       " 0.010002402037570721,\n",
       " 0.01000232904944801,\n",
       " 0.010002258279136594,\n",
       " 0.010002189659246237,\n",
       " 0.010002123124434414,\n",
       " 0.010002058611344095,\n",
       " 0.01000199605854341,\n",
       " 0.010001935406467157,\n",
       " 0.010001876597360071,\n",
       " 0.01000181957522184,\n",
       " 0.010001764285753766,\n",
       " 0.010001710676307077,\n",
       " 0.010001658695832775,\n",
       " 0.010001608294833032,\n",
       " 0.010001559425314062,\n",
       " 0.010001512040740412,\n",
       " 0.01000146609599065,\n",
       " 0.010001421547314402,\n",
       " 0.010001378352290691,\n",
       " 0.010001336469787537,\n",
       " 0.010001295859922795,\n",
       " 0.010001256484026175,\n",
       " 0.01000121830460242,\n",
       " 0.010001181285295601,\n",
       " 0.010001145390854498,\n",
       " 0.010001110587099028,\n",
       " 0.010001076840887706,\n",
       " 0.010001044120086079,\n",
       " 0.010001012393536129,\n",
       " 0.010000981631026605,\n",
       " 0.01000095180326425,\n",
       " 0.010000922881845911,\n",
       " 0.010000894839231491,\n",
       " 0.010000867648717725,\n",
       " 0.010000841284412748,\n",
       " 0.010000815721211448,\n",
       " 0.01000079093477155,\n",
       " 0.010000766901490444,\n",
       " 0.010000743598482709,\n",
       " 0.010000721003558314,\n",
       " 0.010000699095201496,\n",
       " 0.010000677852550268,\n",
       " 0.010000657255376551,\n",
       " 0.010000637284066917,\n",
       " 0.01000061791960391,\n",
       " 0.010000599143547938,\n",
       " 0.01000058093801971,\n",
       " 0.010000563285683215,\n",
       " 0.010000546169729216,\n",
       " 0.010000529573859234,\n",
       " 0.010000513482270037,\n",
       " 0.01000049787963859,\n",
       " 0.010000482751107464,\n",
       " 0.010000468082270682,\n",
       " 0.010000453859160006,\n",
       " 0.010000440068231644,\n",
       " 0.010000426696353334,\n",
       " 0.010000413730791856,\n",
       " 0.010000401159200896,\n",
       " 0.010000388969609302,\n",
       " 0.010000377150409669,\n",
       " 0.010000365690347297,\n",
       " 0.010000354578509469,\n",
       " 0.01000034380431506,\n",
       " 0.010000333357504469,\n",
       " 0.010000323228129833,\n",
       " 0.01000031340654557,\n",
       " 0.010000303883399186,\n",
       " 0.010000294649622371,\n",
       " 0.010000285696422365,\n",
       " 0.010000277015273581,\n",
       " 0.010000268597909496,\n",
       " 0.010000260436314766,\n",
       " 0.010000252522717605,\n",
       " 0.010000244849582379,\n",
       " 0.010000237409602429,\n",
       " 0.010000230195693119,\n",
       " 0.010000223200985084,\n",
       " 0.01000021641881769,\n",
       " 0.010000209842732697,\n",
       " 0.0100002034664681,\n",
       " 0.010000197283952171]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilonVals[35000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5104fae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k,v in epsilonVals.items():\n",
    "    if k==1000:\n",
    "        continue\n",
    "    plt.plot(v, label=f'EPSILON DECAY: {k}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bd68dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.01, 35000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAM_epsilon_decay = 35000\n",
    "PARAM_epsilon, PARAM_epsilon_min, PARAM_epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e5907",
   "metadata": {},
   "source": [
    "# Running the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "781d9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityFlowEnv(10800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122804ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "policyNet = torch.load(os.path.join('models', 'PolicyNetv2.pth'), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea8c5915",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetNet = torch.load(os.path.join('models', 'TargetNetv2.pth'), weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7508096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env._getState()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a119d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1\n",
      "QVals:  tensor([-2288.4387, -2281.2598, -2284.5039, -2288.8259], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  [[1]]\n",
      "State:  [3. 3. 3. 3.]\n",
      "------------------------------\n",
      "Step:  2\n",
      "QVals:  tensor([-2288.7825, -2281.6006, -2284.8423, -2289.1736], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  [[1]]\n",
      "State:  [4. 3. 3. 3.]\n",
      "------------------------------\n",
      "Step:  3\n",
      "QVals:  tensor([-2289.6953, -2282.5120, -2285.7534, -2290.0994], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  [[1]]\n",
      "State:  [6. 3. 4. 3.]\n",
      "------------------------------\n",
      "Step:  4\n",
      "QVals:  tensor([-2289.8962, -2282.7192, -2285.9634, -2290.2983], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "best Action:  [[1]]\n",
      "State:  [7. 4. 4. 3.]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1160/2679371309.py:76: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.engine.set_tl_phase(intersection_id, action)\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = torch.tensor(env._getState(), device=PARAM_device, dtype=torch.float32) # starting state\n",
    "rewards = []\n",
    "step = 1\n",
    "while not done:\n",
    "    print(\"Step: \", step)\n",
    "    # get the prediction from the model\n",
    "    QVals = policyNet(state)\n",
    "    print(\"QVals: \", QVals)\n",
    "    # get the best action from QVals\n",
    "    bestAction = QVals.argmax(dim=0).view(1,1).cpu().numpy()\n",
    "    print(\"best Action: \", bestAction)\n",
    "    # perform the action\n",
    "    nextState, r, done = env.take_action(bestAction)\n",
    "    print('State: ', state.cpu().numpy())\n",
    "    print('------------------------------')\n",
    "\n",
    "    # update the current state\n",
    "    state = torch.tensor(nextState, device=PARAM_device, dtype=torch.float32)\n",
    "    rewards.append(r)\n",
    "    step+=1\n",
    "    if step==5:\n",
    "        break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85426640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3.], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = torch.tensor([3,3,3,3], dtype=torch.float32, device=PARAM_device)\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f03ae2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2306.7148, -2293.3281, -2304.5603, -2307.8433], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetNet(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03e1b8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3.], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22f5a8",
   "metadata": {},
   "source": [
    "# Even with the epsilon decay _bug_ fixed, it didnt make any difference. I have two ways of going on from here.\n",
    "1. Change the environment, like the flow, and train in it, to see if anything changes, maybe use GPLight's flow data.\n",
    "2. Go forward directly to the nxn grid. As for a single intersection, this might not makes sense.\n",
    "___\n",
    "Im going forward with number 1 for now. And then see what happens. Ill maybe use GPLight's flow, and even reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f641a98e",
   "metadata": {},
   "source": [
    "#### Scratch that, going with number 2\n",
    "\n",
    "Testing the env without any RL control to see if its working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5c57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = cityflow.Engine(\"generated/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cde8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3600):\n",
    "    env.next_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "450a7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    env.next_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc8e168e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'road_0_1_0_0': 32,\n",
       "  'road_0_1_0_1': 31,\n",
       "  'road_0_1_0_2': 1,\n",
       "  'road_1_0_1_0': 1,\n",
       "  'road_1_0_1_1': 1,\n",
       "  'road_1_0_1_2': 1,\n",
       "  'road_1_1_0_0': 2,\n",
       "  'road_1_1_0_1': 1,\n",
       "  'road_1_1_0_2': 1,\n",
       "  'road_1_1_1_0': 1,\n",
       "  'road_1_1_1_1': 1,\n",
       "  'road_1_1_1_2': 1,\n",
       "  'road_1_1_2_0': 1,\n",
       "  'road_1_1_2_1': 1,\n",
       "  'road_1_1_2_2': 1,\n",
       "  'road_1_1_3_0': 1,\n",
       "  'road_1_1_3_1': 1,\n",
       "  'road_1_1_3_2': 1,\n",
       "  'road_1_2_3_0': 1,\n",
       "  'road_1_2_3_1': 1,\n",
       "  'road_1_2_3_2': 1,\n",
       "  'road_2_1_2_0': 33,\n",
       "  'road_2_1_2_1': 31,\n",
       "  'road_2_1_2_2': 1},\n",
       " {'road_0_1_0_0': 28,\n",
       "  'road_0_1_0_1': 27,\n",
       "  'road_0_1_0_2': 0,\n",
       "  'road_1_0_1_0': 0,\n",
       "  'road_1_0_1_1': 0,\n",
       "  'road_1_0_1_2': 0,\n",
       "  'road_1_1_0_0': 0,\n",
       "  'road_1_1_0_1': 0,\n",
       "  'road_1_1_0_2': 0,\n",
       "  'road_1_1_1_0': 0,\n",
       "  'road_1_1_1_1': 0,\n",
       "  'road_1_1_1_2': 0,\n",
       "  'road_1_1_2_0': 0,\n",
       "  'road_1_1_2_1': 0,\n",
       "  'road_1_1_2_2': 0,\n",
       "  'road_1_1_3_0': 0,\n",
       "  'road_1_1_3_1': 0,\n",
       "  'road_1_1_3_2': 0,\n",
       "  'road_1_2_3_0': 0,\n",
       "  'road_1_2_3_1': 0,\n",
       "  'road_1_2_3_2': 0,\n",
       "  'road_2_1_2_0': 29,\n",
       "  'road_2_1_2_1': 26,\n",
       "  'road_2_1_2_2': 0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "env.get_lane_vehicle_count(), env.get_lane_waiting_vehicle_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c9aee",
   "metadata": {},
   "source": [
    "The flow file seems to work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d446c",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "It does not seem that any kind of normalization is being used here. And the current state is, current traffic light and number of cars in each lane. Wonder what is it that Im doing wrong here. Ill start off by doing the following:\n",
    "1. Change the state definition to include the phase.\n",
    "2. Change the reward function.\n",
    "3. Finally Ill train two different models, one with normalization, one without, and see what works.\n",
    "\n",
    "__I HOPE IT WORKS BECAUSE MY IMPOSTOR SYNDROME IS KICKING IN BIG TIME__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece59101",
   "metadata": {},
   "source": [
    "Creating the new reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ed23ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get the P_i for each of the 12 directions\n",
    "directions = [('road_0_1_0_0', 'road_1_1_1_0'), # left\n",
    "              ('road_1_0_1_0', 'road_1_1_2_0'), # left\n",
    "              ('road_2_1_2_0', 'road_1_1_3_0'), # left\n",
    "              ('road_1_2_3_0', 'road_1_1_0_0'), # left\n",
    "              ('road_0_1_0_1', 'road_1_1_0_1'), # straight\n",
    "              ('road_1_0_1_1', 'road_1_1_1_1'), # straight\n",
    "              ('road_2_1_2_1', 'road_1_1_2_1'), # straight\n",
    "              ('road_1_2_3_1', 'road_1_1_3_1'), # straight\n",
    "              ('road_0_1_0_2', 'road_1_1_3_2'), # right\n",
    "              ('road_1_0_1_2', 'road_1_1_0_2'), # right\n",
    "              ('road_2_1_2_2', 'road_1_1_1_2'), # right\n",
    "              ('road_1_2_3_2', 'road_1_1_2_2') # right\n",
    "              ]\n",
    "\n",
    "capacity = 40 # capacity for each lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a0778fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 0\n",
      "1 0\n",
      "30 0\n",
      "1 0\n",
      "19 0\n",
      "1 0\n",
      "27 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "r = 0\n",
    "for d in directions:\n",
    "    # calculaing r_i for each direction\n",
    "    nIn = env.get_lane_waiting_vehicle_count()[d[0]] # waiting vics in incoming lane\n",
    "    nOut = env.get_lane_waiting_vehicle_count()[d[1]] # waiting vics in outgoing lane\n",
    "    print(nIn, nOut)\n",
    "    r_i = -1 * nIn * (1 - (nOut/capacity))\n",
    "    r += r_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b319aae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a73e2",
   "metadata": {},
   "source": [
    "### New env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65e430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PressureEnv:\n",
    "    '''\n",
    "        This class is the environment implemented in cityflow for a single intersection.\n",
    "    '''\n",
    "    def __init__(self, maxSteps, configPath=os.path.join('generated', 'config.json'), numThreads=1):\n",
    "        # initializing the cityflow engine\n",
    "        self.engine = cityflow.Engine(configPath, thread_num=numThreads)\n",
    "        self.numSteps = 0 # to track how many steps have been taken\n",
    "        self.maxSteps = maxSteps # the maximum number of steps allowed\n",
    "        self.directions = [('road_0_1_0_0', 'road_1_1_1_0'), # left\n",
    "              ('road_1_0_1_0', 'road_1_1_2_0'), # left\n",
    "              ('road_2_1_2_0', 'road_1_1_3_0'), # left\n",
    "              ('road_1_2_3_0', 'road_1_1_0_0'), # left\n",
    "              ('road_0_1_0_1', 'road_1_1_0_1'), # straight\n",
    "              ('road_1_0_1_1', 'road_1_1_1_1'), # straight\n",
    "              ('road_2_1_2_1', 'road_1_1_2_1'), # straight\n",
    "              ('road_1_2_3_1', 'road_1_1_3_1'), # straight\n",
    "              ('road_0_1_0_2', 'road_1_1_3_2'), # right\n",
    "              ('road_1_0_1_2', 'road_1_1_0_2'), # right\n",
    "              ('road_2_1_2_2', 'road_1_1_1_2'), # right\n",
    "              ('road_1_2_3_2', 'road_1_1_2_2') # right\n",
    "              ]\n",
    "        self.incoming = [t[0] for t in self.directions]\n",
    "        self.capacity = 40 # capacity of the lanes\n",
    "    \n",
    "    def _getState(self, currTLPhase):\n",
    "        '''\n",
    "            This function returns the state the environment is in right now\n",
    "        '''\n",
    "        # get lanecounts\n",
    "        laneCounts = self.engine.get_lane_vehicle_count()\n",
    "        # add to a dictionary and return\n",
    "        stArray = []\n",
    "        cumLaneLenghts = {'road_0_1_0':0, 'road_2_1_2':0, 'road_1_2_3':0, 'road_1_0_1':0}\n",
    "        for k,v in laneCounts.items():\n",
    "            if k in self.incoming:\n",
    "                stArray.append(v)\n",
    "        # appending the current phase\n",
    "        stArray.append(currTLPhase)\n",
    "        \n",
    "        return stArray\n",
    "    \n",
    "    def _getReward(self):\n",
    "        '''\n",
    "            This function returns the reward after taking the current state\n",
    "        '''\n",
    "        # NOTE: reward will be generated after the action is done, so we need to implement the do_action and simulate traffic for the next 10 seconds\n",
    "        # after that, calculate the reward\n",
    "        # get the lanelengths\n",
    "        r = 0\n",
    "        vicCounts = self.engine.get_lane_waiting_vehicle_count()\n",
    "        for d in self.directions:\n",
    "            # calculate the number of incoming and outgoing vehicles\n",
    "            nIn = vicCounts[d[0]]\n",
    "            nOut = vicCounts[d[1]]\n",
    "            r_i = -1 * nIn * (1 - (nOut/self.capacity))\n",
    "            r += r_i\n",
    "        return r\n",
    "    \n",
    "    def _peformAction(self):\n",
    "        '''\n",
    "            This function will take action, which is setting the traffic light to a specific phase.\n",
    "        '''\n",
    "        pass\n",
    "        # set trafficlight phase\n",
    "        # simulate for the next 10 seconds\n",
    "        self._step(10)\n",
    "\n",
    "    def _step(self, t=10):\n",
    "        '''\n",
    "            This function steps the environment for the next t seconds.\n",
    "        '''\n",
    "        # NOTE TO SELF: rn, the interval is hardcoded to 1 second, same as the config definition, REMEMBER to make this dynamic\n",
    "        finished = False\n",
    "        for i in range(t):\n",
    "            self.numSteps+=1\n",
    "            if self.numSteps==self.maxSteps:\n",
    "                finished = True\n",
    "                break\n",
    "            self.engine.next_step()\n",
    "        return finished\n",
    "\n",
    "    def take_action(self, action, t=10, intersection_id='intersection_1_1'):\n",
    "        '''\n",
    "            This is the main callable function for taking a step in the environment. It does the following:\n",
    "                1. takes the action.\n",
    "                2. simulates for the next t seconds.\n",
    "                3. gets the reward\n",
    "                4. get next state\n",
    "            Action will be the index of the tl phase for the intersection defined as defined in the roadnet file for that intersection\n",
    "        '''\n",
    "        # take action, set the tl phase to the provided index\n",
    "        self.engine.set_tl_phase(intersection_id, action)\n",
    "        # run the engine\n",
    "        finished = self._step(t)\n",
    "        # get the state\n",
    "        next_state = self._getState(action)\n",
    "        # get the reward\n",
    "        r = self._getReward()\n",
    "\n",
    "        return next_state, r, finished\n",
    "    \n",
    "    def reset(self,currTLPhase):\n",
    "        '''\n",
    "            This function resets the environment to the original state.\n",
    "        '''\n",
    "        self.engine.reset()\n",
    "        self.numSteps = 0\n",
    "        # clearing the replay and the roadnetlog files\n",
    "        open(os.path.join('generated', 'GeneratedRoadNetLogExpt.json'), 'w').close()\n",
    "        open(os.path.join('generated', 'GeneratedReplayLogExpt.txt'), 'w').close()\n",
    "        return self._getState(currTLPhase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b139d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PressureEnv(maxSteps=3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d874c9f",
   "metadata": {},
   "source": [
    "#### Before starting training, time to check which one would be a good decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8637d295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAM_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2bbfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9efeb43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "10000\n",
      "25000\n",
      "35000\n",
      "50000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "INUSE_epsilon = copy.deepcopy(PARAM_epsilon)\n",
    "epsilonVals = {}\n",
    "stepsDone = 0\n",
    "decayList = [1000, 10000, 25000, 35000, 50000, 100000]\n",
    "for d in decayList:\n",
    "    print(d)\n",
    "    epsilons = []\n",
    "    stepsDone = 0\n",
    "    for ep in range(500):\n",
    "        for i in range(360):\n",
    "            stepsDone+=1\n",
    "        # decaying epsilon\n",
    "        epsilon = PARAM_epsilon_min + (PARAM_epsilon - PARAM_epsilon_min) * math.exp(-1. * stepsDone/d)\n",
    "        # print('Epsilon: ', epsilon, '|| Episode: ', ep)\n",
    "        epsilons.append(epsilon)\n",
    "    epsilonVals[d] = epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4556195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k,v in epsilonVals.items():\n",
    "    if k==1000:\n",
    "        continue\n",
    "    plt.plot(v, label=f'EPSILON DECAY: {k}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a71800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50k looks okay\n",
    "PARAM_epsilon_decay = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fbf2f",
   "metadata": {},
   "source": [
    "There was another bug that I noticed, the wrong epsilon variable was being used or updated. I need to change that too. Im going to now train with the new environment. Please make it work GOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12606436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the epsilon value to use\n",
    "INUSE_epsilon = copy.deepcopy(PARAM_epsilon)\n",
    "# initializing the memory\n",
    "memory = ReplayMemory(PARAM_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3567afa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declaring the networks\n",
    "policyNet = DQN(input_dim, output_dim)\n",
    "targetNet = DQN(input_dim, output_dim)\n",
    "# loading the state dict from policy to target so that they have the same starting weights\n",
    "targetNet.load_state_dict(policyNet.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bf890f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.Adam(policyNet.parameters(), lr=PARAM_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3094fa5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=13, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving the models to gpu\n",
    "policyNet.to(PARAM_device)\n",
    "targetNet.to(PARAM_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da492bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  0\n",
      "Epsilon Used for the episode:  1.0\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -28933.0 || average_reward: -80.36944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  1\n",
      "Epsilon Used for the episode:  0.9928975993247753\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -30206.0 || average_reward: -83.90555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  2\n",
      "Epsilon Used for the episode:  0.9858461522812185\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -29424.0 || average_reward: -81.73333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  3\n",
      "Epsilon Used for the episode:  0.9788452933207361\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -28279.0 || average_reward: -78.55277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  4\n",
      "Epsilon Used for the episode:  0.9718946595172315\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -29181.0 || average_reward: -81.05833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  5\n",
      "Epsilon Used for the episode:  0.9649938905482919\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -27953.0 || average_reward: -77.64722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  6\n",
      "Epsilon Used for the episode:  0.9581426286765082\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -28551.0 || average_reward: -79.30833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  7\n",
      "Epsilon Used for the episode:  0.9513405187309311\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -28957.0 || average_reward: -80.43611111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  8\n",
      "Epsilon Used for the episode:  0.9445872080886573\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -28639.0 || average_reward: -79.55277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  9\n",
      "Epsilon Used for the episode:  0.9378823466565509\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -29813.0 || average_reward: -82.81388888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  10\n",
      "Epsilon Used for the episode:  0.9312255868530936\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -28731.0 || average_reward: -79.80833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  11\n",
      "Epsilon Used for the episode:  0.9246165835903666\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -28423.0 || average_reward: -78.95277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  12\n",
      "Epsilon Used for the episode:  0.9180549942561606\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -28694.0 || average_reward: -79.70555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  13\n",
      "Epsilon Used for the episode:  0.9115404786962149\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -28135.0 || average_reward: -78.15277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  14\n",
      "Epsilon Used for the episode:  0.9050726991965842\n",
      "EPISODE COMPLETE\n",
      "min_reward: -133.0 || max_reward: 0.0 || total_reward: -27717.0 || average_reward: -76.99166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  15\n",
      "Epsilon Used for the episode:  0.8986513204661305\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -27459.0 || average_reward: -76.275\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  16\n",
      "Epsilon Used for the episode:  0.8922760096191426\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -27580.0 || average_reward: -76.61111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  17\n",
      "Epsilon Used for the episode:  0.8859464361580783\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -28035.0 || average_reward: -77.875\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  18\n",
      "Epsilon Used for the episode:  0.879662271956432\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -26993.0 || average_reward: -74.98055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  19\n",
      "Epsilon Used for the episode:  0.8734231912417241\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -27223.0 || average_reward: -75.61944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  20\n",
      "Epsilon Used for the episode:  0.867228870578613\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -28983.0 || average_reward: -80.50833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  21\n",
      "Epsilon Used for the episode:  0.8610789888521284\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -27873.0 || average_reward: -77.425\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  22\n",
      "Epsilon Used for the episode:  0.8549732272510244\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -27265.0 || average_reward: -75.73611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  23\n",
      "Epsilon Used for the episode:  0.8489112692512522\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -28070.0 || average_reward: -77.97222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  24\n",
      "Epsilon Used for the episode:  0.8428928005995513\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -28135.0 || average_reward: -78.15277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  25\n",
      "Epsilon Used for the episode:  0.8369175092971592\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -27074.0 || average_reward: -75.20555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  26\n",
      "Epsilon Used for the episode:  0.8309850855836367\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -28600.0 || average_reward: -79.44444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  27\n",
      "Epsilon Used for the episode:  0.8250952219208098\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -26963.0 || average_reward: -74.89722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  28\n",
      "Epsilon Used for the episode:  0.8192476129768271\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -28100.0 || average_reward: -78.05555555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  29\n",
      "Epsilon Used for the episode:  0.8134419556103315\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -28164.0 || average_reward: -78.23333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  30\n",
      "Epsilon Used for the episode:  0.8076779488547449\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -27672.0 || average_reward: -76.86666666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  31\n",
      "Epsilon Used for the episode:  0.8019552939026663\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -28393.0 || average_reward: -78.86944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  32\n",
      "Epsilon Used for the episode:  0.7962736940903813\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -28965.0 || average_reward: -80.45833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  33\n",
      "Epsilon Used for the episode:  0.7906328548824832\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -26896.0 || average_reward: -74.71111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  34\n",
      "Epsilon Used for the episode:  0.7850324838566044\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -27720.0 || average_reward: -77.0\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  35\n",
      "Epsilon Used for the episode:  0.7794722906882566\n",
      "EPISODE COMPLETE\n",
      "min_reward: -132.0 || max_reward: 0.0 || total_reward: -27423.0 || average_reward: -76.175\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  36\n",
      "Epsilon Used for the episode:  0.773951987135781\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -29970.0 || average_reward: -83.25\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  37\n",
      "Epsilon Used for the episode:  0.7684712870254046\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -27845.0 || average_reward: -77.34722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  38\n",
      "Epsilon Used for the episode:  0.7630299062364069\n",
      "EPISODE COMPLETE\n",
      "min_reward: -133.0 || max_reward: 0.0 || total_reward: -27446.0 || average_reward: -76.2388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  39\n",
      "Epsilon Used for the episode:  0.7576275626863888\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -28991.0 || average_reward: -80.53055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  40\n",
      "Epsilon Used for the episode:  0.7522639763166509\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -29802.0 || average_reward: -82.78333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  41\n",
      "Epsilon Used for the episode:  0.7469388690776747\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -28112.0 || average_reward: -78.08888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  42\n",
      "Epsilon Used for the episode:  0.7416519649147084\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -26903.0 || average_reward: -74.73055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  43\n",
      "Epsilon Used for the episode:  0.7364029897534561\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -29345.0 || average_reward: -81.51388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  44\n",
      "Epsilon Used for the episode:  0.7311916714858699\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -27918.0 || average_reward: -77.55\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  45\n",
      "Epsilon Used for the episode:  0.7260177399560439\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -27403.0 || average_reward: -76.11944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  46\n",
      "Epsilon Used for the episode:  0.720880926946209\n",
      "EPISODE COMPLETE\n",
      "min_reward: -132.0 || max_reward: 0.0 || total_reward: -28050.0 || average_reward: -77.91666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  47\n",
      "Epsilon Used for the episode:  0.7157809661628279\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -29878.0 || average_reward: -82.99444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  48\n",
      "Epsilon Used for the episode:  0.7107175932227919\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -27056.0 || average_reward: -75.15555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  49\n",
      "Epsilon Used for the episode:  0.7056905456397137\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -28426.0 || average_reward: -78.96111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  50\n",
      "Epsilon Used for the episode:  0.7006995628103208\n",
      "EPISODE COMPLETE\n",
      "min_reward: -173.0 || max_reward: 0.0 || total_reward: -29944.0 || average_reward: -83.17777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  51\n",
      "Epsilon Used for the episode:  0.6957443860009455\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -28738.0 || average_reward: -79.82777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  52\n",
      "Epsilon Used for the episode:  0.6908247583341125\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -30347.0 || average_reward: -84.29722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  53\n",
      "Epsilon Used for the episode:  0.6859404247752217\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -28246.0 || average_reward: -78.46111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  54\n",
      "Epsilon Used for the episode:  0.6810911321193275\n",
      "EPISODE COMPLETE\n",
      "min_reward: -194.0 || max_reward: 0.0 || total_reward: -32933.0 || average_reward: -91.48055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  55\n",
      "Epsilon Used for the episode:  0.6762766289780126\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -28976.0 || average_reward: -80.4888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  56\n",
      "Epsilon Used for the episode:  0.671496665766356\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -28087.0 || average_reward: -78.01944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  57\n",
      "Epsilon Used for the episode:  0.6667509946899945\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -28656.0 || average_reward: -79.6\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  58\n",
      "Epsilon Used for the episode:  0.6620393697322765\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -27782.0 || average_reward: -77.17222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  59\n",
      "Epsilon Used for the episode:  0.6573615466415093\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -29815.0 || average_reward: -82.81944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  60\n",
      "Epsilon Used for the episode:  0.652717282918296\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -36068.0 || average_reward: -100.18888888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  61\n",
      "Epsilon Used for the episode:  0.648106337802965\n",
      "EPISODE COMPLETE\n",
      "min_reward: -173.0 || max_reward: 0.0 || total_reward: -34193.0 || average_reward: -94.98055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  62\n",
      "Epsilon Used for the episode:  0.6435284722630893\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -35586.0 || average_reward: -98.85\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  63\n",
      "Epsilon Used for the episode:  0.6389834489810938\n",
      "EPISODE COMPLETE\n",
      "min_reward: -200.0 || max_reward: 0.0 || total_reward: -37513.0 || average_reward: -104.20277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  64\n",
      "Epsilon Used for the episode:  0.634471032341954\n",
      "EPISODE COMPLETE\n",
      "min_reward: -181.0 || max_reward: 0.0 || total_reward: -30602.0 || average_reward: -85.00555555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  65\n",
      "Epsilon Used for the episode:  0.6299909884209803\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -31787.0 || average_reward: -88.29722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  66\n",
      "Epsilon Used for the episode:  0.625543084971693\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -32360.0 || average_reward: -89.88888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  67\n",
      "Epsilon Used for the episode:  0.621127091413781\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -32238.0 || average_reward: -89.55\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  68\n",
      "Epsilon Used for the episode:  0.6167427788211494\n",
      "EPISODE COMPLETE\n",
      "min_reward: -181.0 || max_reward: 0.0 || total_reward: -34646.0 || average_reward: -96.2388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  69\n",
      "Epsilon Used for the episode:  0.6123899199100513\n",
      "EPISODE COMPLETE\n",
      "min_reward: -181.0 || max_reward: 0.0 || total_reward: -39294.0 || average_reward: -109.15\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  70\n",
      "Epsilon Used for the episode:  0.608068289027306\n",
      "EPISODE COMPLETE\n",
      "min_reward: -191.0 || max_reward: 0.0 || total_reward: -37935.0 || average_reward: -105.375\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  71\n",
      "Epsilon Used for the episode:  0.603777662138601\n",
      "EPISODE COMPLETE\n",
      "min_reward: -181.0 || max_reward: 0.0 || total_reward: -38867.0 || average_reward: -107.96388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  72\n",
      "Epsilon Used for the episode:  0.5995178168168771\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -38356.0 || average_reward: -106.54444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  73\n",
      "Epsilon Used for the episode:  0.5952885322307991\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -38694.0 || average_reward: -107.48333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  74\n",
      "Epsilon Used for the episode:  0.5910895891333068\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -34032.0 || average_reward: -94.53333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  75\n",
      "Epsilon Used for the episode:  0.5869207698502498\n",
      "EPISODE COMPLETE\n",
      "min_reward: -195.0 || max_reward: 0.0 || total_reward: -39145.0 || average_reward: -108.73611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  76\n",
      "Epsilon Used for the episode:  0.5827818582691027\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -35292.0 || average_reward: -98.03333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  77\n",
      "Epsilon Used for the episode:  0.5786726398277623\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -37116.0 || average_reward: -103.1\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  78\n",
      "Epsilon Used for the episode:  0.5745929015034243\n",
      "EPISODE COMPLETE\n",
      "min_reward: -188.0 || max_reward: 0.0 || total_reward: -38063.0 || average_reward: -105.73055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  79\n",
      "Epsilon Used for the episode:  0.5705424318015405\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -37485.0 || average_reward: -104.125\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  80\n",
      "Epsilon Used for the episode:  0.5665210207448542\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -36714.0 || average_reward: -101.98333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  81\n",
      "Epsilon Used for the episode:  0.5625284598625158\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -39059.0 || average_reward: -108.49722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  82\n",
      "Epsilon Used for the episode:  0.558564542179275\n",
      "EPISODE COMPLETE\n",
      "min_reward: -183.0 || max_reward: 0.0 || total_reward: -39483.0 || average_reward: -109.675\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  83\n",
      "Epsilon Used for the episode:  0.5546290622047512\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -35383.0 || average_reward: -98.28611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  84\n",
      "Epsilon Used for the episode:  0.5507218159227815\n",
      "EPISODE COMPLETE\n",
      "min_reward: -208.0 || max_reward: 0.0 || total_reward: -42455.0 || average_reward: -117.93055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  85\n",
      "Epsilon Used for the episode:  0.5468426007808435\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -40532.0 || average_reward: -112.58888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  86\n",
      "Epsilon Used for the episode:  0.5429912156795552\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -37276.0 || average_reward: -103.54444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  87\n",
      "Epsilon Used for the episode:  0.5391674609622508\n",
      "EPISODE COMPLETE\n",
      "min_reward: -178.0 || max_reward: 0.0 || total_reward: -37855.0 || average_reward: -105.15277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  88\n",
      "Epsilon Used for the episode:  0.5353711384046292\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -37766.0 || average_reward: -104.90555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  89\n",
      "Epsilon Used for the episode:  0.5316020512044791\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -38112.0 || average_reward: -105.86666666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  90\n",
      "Epsilon Used for the episode:  0.5278600039714758\n",
      "EPISODE COMPLETE\n",
      "min_reward: -201.0 || max_reward: 0.0 || total_reward: -38691.0 || average_reward: -107.475\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  91\n",
      "Epsilon Used for the episode:  0.5241448027170527\n",
      "EPISODE COMPLETE\n",
      "min_reward: -188.0 || max_reward: 0.0 || total_reward: -44712.0 || average_reward: -124.2\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  92\n",
      "Epsilon Used for the episode:  0.5204562548443447\n",
      "EPISODE COMPLETE\n",
      "min_reward: -172.0 || max_reward: 0.0 || total_reward: -38483.0 || average_reward: -106.89722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  93\n",
      "Epsilon Used for the episode:  0.5167941691382041\n",
      "EPISODE COMPLETE\n",
      "min_reward: -195.0 || max_reward: 0.0 || total_reward: -39399.0 || average_reward: -109.44166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  94\n",
      "Epsilon Used for the episode:  0.5131583557552878\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -37108.0 || average_reward: -103.07777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  95\n",
      "Epsilon Used for the episode:  0.5095486262142156\n",
      "EPISODE COMPLETE\n",
      "min_reward: -189.0 || max_reward: 0.0 || total_reward: -41399.0 || average_reward: -114.99722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  96\n",
      "Epsilon Used for the episode:  0.5059647933858\n",
      "EPISODE COMPLETE\n",
      "min_reward: -186.0 || max_reward: 0.0 || total_reward: -38879.0 || average_reward: -107.99722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  97\n",
      "Epsilon Used for the episode:  0.5024066714833445\n",
      "EPISODE COMPLETE\n",
      "min_reward: -210.0 || max_reward: 0.0 || total_reward: -44236.0 || average_reward: -122.87777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  98\n",
      "Epsilon Used for the episode:  0.4988740760530127\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -46119.0 || average_reward: -128.10833333333332\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  99\n",
      "Epsilon Used for the episode:  0.4953668239642664\n",
      "EPISODE COMPLETE\n",
      "min_reward: -175.0 || max_reward: 0.0 || total_reward: -41076.0 || average_reward: -114.1\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  100\n",
      "Epsilon Used for the episode:  0.491884733400372\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -47261.0 || average_reward: -131.28055555555557\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  101\n",
      "Epsilon Used for the episode:  0.48842762384897465\n",
      "EPISODE COMPLETE\n",
      "min_reward: -202.0 || max_reward: 0.0 || total_reward: -46519.0 || average_reward: -129.21944444444443\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  102\n",
      "Epsilon Used for the episode:  0.48499531609274116\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -41267.0 || average_reward: -114.63055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  103\n",
      "Epsilon Used for the episode:  0.4815876322000687\n",
      "EPISODE COMPLETE\n",
      "min_reward: -201.0 || max_reward: 0.0 || total_reward: -44869.0 || average_reward: -124.6361111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  104\n",
      "Epsilon Used for the episode:  0.47820439551586125\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -38116.0 || average_reward: -105.87777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  105\n",
      "Epsilon Used for the episode:  0.4748454306523713\n",
      "EPISODE COMPLETE\n",
      "min_reward: -212.0 || max_reward: 0.0 || total_reward: -50078.0 || average_reward: -139.10555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  106\n",
      "Epsilon Used for the episode:  0.4715105634801081\n",
      "EPISODE COMPLETE\n",
      "min_reward: -195.0 || max_reward: 0.0 || total_reward: -39788.0 || average_reward: -110.52222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  107\n",
      "Epsilon Used for the episode:  0.46819962111881064\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -49765.0 || average_reward: -138.23611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  108\n",
      "Epsilon Used for the episode:  0.4649124319284854\n",
      "EPISODE COMPLETE\n",
      "min_reward: -191.0 || max_reward: 0.0 || total_reward: -43552.0 || average_reward: -120.97777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  109\n",
      "Epsilon Used for the episode:  0.46164882550050856\n",
      "EPISODE COMPLETE\n",
      "min_reward: -207.0 || max_reward: 0.0 || total_reward: -45881.0 || average_reward: -127.44722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  110\n",
      "Epsilon Used for the episode:  0.4584086326487921\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -40105.0 || average_reward: -111.40277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  111\n",
      "Epsilon Used for the episode:  0.45519168540101296\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -48965.0 || average_reward: -136.01388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  112\n",
      "Epsilon Used for the episode:  0.45199781698990527\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -52187.0 || average_reward: -144.9638888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  113\n",
      "Epsilon Used for the episode:  0.4488268618446154\n",
      "EPISODE COMPLETE\n",
      "min_reward: -175.0 || max_reward: 0.0 || total_reward: -40528.0 || average_reward: -112.57777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  114\n",
      "Epsilon Used for the episode:  0.44567865558211844\n",
      "EPISODE COMPLETE\n",
      "min_reward: -181.0 || max_reward: 0.0 || total_reward: -41750.0 || average_reward: -115.97222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  115\n",
      "Epsilon Used for the episode:  0.4425530349986968\n",
      "EPISODE COMPLETE\n",
      "min_reward: -189.0 || max_reward: 0.0 || total_reward: -40992.0 || average_reward: -113.86666666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  116\n",
      "Epsilon Used for the episode:  0.43944983806147925\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -44460.0 || average_reward: -123.5\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  117\n",
      "Epsilon Used for the episode:  0.4363689039000419\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -48576.0 || average_reward: -134.93333333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  118\n",
      "Epsilon Used for the episode:  0.43331007279806766\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -39238.0 || average_reward: -108.99444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  119\n",
      "Epsilon Used for the episode:  0.4302731861850672\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -54946.0 || average_reward: -152.62777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  120\n",
      "Epsilon Used for the episode:  0.42725808662815845\n",
      "EPISODE COMPLETE\n",
      "min_reward: -210.0 || max_reward: 0.0 || total_reward: -47022.0 || average_reward: -130.61666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  121\n",
      "Epsilon Used for the episode:  0.42426461782390507\n",
      "EPISODE COMPLETE\n",
      "min_reward: -210.0 || max_reward: 0.0 || total_reward: -48496.0 || average_reward: -134.7111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  122\n",
      "Epsilon Used for the episode:  0.4212926245902139\n",
      "EPISODE COMPLETE\n",
      "min_reward: -207.0 || max_reward: 0.0 || total_reward: -46697.0 || average_reward: -129.7138888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  123\n",
      "Epsilon Used for the episode:  0.4183419528582901\n",
      "EPISODE COMPLETE\n",
      "min_reward: -210.0 || max_reward: 0.0 || total_reward: -51353.0 || average_reward: -142.6472222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  124\n",
      "Epsilon Used for the episode:  0.41541244966465046\n",
      "EPISODE COMPLETE\n",
      "min_reward: -193.0 || max_reward: 0.0 || total_reward: -45732.0 || average_reward: -127.03333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  125\n",
      "Epsilon Used for the episode:  0.4125039631431931\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -49497.0 || average_reward: -137.49166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  126\n",
      "Epsilon Used for the episode:  0.4096163425173256\n",
      "EPISODE COMPLETE\n",
      "min_reward: -200.0 || max_reward: 0.0 || total_reward: -43624.0 || average_reward: -121.17777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  127\n",
      "Epsilon Used for the episode:  0.4067494380921479\n",
      "EPISODE COMPLETE\n",
      "min_reward: -196.0 || max_reward: 0.0 || total_reward: -46495.0 || average_reward: -129.15277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  128\n",
      "Epsilon Used for the episode:  0.40390310124669265\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -54462.0 || average_reward: -151.28333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  129\n",
      "Epsilon Used for the episode:  0.40107718442622026\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -40434.0 || average_reward: -112.31666666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  130\n",
      "Epsilon Used for the episode:  0.3982715411345699\n",
      "EPISODE COMPLETE\n",
      "min_reward: -205.0 || max_reward: 0.0 || total_reward: -49201.0 || average_reward: -136.66944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  131\n",
      "Epsilon Used for the episode:  0.3954860259265652\n",
      "EPISODE COMPLETE\n",
      "min_reward: -202.0 || max_reward: 0.0 || total_reward: -45347.0 || average_reward: -125.96388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  132\n",
      "Epsilon Used for the episode:  0.3927204944004737\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -55027.0 || average_reward: -152.8527777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  133\n",
      "Epsilon Used for the episode:  0.38997480319052186\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -39252.0 || average_reward: -109.03333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  134\n",
      "Epsilon Used for the episode:  0.38724880995946254\n",
      "EPISODE COMPLETE\n",
      "min_reward: -211.0 || max_reward: 0.0 || total_reward: -48034.0 || average_reward: -133.42777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  135\n",
      "Epsilon Used for the episode:  0.38454237339119607\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -51482.0 || average_reward: -143.00555555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  136\n",
      "Epsilon Used for the episode:  0.38185535318344466\n",
      "EPISODE COMPLETE\n",
      "min_reward: -208.0 || max_reward: 0.0 || total_reward: -49142.0 || average_reward: -136.50555555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  137\n",
      "Epsilon Used for the episode:  0.37918761004047885\n",
      "EPISODE COMPLETE\n",
      "min_reward: -203.0 || max_reward: 0.0 || total_reward: -45687.0 || average_reward: -126.90833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  138\n",
      "Epsilon Used for the episode:  0.37653900566589693\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -51013.0 || average_reward: -141.70277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  139\n",
      "Epsilon Used for the episode:  0.37390940275545487\n",
      "EPISODE COMPLETE\n",
      "min_reward: -192.0 || max_reward: 0.0 || total_reward: -45007.0 || average_reward: -125.01944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  140\n",
      "Epsilon Used for the episode:  0.3712986649899488\n",
      "EPISODE COMPLETE\n",
      "min_reward: -208.0 || max_reward: 0.0 || total_reward: -48522.0 || average_reward: -134.78333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  141\n",
      "Epsilon Used for the episode:  0.36870665702814837\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -52551.0 || average_reward: -145.975\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  142\n",
      "Epsilon Used for the episode:  0.36613324449978035\n",
      "EPISODE COMPLETE\n",
      "min_reward: -206.0 || max_reward: 0.0 || total_reward: -44047.0 || average_reward: -122.35277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  143\n",
      "Epsilon Used for the episode:  0.36357829399856295\n",
      "EPISODE COMPLETE\n",
      "min_reward: -210.0 || max_reward: 0.0 || total_reward: -48464.0 || average_reward: -134.62222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  144\n",
      "Epsilon Used for the episode:  0.36104167307529\n",
      "EPISODE COMPLETE\n",
      "min_reward: -210.0 || max_reward: 0.0 || total_reward: -43359.0 || average_reward: -120.44166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  145\n",
      "Epsilon Used for the episode:  0.35852325023096476\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -52328.0 || average_reward: -145.35555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  146\n",
      "Epsilon Used for the episode:  0.356022894909983\n",
      "EPISODE COMPLETE\n",
      "min_reward: -209.0 || max_reward: 0.0 || total_reward: -50125.0 || average_reward: -139.23611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  147\n",
      "Epsilon Used for the episode:  0.3535404774933649\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -52741.0 || average_reward: -146.50277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  148\n",
      "Epsilon Used for the episode:  0.3510758692920357\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -56252.0 || average_reward: -156.25555555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  149\n",
      "Epsilon Used for the episode:  0.34862894254015425\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -52842.0 || average_reward: -146.78333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  150\n",
      "Epsilon Used for the episode:  0.34619957038848975\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -53545.0 || average_reward: -148.73611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  151\n",
      "Epsilon Used for the episode:  0.34378762689784587\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59430.0 || average_reward: -165.08333333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  152\n",
      "Epsilon Used for the episode:  0.34139298703253174\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -49848.0 || average_reward: -138.46666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  153\n",
      "Epsilon Used for the episode:  0.33901552665388063\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59753.0 || average_reward: -165.98055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  154\n",
      "Epsilon Used for the episode:  0.336655122513814\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -55494.0 || average_reward: -154.15\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  155\n",
      "Epsilon Used for the episode:  0.33431165224845266\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -52948.0 || average_reward: -147.07777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  156\n",
      "Epsilon Used for the episode:  0.3319849943717732\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58825.0 || average_reward: -163.40277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  157\n",
      "Epsilon Used for the episode:  0.32967502826931017\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -54059.0 || average_reward: -150.1638888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  158\n",
      "Epsilon Used for the episode:  0.3273816341919037\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -54995.0 || average_reward: -152.76388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  159\n",
      "Epsilon Used for the episode:  0.32510469324949104\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -55914.0 || average_reward: -155.31666666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  160\n",
      "Epsilon Used for the episode:  0.32284408740494397\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -60221.0 || average_reward: -167.28055555555557\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  161\n",
      "Epsilon Used for the episode:  0.320599699467949\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58389.0 || average_reward: -162.19166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  162\n",
      "Epsilon Used for the episode:  0.318371413088933\n",
      "EPISODE COMPLETE\n",
      "min_reward: -206.0 || max_reward: 0.0 || total_reward: -48405.0 || average_reward: -134.45833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  163\n",
      "Epsilon Used for the episode:  0.31615911275303116\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -56074.0 || average_reward: -155.76111111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  164\n",
      "Epsilon Used for the episode:  0.31396268377409847\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -52551.0 || average_reward: -145.975\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  165\n",
      "Epsilon Used for the episode:  0.31178201228876484\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -60535.0 || average_reward: -168.15277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  166\n",
      "Epsilon Used for the episode:  0.3096169852505321\n",
      "EPISODE COMPLETE\n",
      "min_reward: -215.0 || max_reward: 0.0 || total_reward: -49995.0 || average_reward: -138.875\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  167\n",
      "Epsilon Used for the episode:  0.3074674904239138\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -54917.0 || average_reward: -152.54722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  168\n",
      "Epsilon Used for the episode:  0.30533341637861655\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59850.0 || average_reward: -166.25\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  169\n",
      "Epsilon Used for the episode:  0.30321465248376406\n",
      "EPISODE COMPLETE\n",
      "min_reward: -210.0 || max_reward: 0.0 || total_reward: -51849.0 || average_reward: -144.025\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  170\n",
      "Epsilon Used for the episode:  0.3011110889021616\n",
      "EPISODE COMPLETE\n",
      "min_reward: -205.0 || max_reward: 0.0 || total_reward: -50995.0 || average_reward: -141.65277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  171\n",
      "Epsilon Used for the episode:  0.29902261658460183\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59494.0 || average_reward: -165.26111111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  172\n",
      "Epsilon Used for the episode:  0.2969491272642123\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -60931.0 || average_reward: -169.25277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  173\n",
      "Epsilon Used for the episode:  0.29489051345084205\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59185.0 || average_reward: -164.40277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  174\n",
      "Epsilon Used for the episode:  0.2928466684254901\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -56336.0 || average_reward: -156.48888888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  175\n",
      "Epsilon Used for the episode:  0.2908174862347727\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58910.0 || average_reward: -163.63888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  176\n",
      "Epsilon Used for the episode:  0.28880286168543046\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -54960.0 || average_reward: -152.66666666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  177\n",
      "Epsilon Used for the episode:  0.28680269033887573\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -60288.0 || average_reward: -167.46666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  178\n",
      "Epsilon Used for the episode:  0.28481686850577786\n",
      "EPISODE COMPLETE\n",
      "min_reward: -209.0 || max_reward: 0.0 || total_reward: -46120.0 || average_reward: -128.11111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  179\n",
      "Epsilon Used for the episode:  0.2828452932406884\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58904.0 || average_reward: -163.62222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  180\n",
      "Epsilon Used for the episode:  0.280887862336704\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58671.0 || average_reward: -162.975\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  181\n",
      "Epsilon Used for the episode:  0.2789444743201682\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -51735.0 || average_reward: -143.70833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  182\n",
      "Epsilon Used for the episode:  0.2770150284454111\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -56696.0 || average_reward: -157.48888888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  183\n",
      "Epsilon Used for the episode:  0.27509942468952636\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59450.0 || average_reward: -165.13888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  184\n",
      "Epsilon Used for the episode:  0.2731975637471864\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -55062.0 || average_reward: -152.95\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  185\n",
      "Epsilon Used for the episode:  0.271309347025494\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59004.0 || average_reward: -163.9\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  186\n",
      "Epsilon Used for the episode:  0.26943467663887133\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58347.0 || average_reward: -162.075\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  187\n",
      "Epsilon Used for the episode:  0.2675734554039858\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59240.0 || average_reward: -164.55555555555554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  188\n",
      "Epsilon Used for the episode:  0.2657255868347118\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -54076.0 || average_reward: -150.2111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  189\n",
      "Epsilon Used for the episode:  0.2638909751371289\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58317.0 || average_reward: -161.99166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  190\n",
      "Epsilon Used for the episode:  0.2620695252045557\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59792.0 || average_reward: -166.0888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  191\n",
      "Epsilon Used for the episode:  0.26026114261261996\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59426.0 || average_reward: -165.07222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  192\n",
      "Epsilon Used for the episode:  0.2584657336143629\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -57216.0 || average_reward: -158.93333333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  193\n",
      "Epsilon Used for the episode:  0.2566832051353803\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58890.0 || average_reward: -163.58333333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  194\n",
      "Epsilon Used for the episode:  0.2549134647689963\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -56668.0 || average_reward: -157.4111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  195\n",
      "Epsilon Used for the episode:  0.25315642077147416\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59934.0 || average_reward: -166.48333333333332\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  196\n",
      "Epsilon Used for the episode:  0.2514119820572594\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58394.0 || average_reward: -162.20555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  197\n",
      "Epsilon Used for the episode:  0.24968005819425854\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -57761.0 || average_reward: -160.44722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  198\n",
      "Epsilon Used for the episode:  0.24796055939915068\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58376.0 || average_reward: -162.15555555555557\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  199\n",
      "Epsilon Used for the episode:  0.2462533965327331\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58178.0 || average_reward: -161.60555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  200\n",
      "Epsilon Used for the episode:  0.24455848109530054\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -57194.0 || average_reward: -158.87222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  201\n",
      "Epsilon Used for the episode:  0.24287572522205716\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -60046.0 || average_reward: -166.79444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  202\n",
      "Epsilon Used for the episode:  0.2412050416785616\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -61278.0 || average_reward: -170.21666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  203\n",
      "Epsilon Used for the episode:  0.23954634385620485\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59093.0 || average_reward: -164.1472222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  204\n",
      "Epsilon Used for the episode:  0.2378995457677203\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -61608.0 || average_reward: -171.13333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  205\n",
      "Epsilon Used for the episode:  0.23626456204272628\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -61442.0 || average_reward: -170.67222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  206\n",
      "Epsilon Used for the episode:  0.23464130792330032\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59353.0 || average_reward: -164.86944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  207\n",
      "Epsilon Used for the episode:  0.23302969925958536\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58428.0 || average_reward: -162.3\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  208\n",
      "Epsilon Used for the episode:  0.23142965250542732\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -58340.0 || average_reward: -162.05555555555554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  209\n",
      "Epsilon Used for the episode:  0.22984108471404416\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -59585.0 || average_reward: -165.51388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  210\n",
      "Epsilon Used for the episode:  0.22826391353372577\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -57411.0 || average_reward: -159.475\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  211\n",
      "Epsilon Used for the episode:  0.226698057203565\n",
      "EPISODE COMPLETE\n",
      "min_reward: -202.0 || max_reward: 0.0 || total_reward: -50442.0 || average_reward: -140.11666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  212\n",
      "Epsilon Used for the episode:  0.22514343454921906\n",
      "EPISODE COMPLETE\n",
      "min_reward: -204.0 || max_reward: 0.0 || total_reward: -50786.0 || average_reward: -141.07222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  213\n",
      "Epsilon Used for the episode:  0.22359996497870133\n",
      "EPISODE COMPLETE\n",
      "min_reward: -201.0 || max_reward: 0.0 || total_reward: -52442.0 || average_reward: -145.67222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  214\n",
      "Epsilon Used for the episode:  0.22206756847820366\n",
      "EPISODE COMPLETE\n",
      "min_reward: -196.0 || max_reward: 0.0 || total_reward: -51562.0 || average_reward: -143.2277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  215\n",
      "Epsilon Used for the episode:  0.2205461656079482\n",
      "EPISODE COMPLETE\n",
      "min_reward: -199.0 || max_reward: 0.0 || total_reward: -48725.0 || average_reward: -135.34722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  216\n",
      "Epsilon Used for the episode:  0.21903567749806957\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -51968.0 || average_reward: -144.35555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  217\n",
      "Epsilon Used for the episode:  0.21753602584452578\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -53769.0 || average_reward: -149.35833333333332\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  218\n",
      "Epsilon Used for the episode:  0.21604713290503924\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -48046.0 || average_reward: -133.4611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  219\n",
      "Epsilon Used for the episode:  0.21456892149506668\n",
      "EPISODE COMPLETE\n",
      "min_reward: -176.0 || max_reward: 0.0 || total_reward: -46741.0 || average_reward: -129.8361111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  220\n",
      "Epsilon Used for the episode:  0.2131013149837974\n",
      "EPISODE COMPLETE\n",
      "min_reward: -172.0 || max_reward: 0.0 || total_reward: -46668.0 || average_reward: -129.63333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  221\n",
      "Epsilon Used for the episode:  0.2116442372901813\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -60227.0 || average_reward: -167.29722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  222\n",
      "Epsilon Used for the episode:  0.21019761287898434\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -51222.0 || average_reward: -142.28333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  223\n",
      "Epsilon Used for the episode:  0.20876136675687315\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -37456.0 || average_reward: -104.04444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  224\n",
      "Epsilon Used for the episode:  0.20733542446852707\n",
      "EPISODE COMPLETE\n",
      "min_reward: -194.0 || max_reward: 0.0 || total_reward: -45000.0 || average_reward: -125.0\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  225\n",
      "Epsilon Used for the episode:  0.2059197120927785\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -44371.0 || average_reward: -123.25277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  226\n",
      "Epsilon Used for the episode:  0.20451415623878097\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -42382.0 || average_reward: -117.72777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  227\n",
      "Epsilon Used for the episode:  0.2031186840422041\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -39448.0 || average_reward: -109.57777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  228\n",
      "Epsilon Used for the episode:  0.20173322316145678\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -46289.0 || average_reward: -128.58055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  229\n",
      "Epsilon Used for the episode:  0.2003577017739366\n",
      "EPISODE COMPLETE\n",
      "min_reward: -181.0 || max_reward: 0.0 || total_reward: -46846.0 || average_reward: -130.12777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  230\n",
      "Epsilon Used for the episode:  0.19899204857230687\n",
      "EPISODE COMPLETE\n",
      "min_reward: -169.0 || max_reward: 0.0 || total_reward: -44160.0 || average_reward: -122.66666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  231\n",
      "Epsilon Used for the episode:  0.19763619276079972\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -60613.0 || average_reward: -168.36944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  232\n",
      "Epsilon Used for the episode:  0.19629006405154625\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -49506.0 || average_reward: -137.51666666666668\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  233\n",
      "Epsilon Used for the episode:  0.19495359266093276\n",
      "EPISODE COMPLETE\n",
      "min_reward: -172.0 || max_reward: 0.0 || total_reward: -47412.0 || average_reward: -131.7\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  234\n",
      "Epsilon Used for the episode:  0.193626709305983\n",
      "EPISODE COMPLETE\n",
      "min_reward: -170.0 || max_reward: 0.0 || total_reward: -47763.0 || average_reward: -132.675\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  235\n",
      "Epsilon Used for the episode:  0.19230934520076673\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -45188.0 || average_reward: -125.52222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  236\n",
      "Epsilon Used for the episode:  0.19100143205283368\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -42867.0 || average_reward: -119.075\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  237\n",
      "Epsilon Used for the episode:  0.18970290205967338\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -42540.0 || average_reward: -118.16666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  238\n",
      "Epsilon Used for the episode:  0.1884136879052002\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -42741.0 || average_reward: -118.725\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  239\n",
      "Epsilon Used for the episode:  0.18713372275626355\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -42605.0 || average_reward: -118.34722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  240\n",
      "Epsilon Used for the episode:  0.18586294025918362\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -43150.0 || average_reward: -119.86111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  241\n",
      "Epsilon Used for the episode:  0.184601274536311\n",
      "EPISODE COMPLETE\n",
      "min_reward: -210.0 || max_reward: 0.0 || total_reward: -48558.0 || average_reward: -134.88333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  242\n",
      "Epsilon Used for the episode:  0.18334866018261226\n",
      "EPISODE COMPLETE\n",
      "min_reward: -187.0 || max_reward: 0.0 || total_reward: -47241.0 || average_reward: -131.225\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  243\n",
      "Epsilon Used for the episode:  0.18210503226227862\n",
      "EPISODE COMPLETE\n",
      "min_reward: -189.0 || max_reward: 0.0 || total_reward: -46391.0 || average_reward: -128.86388888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  244\n",
      "Epsilon Used for the episode:  0.18087032630536026\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -44063.0 || average_reward: -122.39722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  245\n",
      "Epsilon Used for the episode:  0.1796444783044238\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -45063.0 || average_reward: -125.175\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  246\n",
      "Epsilon Used for the episode:  0.1784274247112344\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -44997.0 || average_reward: -124.99166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  247\n",
      "Epsilon Used for the episode:  0.17721910243346126\n",
      "EPISODE COMPLETE\n",
      "min_reward: -181.0 || max_reward: 0.0 || total_reward: -45136.0 || average_reward: -125.37777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  248\n",
      "Epsilon Used for the episode:  0.1760194488314068\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -46568.0 || average_reward: -129.35555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  249\n",
      "Epsilon Used for the episode:  0.1748284017147597\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -43869.0 || average_reward: -121.85833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  250\n",
      "Epsilon Used for the episode:  0.17364589933937066\n",
      "EPISODE COMPLETE\n",
      "min_reward: -178.0 || max_reward: 0.0 || total_reward: -42873.0 || average_reward: -119.09166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  251\n",
      "Epsilon Used for the episode:  0.17247188040405179\n",
      "EPISODE COMPLETE\n",
      "min_reward: -185.0 || max_reward: 0.0 || total_reward: -50639.0 || average_reward: -140.6638888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  252\n",
      "Epsilon Used for the episode:  0.17130628404739848\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -48709.0 || average_reward: -135.30277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  253\n",
      "Epsilon Used for the episode:  0.17014904984463455\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -47594.0 || average_reward: -132.20555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  254\n",
      "Epsilon Used for the episode:  0.1690001178044799\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -49810.0 || average_reward: -138.36111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  255\n",
      "Epsilon Used for the episode:  0.16785942836604012\n",
      "EPISODE COMPLETE\n",
      "min_reward: -188.0 || max_reward: 0.0 || total_reward: -49737.0 || average_reward: -138.15833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  256\n",
      "Epsilon Used for the episode:  0.1667269223957194\n",
      "EPISODE COMPLETE\n",
      "min_reward: -189.0 || max_reward: 0.0 || total_reward: -48789.0 || average_reward: -135.525\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  257\n",
      "Epsilon Used for the episode:  0.16560254118415446\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -48565.0 || average_reward: -134.90277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  258\n",
      "Epsilon Used for the episode:  0.16448622644317162\n",
      "EPISODE COMPLETE\n",
      "min_reward: -191.0 || max_reward: 0.0 || total_reward: -51893.0 || average_reward: -144.1472222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  259\n",
      "Epsilon Used for the episode:  0.16337792030276463\n",
      "EPISODE COMPLETE\n",
      "min_reward: -201.0 || max_reward: 0.0 || total_reward: -52221.0 || average_reward: -145.05833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  260\n",
      "Epsilon Used for the episode:  0.16227756530809498\n",
      "EPISODE COMPLETE\n",
      "min_reward: -186.0 || max_reward: 0.0 || total_reward: -50364.0 || average_reward: -139.9\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  261\n",
      "Epsilon Used for the episode:  0.1611851044165134\n",
      "EPISODE COMPLETE\n",
      "min_reward: -188.0 || max_reward: 0.0 || total_reward: -50498.0 || average_reward: -140.2722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  262\n",
      "Epsilon Used for the episode:  0.16010048099460247\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -50059.0 || average_reward: -139.05277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  263\n",
      "Epsilon Used for the episode:  0.15902363881524129\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -50774.0 || average_reward: -141.0388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  264\n",
      "Epsilon Used for the episode:  0.1579545220546899\n",
      "EPISODE COMPLETE\n",
      "min_reward: -176.0 || max_reward: 0.0 || total_reward: -48647.0 || average_reward: -135.13055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  265\n",
      "Epsilon Used for the episode:  0.1568930752896962\n",
      "EPISODE COMPLETE\n",
      "min_reward: -191.0 || max_reward: 0.0 || total_reward: -51863.0 || average_reward: -144.0638888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  266\n",
      "Epsilon Used for the episode:  0.15583924349462205\n",
      "EPISODE COMPLETE\n",
      "min_reward: -177.0 || max_reward: 0.0 || total_reward: -49954.0 || average_reward: -138.76111111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  267\n",
      "Epsilon Used for the episode:  0.15479297203859127\n",
      "EPISODE COMPLETE\n",
      "min_reward: -189.0 || max_reward: 0.0 || total_reward: -51115.0 || average_reward: -141.98611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  268\n",
      "Epsilon Used for the episode:  0.15375420668265727\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -49456.0 || average_reward: -137.37777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  269\n",
      "Epsilon Used for the episode:  0.1527228935769913\n",
      "EPISODE COMPLETE\n",
      "min_reward: -181.0 || max_reward: 0.0 || total_reward: -49468.0 || average_reward: -137.4111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  270\n",
      "Epsilon Used for the episode:  0.15169897925809103\n",
      "EPISODE COMPLETE\n",
      "min_reward: -174.0 || max_reward: 0.0 || total_reward: -48160.0 || average_reward: -133.77777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  271\n",
      "Epsilon Used for the episode:  0.15068241064600887\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -47051.0 || average_reward: -130.69722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  272\n",
      "Epsilon Used for the episode:  0.14967313504160032\n",
      "EPISODE COMPLETE\n",
      "min_reward: -199.0 || max_reward: 0.0 || total_reward: -48449.0 || average_reward: -134.58055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  273\n",
      "Epsilon Used for the episode:  0.148671100123792\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -50615.0 || average_reward: -140.59722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  274\n",
      "Epsilon Used for the episode:  0.14767625394686937\n",
      "EPISODE COMPLETE\n",
      "min_reward: -215.0 || max_reward: 0.0 || total_reward: -46969.0 || average_reward: -130.46944444444443\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  275\n",
      "Epsilon Used for the episode:  0.1466885449377839\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -49528.0 || average_reward: -137.57777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  276\n",
      "Epsilon Used for the episode:  0.14570792189347923\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -53721.0 || average_reward: -149.225\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  277\n",
      "Epsilon Used for the episode:  0.1447343339782372\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -56765.0 || average_reward: -157.68055555555554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  278\n",
      "Epsilon Used for the episode:  0.14376773072104232\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -56270.0 || average_reward: -156.30555555555554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  279\n",
      "Epsilon Used for the episode:  0.14280806201296511\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -53494.0 || average_reward: -148.59444444444443\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  280\n",
      "Epsilon Used for the episode:  0.14185527810456489\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -43654.0 || average_reward: -121.2611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  281\n",
      "Epsilon Used for the episode:  0.14090932960331054\n",
      "EPISODE COMPLETE\n",
      "min_reward: -182.0 || max_reward: 0.0 || total_reward: -41693.0 || average_reward: -115.81388888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  282\n",
      "Epsilon Used for the episode:  0.13997016747101984\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -42926.0 || average_reward: -119.2388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  283\n",
      "Epsilon Used for the episode:  0.1390377430213176\n",
      "EPISODE COMPLETE\n",
      "min_reward: -185.0 || max_reward: 0.0 || total_reward: -47614.0 || average_reward: -132.26111111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  284\n",
      "Epsilon Used for the episode:  0.13811200791711142\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -50251.0 || average_reward: -139.5861111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  285\n",
      "Epsilon Used for the episode:  0.13719291416808627\n",
      "EPISODE COMPLETE\n",
      "min_reward: -189.0 || max_reward: 0.0 || total_reward: -48931.0 || average_reward: -135.91944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  286\n",
      "Epsilon Used for the episode:  0.13628041412821634\n",
      "EPISODE COMPLETE\n",
      "min_reward: -198.0 || max_reward: 0.0 || total_reward: -50775.0 || average_reward: -141.04166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  287\n",
      "Epsilon Used for the episode:  0.1353744604932953\n",
      "EPISODE COMPLETE\n",
      "min_reward: -195.0 || max_reward: 0.0 || total_reward: -50336.0 || average_reward: -139.82222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  288\n",
      "Epsilon Used for the episode:  0.13447500629848363\n",
      "EPISODE COMPLETE\n",
      "min_reward: -196.0 || max_reward: 0.0 || total_reward: -52405.0 || average_reward: -145.56944444444446\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  289\n",
      "Epsilon Used for the episode:  0.13358200491587457\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -55451.0 || average_reward: -154.03055555555557\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  290\n",
      "Epsilon Used for the episode:  0.1326954100520764\n",
      "EPISODE COMPLETE\n",
      "min_reward: -179.0 || max_reward: 0.0 || total_reward: -43197.0 || average_reward: -119.99166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  291\n",
      "Epsilon Used for the episode:  0.13181517574581292\n",
      "EPISODE COMPLETE\n",
      "min_reward: -192.0 || max_reward: 0.0 || total_reward: -49439.0 || average_reward: -137.33055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  292\n",
      "Epsilon Used for the episode:  0.13094125636554055\n",
      "EPISODE COMPLETE\n",
      "min_reward: -192.0 || max_reward: 0.0 || total_reward: -47057.0 || average_reward: -130.7138888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  293\n",
      "Epsilon Used for the episode:  0.13007360660708284\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -47441.0 || average_reward: -131.78055555555557\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  294\n",
      "Epsilon Used for the episode:  0.12921218149128197\n",
      "EPISODE COMPLETE\n",
      "min_reward: -202.0 || max_reward: 0.0 || total_reward: -50150.0 || average_reward: -139.30555555555554\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  295\n",
      "Epsilon Used for the episode:  0.12835693636166712\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -55996.0 || average_reward: -155.54444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  296\n",
      "Epsilon Used for the episode:  0.12750782688213919\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -54401.0 || average_reward: -151.11388888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  297\n",
      "Epsilon Used for the episode:  0.12666480903467267\n",
      "EPISODE COMPLETE\n",
      "min_reward: -208.0 || max_reward: 0.0 || total_reward: -54036.0 || average_reward: -150.1\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  298\n",
      "Epsilon Used for the episode:  0.12582783911703344\n",
      "EPISODE COMPLETE\n",
      "min_reward: -202.0 || max_reward: 0.0 || total_reward: -54077.0 || average_reward: -150.2138888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  299\n",
      "Epsilon Used for the episode:  0.12499687374051358\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -53707.0 || average_reward: -149.1861111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  300\n",
      "Epsilon Used for the episode:  0.12417186982768189\n",
      "EPISODE COMPLETE\n",
      "min_reward: -185.0 || max_reward: 0.0 || total_reward: -44913.0 || average_reward: -124.75833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  301\n",
      "Epsilon Used for the episode:  0.12335278461015081\n",
      "EPISODE COMPLETE\n",
      "min_reward: -215.0 || max_reward: 0.0 || total_reward: -48189.0 || average_reward: -133.85833333333332\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  302\n",
      "Epsilon Used for the episode:  0.12253957562635913\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -38714.0 || average_reward: -107.53888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  303\n",
      "Epsilon Used for the episode:  0.12173220071937108\n",
      "EPISODE COMPLETE\n",
      "min_reward: -212.0 || max_reward: 0.0 || total_reward: -50037.0 || average_reward: -138.99166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  304\n",
      "Epsilon Used for the episode:  0.12093061803469064\n",
      "EPISODE COMPLETE\n",
      "min_reward: -205.0 || max_reward: 0.0 || total_reward: -47885.0 || average_reward: -133.01388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  305\n",
      "Epsilon Used for the episode:  0.12013478601809197\n",
      "EPISODE COMPLETE\n",
      "min_reward: -197.0 || max_reward: 0.0 || total_reward: -47085.0 || average_reward: -130.79166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  306\n",
      "Epsilon Used for the episode:  0.1193446634134651\n",
      "EPISODE COMPLETE\n",
      "min_reward: -206.0 || max_reward: 0.0 || total_reward: -48391.0 || average_reward: -134.41944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  307\n",
      "Epsilon Used for the episode:  0.11856020926067717\n",
      "EPISODE COMPLETE\n",
      "min_reward: -189.0 || max_reward: 0.0 || total_reward: -45655.0 || average_reward: -126.81944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  308\n",
      "Epsilon Used for the episode:  0.11778138289344932\n",
      "EPISODE COMPLETE\n",
      "min_reward: -180.0 || max_reward: 0.0 || total_reward: -46435.0 || average_reward: -128.98611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  309\n",
      "Epsilon Used for the episode:  0.1170081439372482\n",
      "EPISODE COMPLETE\n",
      "min_reward: -176.0 || max_reward: 0.0 || total_reward: -44551.0 || average_reward: -123.75277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  310\n",
      "Epsilon Used for the episode:  0.11624045230719318\n",
      "EPISODE COMPLETE\n",
      "min_reward: -185.0 || max_reward: 0.0 || total_reward: -47324.0 || average_reward: -131.45555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  311\n",
      "Epsilon Used for the episode:  0.11547826820597827\n",
      "EPISODE COMPLETE\n",
      "min_reward: -186.0 || max_reward: 0.0 || total_reward: -47958.0 || average_reward: -133.21666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  312\n",
      "Epsilon Used for the episode:  0.11472155212180889\n",
      "EPISODE COMPLETE\n",
      "min_reward: -188.0 || max_reward: 0.0 || total_reward: -47641.0 || average_reward: -132.3361111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  313\n",
      "Epsilon Used for the episode:  0.1139702648263538\n",
      "EPISODE COMPLETE\n",
      "min_reward: -188.0 || max_reward: 0.0 || total_reward: -46670.0 || average_reward: -129.63888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  314\n",
      "Epsilon Used for the episode:  0.11322436737271138\n",
      "EPISODE COMPLETE\n",
      "min_reward: -195.0 || max_reward: 0.0 || total_reward: -48404.0 || average_reward: -134.45555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  315\n",
      "Epsilon Used for the episode:  0.1124838210933906\n",
      "EPISODE COMPLETE\n",
      "min_reward: -182.0 || max_reward: 0.0 || total_reward: -46682.0 || average_reward: -129.67222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  316\n",
      "Epsilon Used for the episode:  0.11174858759830644\n",
      "EPISODE COMPLETE\n",
      "min_reward: -171.0 || max_reward: 0.0 || total_reward: -43416.0 || average_reward: -120.6\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  317\n",
      "Epsilon Used for the episode:  0.11101862877278988\n",
      "EPISODE COMPLETE\n",
      "min_reward: -195.0 || max_reward: 0.0 || total_reward: -47167.0 || average_reward: -131.01944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  318\n",
      "Epsilon Used for the episode:  0.11029390677561195\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -43771.0 || average_reward: -121.58611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  319\n",
      "Epsilon Used for the episode:  0.10957438403702201\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -44919.0 || average_reward: -124.775\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  320\n",
      "Epsilon Used for the episode:  0.10886002325680019\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -38617.0 || average_reward: -107.26944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  321\n",
      "Epsilon Used for the episode:  0.10815078740232358\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -40748.0 || average_reward: -113.18888888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  322\n",
      "Epsilon Used for the episode:  0.1074466397066467\n",
      "EPISODE COMPLETE\n",
      "min_reward: -184.0 || max_reward: 0.0 || total_reward: -44296.0 || average_reward: -123.04444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  323\n",
      "Epsilon Used for the episode:  0.1067475436665953\n",
      "EPISODE COMPLETE\n",
      "min_reward: -178.0 || max_reward: 0.0 || total_reward: -45981.0 || average_reward: -127.725\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  324\n",
      "Epsilon Used for the episode:  0.1060534630408741\n",
      "EPISODE COMPLETE\n",
      "min_reward: -175.0 || max_reward: 0.0 || total_reward: -43100.0 || average_reward: -119.72222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  325\n",
      "Epsilon Used for the episode:  0.10536436184818809\n",
      "EPISODE COMPLETE\n",
      "min_reward: -173.0 || max_reward: 0.0 || total_reward: -43145.0 || average_reward: -119.84722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  326\n",
      "Epsilon Used for the episode:  0.10468020436537702\n",
      "EPISODE COMPLETE\n",
      "min_reward: -175.0 || max_reward: 0.0 || total_reward: -43411.0 || average_reward: -120.58611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  327\n",
      "Epsilon Used for the episode:  0.1040009551255638\n",
      "EPISODE COMPLETE\n",
      "min_reward: -176.0 || max_reward: 0.0 || total_reward: -44137.0 || average_reward: -122.60277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  328\n",
      "Epsilon Used for the episode:  0.10332657891631575\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -42313.0 || average_reward: -117.53611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  329\n",
      "Epsilon Used for the episode:  0.10265704077781915\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -40091.0 || average_reward: -111.3638888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  330\n",
      "Epsilon Used for the episode:  0.1019923060010669\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -33538.0 || average_reward: -93.16111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  331\n",
      "Epsilon Used for the episode:  0.10133234012605935\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -35957.0 || average_reward: -99.88055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  332\n",
      "Epsilon Used for the episode:  0.10067710894001775\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -38351.0 || average_reward: -106.53055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  333\n",
      "Epsilon Used for the episode:  0.10002657847561067\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -39602.0 || average_reward: -110.00555555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  334\n",
      "Epsilon Used for the episode:  0.09938071500919317\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -37142.0 || average_reward: -103.17222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  335\n",
      "Epsilon Used for the episode:  0.09873948505905843\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -36692.0 || average_reward: -101.92222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  336\n",
      "Epsilon Used for the episode:  0.09810285538370231\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -39598.0 || average_reward: -109.99444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  337\n",
      "Epsilon Used for the episode:  0.09747079298009984\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -37498.0 || average_reward: -104.16111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  338\n",
      "Epsilon Used for the episode:  0.09684326508199446\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -38461.0 || average_reward: -106.83611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  339\n",
      "Epsilon Used for the episode:  0.09622023915819947\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -39253.0 || average_reward: -109.03611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  340\n",
      "Epsilon Used for the episode:  0.09560168291091133\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -38814.0 || average_reward: -107.81666666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  341\n",
      "Epsilon Used for the episode:  0.09498756427403575\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -36463.0 || average_reward: -101.28611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  342\n",
      "Epsilon Used for the episode:  0.094377851411525\n",
      "EPISODE COMPLETE\n",
      "min_reward: -168.0 || max_reward: 0.0 || total_reward: -37080.0 || average_reward: -103.0\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  343\n",
      "Epsilon Used for the episode:  0.09377251271572783\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -36418.0 || average_reward: -101.16111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  344\n",
      "Epsilon Used for the episode:  0.09317151680575059\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -35508.0 || average_reward: -98.63333333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  345\n",
      "Epsilon Used for the episode:  0.09257483252583075\n",
      "EPISODE COMPLETE\n",
      "min_reward: -186.0 || max_reward: 0.0 || total_reward: -37398.0 || average_reward: -103.88333333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  346\n",
      "Epsilon Used for the episode:  0.09198242894372162\n",
      "EPISODE COMPLETE\n",
      "min_reward: -160.0 || max_reward: 0.0 || total_reward: -34524.0 || average_reward: -95.9\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  347\n",
      "Epsilon Used for the episode:  0.09139427534908882\n",
      "EPISODE COMPLETE\n",
      "min_reward: -216.0 || max_reward: 0.0 || total_reward: -41418.0 || average_reward: -115.05\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  348\n",
      "Epsilon Used for the episode:  0.09081034125191834\n",
      "EPISODE COMPLETE\n",
      "min_reward: -176.0 || max_reward: 0.0 || total_reward: -39645.0 || average_reward: -110.125\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  349\n",
      "Epsilon Used for the episode:  0.09023059638093574\n",
      "EPISODE COMPLETE\n",
      "min_reward: -175.0 || max_reward: 0.0 || total_reward: -38683.0 || average_reward: -107.45277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  350\n",
      "Epsilon Used for the episode:  0.08965501068203711\n",
      "EPISODE COMPLETE\n",
      "min_reward: -128.0 || max_reward: 0.0 || total_reward: -32458.0 || average_reward: -90.16111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  351\n",
      "Epsilon Used for the episode:  0.0890835543167309\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -36160.0 || average_reward: -100.44444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  352\n",
      "Epsilon Used for the episode:  0.08851619766059116\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -34133.0 || average_reward: -94.81388888888888\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  353\n",
      "Epsilon Used for the episode:  0.08795291130172184\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -35368.0 || average_reward: -98.24444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  354\n",
      "Epsilon Used for the episode:  0.08739366603923183\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -36071.0 || average_reward: -100.19722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  355\n",
      "Epsilon Used for the episode:  0.08683843288172156\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -34461.0 || average_reward: -95.725\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  356\n",
      "Epsilon Used for the episode:  0.08628718304577979\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -34699.0 || average_reward: -96.3861111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  357\n",
      "Epsilon Used for the episode:  0.0857398879544916\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -37006.0 || average_reward: -102.79444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  358\n",
      "Epsilon Used for the episode:  0.08519651923595681\n",
      "EPISODE COMPLETE\n",
      "min_reward: -164.0 || max_reward: 0.0 || total_reward: -36578.0 || average_reward: -101.60555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  359\n",
      "Epsilon Used for the episode:  0.08465704872181941\n",
      "EPISODE COMPLETE\n",
      "min_reward: -167.0 || max_reward: 0.0 || total_reward: -36417.0 || average_reward: -101.15833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  360\n",
      "Epsilon Used for the episode:  0.08412144844580714\n",
      "EPISODE COMPLETE\n",
      "min_reward: -130.0 || max_reward: 0.0 || total_reward: -30993.0 || average_reward: -86.09166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  361\n",
      "Epsilon Used for the episode:  0.08358969064228175\n",
      "EPISODE COMPLETE\n",
      "min_reward: -162.0 || max_reward: 0.0 || total_reward: -35795.0 || average_reward: -99.43055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  362\n",
      "Epsilon Used for the episode:  0.08306174774479962\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -34552.0 || average_reward: -95.97777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  363\n",
      "Epsilon Used for the episode:  0.08253759238468267\n",
      "EPISODE COMPLETE\n",
      "min_reward: -166.0 || max_reward: 0.0 || total_reward: -34732.0 || average_reward: -96.47777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  364\n",
      "Epsilon Used for the episode:  0.0820171973895997\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -32152.0 || average_reward: -89.31111111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  365\n",
      "Epsilon Used for the episode:  0.08150053578215756\n",
      "EPISODE COMPLETE\n",
      "min_reward: -165.0 || max_reward: 0.0 || total_reward: -33240.0 || average_reward: -92.33333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  366\n",
      "Epsilon Used for the episode:  0.08098758077850288\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -32113.0 || average_reward: -89.20277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  367\n",
      "Epsilon Used for the episode:  0.08047830578693341\n",
      "EPISODE COMPLETE\n",
      "min_reward: -153.0 || max_reward: 0.0 || total_reward: -34128.0 || average_reward: -94.8\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  368\n",
      "Epsilon Used for the episode:  0.07997268440651945\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -32917.0 || average_reward: -91.43611111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  369\n",
      "Epsilon Used for the episode:  0.07947069042573544\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -33784.0 || average_reward: -93.84444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  370\n",
      "Epsilon Used for the episode:  0.07897229782110102\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -31107.0 || average_reward: -86.40833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  371\n",
      "Epsilon Used for the episode:  0.07847748075583195\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -31958.0 || average_reward: -88.77222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  372\n",
      "Epsilon Used for the episode:  0.07798621357850073\n",
      "EPISODE COMPLETE\n",
      "min_reward: -161.0 || max_reward: 0.0 || total_reward: -32150.0 || average_reward: -89.30555555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  373\n",
      "Epsilon Used for the episode:  0.07749847082170687\n",
      "EPISODE COMPLETE\n",
      "min_reward: -135.0 || max_reward: 0.0 || total_reward: -31299.0 || average_reward: -86.94166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  374\n",
      "Epsilon Used for the episode:  0.07701422720075662\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -32325.0 || average_reward: -89.79166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  375\n",
      "Epsilon Used for the episode:  0.07653345761235225\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -30859.0 || average_reward: -85.71944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  376\n",
      "Epsilon Used for the episode:  0.07605613713329065\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -32676.0 || average_reward: -90.76666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  377\n",
      "Epsilon Used for the episode:  0.07558224101917123\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -31377.0 || average_reward: -87.15833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  378\n",
      "Epsilon Used for the episode:  0.07511174470311331\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -31873.0 || average_reward: -88.53611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  379\n",
      "Epsilon Used for the episode:  0.07464462379448256\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -31751.0 || average_reward: -88.19722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  380\n",
      "Epsilon Used for the episode:  0.0741808540776264\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -34236.0 || average_reward: -95.1\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  381\n",
      "Epsilon Used for the episode:  0.07372041151061891\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -38701.0 || average_reward: -107.50277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  382\n",
      "Epsilon Used for the episode:  0.07326327222401424\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -38331.0 || average_reward: -106.475\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  383\n",
      "Epsilon Used for the episode:  0.07280941251960941\n",
      "EPISODE COMPLETE\n",
      "min_reward: -163.0 || max_reward: 0.0 || total_reward: -37853.0 || average_reward: -105.14722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  384\n",
      "Epsilon Used for the episode:  0.07235880886921574\n",
      "EPISODE COMPLETE\n",
      "min_reward: -159.0 || max_reward: 0.0 || total_reward: -38302.0 || average_reward: -106.39444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  385\n",
      "Epsilon Used for the episode:  0.07191143791343906\n",
      "EPISODE COMPLETE\n",
      "min_reward: -152.0 || max_reward: 0.0 || total_reward: -36905.0 || average_reward: -102.51388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  386\n",
      "Epsilon Used for the episode:  0.0714672764604688\n",
      "EPISODE COMPLETE\n",
      "min_reward: -158.0 || max_reward: 0.0 || total_reward: -37527.0 || average_reward: -104.24166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  387\n",
      "Epsilon Used for the episode:  0.07102630148487579\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -38819.0 || average_reward: -107.83055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  388\n",
      "Epsilon Used for the episode:  0.07058849012641857\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -38091.0 || average_reward: -105.80833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  389\n",
      "Epsilon Used for the episode:  0.07015381968885824\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -36417.0 || average_reward: -101.15833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  390\n",
      "Epsilon Used for the episode:  0.06972226763878199\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -37793.0 || average_reward: -104.98055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  391\n",
      "Epsilon Used for the episode:  0.06929381160443487\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -37927.0 || average_reward: -105.35277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  392\n",
      "Epsilon Used for the episode:  0.06886842937456014\n",
      "EPISODE COMPLETE\n",
      "min_reward: -156.0 || max_reward: 0.0 || total_reward: -38482.0 || average_reward: -106.89444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  393\n",
      "Epsilon Used for the episode:  0.0684460988972477\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -37486.0 || average_reward: -104.12777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  394\n",
      "Epsilon Used for the episode:  0.06802679827879107\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -37957.0 || average_reward: -105.43611111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  395\n",
      "Epsilon Used for the episode:  0.06761050578255229\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -36142.0 || average_reward: -100.39444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  396\n",
      "Epsilon Used for the episode:  0.06719719982783506\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -36256.0 || average_reward: -100.71111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  397\n",
      "Epsilon Used for the episode:  0.06678685898876618\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -38706.0 || average_reward: -107.51666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  398\n",
      "Epsilon Used for the episode:  0.06637946199318466\n",
      "EPISODE COMPLETE\n",
      "min_reward: -157.0 || max_reward: 0.0 || total_reward: -38911.0 || average_reward: -108.08611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  399\n",
      "Epsilon Used for the episode:  0.06597498772153902\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -37846.0 || average_reward: -105.12777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  400\n",
      "Epsilon Used for the episode:  0.06557341520579238\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -36425.0 || average_reward: -101.18055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  401\n",
      "Epsilon Used for the episode:  0.06517472362833565\n",
      "EPISODE COMPLETE\n",
      "min_reward: -155.0 || max_reward: 0.0 || total_reward: -36541.0 || average_reward: -101.50277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  402\n",
      "Epsilon Used for the episode:  0.06477889232090814\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -35363.0 || average_reward: -98.23055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  403\n",
      "Epsilon Used for the episode:  0.06438590076352622\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -37331.0 || average_reward: -103.69722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  404\n",
      "Epsilon Used for the episode:  0.0639957285834196\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -33007.0 || average_reward: -91.68611111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  405\n",
      "Epsilon Used for the episode:  0.06360835555397502\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -31519.0 || average_reward: -87.55277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  406\n",
      "Epsilon Used for the episode:  0.0632237615936879\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -32372.0 || average_reward: -89.92222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  407\n",
      "Epsilon Used for the episode:  0.06284192676512122\n",
      "EPISODE COMPLETE\n",
      "min_reward: -151.0 || max_reward: 0.0 || total_reward: -32913.0 || average_reward: -91.425\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  408\n",
      "Epsilon Used for the episode:  0.06246283127387195\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -31469.0 || average_reward: -87.41388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  409\n",
      "Epsilon Used for the episode:  0.06208645546754494\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -33346.0 || average_reward: -92.62777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  410\n",
      "Epsilon Used for the episode:  0.061712779834734076\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -32149.0 || average_reward: -89.30277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  411\n",
      "Epsilon Used for the episode:  0.06134178500401087\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -31472.0 || average_reward: -87.42222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  412\n",
      "Epsilon Used for the episode:  0.06097345174292022\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -30976.0 || average_reward: -86.04444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  413\n",
      "Epsilon Used for the episode:  0.06060776095698342\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -30796.0 || average_reward: -85.54444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  414\n",
      "Epsilon Used for the episode:  0.06024469368870817\n",
      "EPISODE COMPLETE\n",
      "min_reward: -149.0 || max_reward: 0.0 || total_reward: -31919.0 || average_reward: -88.66388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  415\n",
      "Epsilon Used for the episode:  0.059884231116606006\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -31858.0 || average_reward: -88.49444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  416\n",
      "Epsilon Used for the episode:  0.059526354554216454\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -30671.0 || average_reward: -85.19722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  417\n",
      "Epsilon Used for the episode:  0.05917104544913838\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -30966.0 || average_reward: -86.01666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  418\n",
      "Epsilon Used for the episode:  0.05881828538206822\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -29978.0 || average_reward: -83.27222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  419\n",
      "Epsilon Used for the episode:  0.05846805606584506\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -30726.0 || average_reward: -85.35\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  420\n",
      "Epsilon Used for the episode:  0.058120339344502746\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -32047.0 || average_reward: -89.01944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  421\n",
      "Epsilon Used for the episode:  0.057775117192328564\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -29960.0 || average_reward: -83.22222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  422\n",
      "Epsilon Used for the episode:  0.05743237171292882\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -30527.0 || average_reward: -84.79722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  423\n",
      "Epsilon Used for the episode:  0.05709208513830114\n",
      "EPISODE COMPLETE\n",
      "min_reward: -129.0 || max_reward: 0.0 || total_reward: -30283.0 || average_reward: -84.11944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  424\n",
      "Epsilon Used for the episode:  0.05675423982791324\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -31089.0 || average_reward: -86.35833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  425\n",
      "Epsilon Used for the episode:  0.0564188182677886\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -31842.0 || average_reward: -88.45\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  426\n",
      "Epsilon Used for the episode:  0.056085803069598414\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -31590.0 || average_reward: -87.75\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  427\n",
      "Epsilon Used for the episode:  0.05575517696976026\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -31843.0 || average_reward: -88.45277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  428\n",
      "Epsilon Used for the episode:  0.05542692282854303\n",
      "EPISODE COMPLETE\n",
      "min_reward: -133.0 || max_reward: 0.0 || total_reward: -31537.0 || average_reward: -87.60277777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  429\n",
      "Epsilon Used for the episode:  0.05510102362917855\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -31908.0 || average_reward: -88.63333333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  430\n",
      "Epsilon Used for the episode:  0.05477746247697935\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -33547.0 || average_reward: -93.18611111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  431\n",
      "Epsilon Used for the episode:  0.054456222598462826\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -34221.0 || average_reward: -95.05833333333334\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  432\n",
      "Epsilon Used for the episode:  0.05413728734048177\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -33148.0 || average_reward: -92.07777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  433\n",
      "Epsilon Used for the episode:  0.05382064016936093\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -33814.0 || average_reward: -93.92777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  434\n",
      "Epsilon Used for the episode:  0.05350626467004006\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -33015.0 || average_reward: -91.70833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  435\n",
      "Epsilon Used for the episode:  0.053194144545222886\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -33171.0 || average_reward: -92.14166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  436\n",
      "Epsilon Used for the episode:  0.052884263614532225\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -32356.0 || average_reward: -89.87777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  437\n",
      "Epsilon Used for the episode:  0.05257660581367126\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -33942.0 || average_reward: -94.28333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  438\n",
      "Epsilon Used for the episode:  0.05227115519359065\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -31899.0 || average_reward: -88.60833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  439\n",
      "Epsilon Used for the episode:  0.051967895919661874\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -32634.0 || average_reward: -90.65\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  440\n",
      "Epsilon Used for the episode:  0.05166681227085625\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -33199.0 || average_reward: -92.21944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  441\n",
      "Epsilon Used for the episode:  0.05136788863893\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -31782.0 || average_reward: -88.28333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  442\n",
      "Epsilon Used for the episode:  0.05107110952761509\n",
      "EPISODE COMPLETE\n",
      "min_reward: -130.0 || max_reward: 0.0 || total_reward: -30054.0 || average_reward: -83.48333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  443\n",
      "Epsilon Used for the episode:  0.05077645955181593\n",
      "EPISODE COMPLETE\n",
      "min_reward: -146.0 || max_reward: 0.0 || total_reward: -30208.0 || average_reward: -83.91111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  444\n",
      "Epsilon Used for the episode:  0.05048392343681179\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -30363.0 || average_reward: -84.34166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  445\n",
      "Epsilon Used for the episode:  0.050193486017464956\n",
      "EPISODE COMPLETE\n",
      "min_reward: -135.0 || max_reward: 0.0 || total_reward: -30647.0 || average_reward: -85.13055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  446\n",
      "Epsilon Used for the episode:  0.04990513223743459\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -30854.0 || average_reward: -85.70555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  447\n",
      "Epsilon Used for the episode:  0.04961884714839611\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -29169.0 || average_reward: -81.025\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  448\n",
      "Epsilon Used for the episode:  0.04933461590926641\n",
      "EPISODE COMPLETE\n",
      "min_reward: -131.0 || max_reward: 0.0 || total_reward: -29109.0 || average_reward: -80.85833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  449\n",
      "Epsilon Used for the episode:  0.0490524237854344\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -30101.0 || average_reward: -83.6138888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  450\n",
      "Epsilon Used for the episode:  0.0487722561479972\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -28385.0 || average_reward: -78.84722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  451\n",
      "Epsilon Used for the episode:  0.048494098473001734\n",
      "EPISODE COMPLETE\n",
      "min_reward: -130.0 || max_reward: 0.0 || total_reward: -28839.0 || average_reward: -80.10833333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  452\n",
      "Epsilon Used for the episode:  0.04821793634069181\n",
      "EPISODE COMPLETE\n",
      "min_reward: -130.0 || max_reward: 0.0 || total_reward: -28296.0 || average_reward: -78.6\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  453\n",
      "Epsilon Used for the episode:  0.04794375543476067\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -28476.0 || average_reward: -79.1\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  454\n",
      "Epsilon Used for the episode:  0.047671541541608735\n",
      "EPISODE COMPLETE\n",
      "min_reward: -132.0 || max_reward: 0.0 || total_reward: -28426.0 || average_reward: -78.96111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  455\n",
      "Epsilon Used for the episode:  0.04740128054960685\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -29554.0 || average_reward: -82.09444444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  456\n",
      "Epsilon Used for the episode:  0.04713295844836463\n",
      "EPISODE COMPLETE\n",
      "min_reward: -127.0 || max_reward: 0.0 || total_reward: -28184.0 || average_reward: -78.28888888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  457\n",
      "Epsilon Used for the episode:  0.046866561328004264\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -28684.0 || average_reward: -79.67777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  458\n",
      "Epsilon Used for the episode:  0.04660207537843938\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -28639.0 || average_reward: -79.55277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  459\n",
      "Epsilon Used for the episode:  0.04633948688865911\n",
      "EPISODE COMPLETE\n",
      "min_reward: -135.0 || max_reward: 0.0 || total_reward: -27781.0 || average_reward: -77.16944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  460\n",
      "Epsilon Used for the episode:  0.04607878224601737\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -29345.0 || average_reward: -81.51388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  461\n",
      "Epsilon Used for the episode:  0.045819947935527065\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -30319.0 || average_reward: -84.21944444444445\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  462\n",
      "Epsilon Used for the episode:  0.04556297053915958\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -29771.0 || average_reward: -82.69722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  463\n",
      "Epsilon Used for the episode:  0.045307836735149154\n",
      "EPISODE COMPLETE\n",
      "min_reward: -130.0 || max_reward: 0.0 || total_reward: -28755.0 || average_reward: -79.875\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  464\n",
      "Epsilon Used for the episode:  0.045054533297302225\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -29293.0 || average_reward: -81.36944444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  465\n",
      "Epsilon Used for the episode:  0.04480304709431188\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -29933.0 || average_reward: -83.14722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  466\n",
      "Epsilon Used for the episode:  0.044553365089077014\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -30100.0 || average_reward: -83.61111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  467\n",
      "Epsilon Used for the episode:  0.044305474338026556\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -30371.0 || average_reward: -84.3638888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  468\n",
      "Epsilon Used for the episode:  0.044059361990448465\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -29864.0 || average_reward: -82.95555555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  469\n",
      "Epsilon Used for the episode:  0.04381501528782354\n",
      "EPISODE COMPLETE\n",
      "min_reward: -134.0 || max_reward: 0.0 || total_reward: -29968.0 || average_reward: -83.24444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  470\n",
      "Epsilon Used for the episode:  0.04357242156316397\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -30630.0 || average_reward: -85.08333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  471\n",
      "Epsilon Used for the episode:  0.04333156824035675\n",
      "EPISODE COMPLETE\n",
      "min_reward: -136.0 || max_reward: 0.0 || total_reward: -31162.0 || average_reward: -86.56111111111112\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  472\n",
      "Epsilon Used for the episode:  0.043092442833511685\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -32836.0 || average_reward: -91.21111111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  473\n",
      "Epsilon Used for the episode:  0.04285503294631413\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -31852.0 || average_reward: -88.47777777777777\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  474\n",
      "Epsilon Used for the episode:  0.042619326271382395\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -32536.0 || average_reward: -90.37777777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  475\n",
      "Epsilon Used for the episode:  0.042385310589629625\n",
      "EPISODE COMPLETE\n",
      "min_reward: -137.0 || max_reward: 0.0 || total_reward: -31949.0 || average_reward: -88.74722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  476\n",
      "Epsilon Used for the episode:  0.042152973769630474\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -32174.0 || average_reward: -89.37222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  477\n",
      "Epsilon Used for the episode:  0.041922303766992186\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -32480.0 || average_reward: -90.22222222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  478\n",
      "Epsilon Used for the episode:  0.04169328862373014\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -33801.0 || average_reward: -93.89166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  479\n",
      "Epsilon Used for the episode:  0.04146591646764806\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -33035.0 || average_reward: -91.76388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  480\n",
      "Epsilon Used for the episode:  0.041240175511722404\n",
      "EPISODE COMPLETE\n",
      "min_reward: -144.0 || max_reward: 0.0 || total_reward: -32599.0 || average_reward: -90.55277777777778\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  481\n",
      "Epsilon Used for the episode:  0.0410160540534915\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -32650.0 || average_reward: -90.69444444444444\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  482\n",
      "Epsilon Used for the episode:  0.04079354047444874\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -31457.0 || average_reward: -87.38055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  483\n",
      "Epsilon Used for the episode:  0.04057262323944038\n",
      "EPISODE COMPLETE\n",
      "min_reward: -142.0 || max_reward: 0.0 || total_reward: -31710.0 || average_reward: -88.08333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  484\n",
      "Epsilon Used for the episode:  0.040353290896067456\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -32664.0 || average_reward: -90.73333333333333\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  485\n",
      "Epsilon Used for the episode:  0.04013553207409217\n",
      "EPISODE COMPLETE\n",
      "min_reward: -138.0 || max_reward: 0.0 || total_reward: -31767.0 || average_reward: -88.24166666666666\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  486\n",
      "Epsilon Used for the episode:  0.039919335484848446\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -31371.0 || average_reward: -87.14166666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  487\n",
      "Epsilon Used for the episode:  0.039704689920656656\n",
      "EPISODE COMPLETE\n",
      "min_reward: -154.0 || max_reward: 0.0 || total_reward: -32742.0 || average_reward: -90.95\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  488\n",
      "Epsilon Used for the episode:  0.03949158425424271\n",
      "EPISODE COMPLETE\n",
      "min_reward: -145.0 || max_reward: 0.0 || total_reward: -33126.0 || average_reward: -92.01666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  489\n",
      "Epsilon Used for the episode:  0.03928000743816112\n",
      "EPISODE COMPLETE\n",
      "min_reward: -148.0 || max_reward: 0.0 || total_reward: -33205.0 || average_reward: -92.23611111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  490\n",
      "Epsilon Used for the episode:  0.03906994850422234\n",
      "EPISODE COMPLETE\n",
      "min_reward: -150.0 || max_reward: 0.0 || total_reward: -32249.0 || average_reward: -89.58055555555555\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  491\n",
      "Epsilon Used for the episode:  0.038861396562924214\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -30686.0 || average_reward: -85.2388888888889\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  492\n",
      "Epsilon Used for the episode:  0.038654340802887405\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -31193.0 || average_reward: -86.64722222222223\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  493\n",
      "Epsilon Used for the episode:  0.03844877049029494\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -31385.0 || average_reward: -87.18055555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  494\n",
      "Epsilon Used for the episode:  0.03824467496833576\n",
      "EPISODE COMPLETE\n",
      "min_reward: -140.0 || max_reward: 0.0 || total_reward: -30437.0 || average_reward: -84.54722222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  495\n",
      "Epsilon Used for the episode:  0.03804204365665231\n",
      "EPISODE COMPLETE\n",
      "min_reward: -139.0 || max_reward: 0.0 || total_reward: -30498.0 || average_reward: -84.71666666666667\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  496\n",
      "Epsilon Used for the episode:  0.03784086605079202\n",
      "EPISODE COMPLETE\n",
      "min_reward: -143.0 || max_reward: 0.0 || total_reward: -31202.0 || average_reward: -86.67222222222222\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  497\n",
      "Epsilon Used for the episode:  0.037641131721662745\n",
      "EPISODE COMPLETE\n",
      "min_reward: -147.0 || max_reward: 0.0 || total_reward: -30620.0 || average_reward: -85.05555555555556\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  498\n",
      "Epsilon Used for the episode:  0.03744283031499212\n",
      "EPISODE COMPLETE\n",
      "min_reward: -135.0 || max_reward: 0.0 || total_reward: -30559.0 || average_reward: -84.8861111111111\n",
      "###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-###-\n",
      "Episode:  499\n",
      "Epsilon Used for the episode:  0.03724595155079083\n",
      "EPISODE COMPLETE\n",
      "min_reward: -141.0 || max_reward: 0.0 || total_reward: -31413.0 || average_reward: -87.25833333333334\n"
     ]
    }
   ],
   "source": [
    "episodeRewards = [] # to store the episode rewards for logging/plotting purposes\n",
    "stepsDone = 0\n",
    "# creating a loop for training\n",
    "for ep in range(PARAM_episodes):\n",
    "    print('###-'* 30)\n",
    "    print(\"Episode: \", ep)\n",
    "    print(\"Epsilon Used for the episode: \", INUSE_epsilon)\n",
    "    # reset the environment\n",
    "    state = env.reset(0)\n",
    "    epReward = []\n",
    "    done = False\n",
    "\n",
    "    # training in the episode\n",
    "    while not done:\n",
    "        # if stepsDone%250==0:\n",
    "        #     print(\"250 Steps done!, \", stepsDone)\n",
    "        # recording the previous action\n",
    "        # recording previous action the previous TL phase\n",
    "        # print(\"stepsDone: \", stepsDone)\n",
    "        if stepsDone==0:\n",
    "            prevAction = 0\n",
    "        else:\n",
    "            prevAction = action\n",
    "        # print(\"Prev action: \", prevAction)\n",
    "        # select an action and perform it\n",
    "        action = selectAction(state, INUSE_epsilon)\n",
    "        stepsDone+=1\n",
    "        # print(\"ACTION SELECTED: \", action)\n",
    "        nextSt, r, done = env.take_action(action = action)\n",
    "        # print(\"nextSt: \", nextSt)\n",
    "        # debug\n",
    "        # if env.numSteps == 10800:\n",
    "        #     print(\"\\n!!!10800 steps done!!!\\n\")\n",
    "\n",
    "        # store current state, action, reward, next state, done in replay memory\n",
    "        memory.insert(state, action, r, nextSt, done)\n",
    "\n",
    "        # update current state to next state\n",
    "        state = nextSt\n",
    "        epReward.append(r)\n",
    "\n",
    "        # update model parameters\n",
    "        # if memory does not have enough tuples to sample (batch_size), we cant sample so we continue the loop\n",
    "        # print('Memory size: ', memory.size())\n",
    "        if memory.size() < PARAM_batch_size:\n",
    "            continue\n",
    "        # else, we train the networks\n",
    "        # take a sample\n",
    "        stBatch, aBatch, rBatch, nstBatch, doneBatch = memory.sample(PARAM_batch_size)\n",
    "        stBatch = torch.tensor(stBatch, device=PARAM_device, dtype=torch.float32)\n",
    "        aBatch = torch.tensor(aBatch, device=PARAM_device).unsqueeze(1)\n",
    "        rBatch = torch.tensor(rBatch, device=PARAM_device, dtype=torch.float32)\n",
    "        nstBatch = torch.tensor(nstBatch, device=PARAM_device, dtype=torch.float32)\n",
    "        # getting Q values for current states\n",
    "        QVals = policyNet(stBatch).gather(1, aBatch)\n",
    "\n",
    "        # Updating q values using target network\n",
    "        with torch.no_grad():\n",
    "            # next state q values using target network\n",
    "            nextQVals = targetNet(nstBatch).max(1)[0]\n",
    "            # updating the q values for policy network\n",
    "            targetQVals = rBatch + PARAM_gamma * nextQVals\n",
    "\n",
    "        # calculating the loss\n",
    "        lossFunction = nn.SmoothL1Loss()\n",
    "        loss = lossFunction(QVals, targetQVals.unsqueeze(1))\n",
    "\n",
    "        # updating model weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # updaing the target network periodically\n",
    "        if ep%PARAM_target_update_freq==0:\n",
    "            # targetNetStateDict = targetNet.state_dict()\n",
    "            # policyNetStateDict = policyNet.state_dict()\n",
    "            # for key in policyNetStateDict:\n",
    "            #     targetNetStateDict[key] = policyNetStateDict*PARAM_tau\n",
    "            targetNet.load_state_dict(policyNet.state_dict())\n",
    "        \n",
    "\n",
    "    # decay epsilon after episode is complete\n",
    "    INUSE_epsilon = PARAM_epsilon_min + (PARAM_epsilon - PARAM_epsilon_min) * math.exp(-1. * stepsDone/PARAM_epsilon_decay)\n",
    "    episodeRewards.append(epReward)\n",
    "\n",
    "    # printing training stats\n",
    "    print(\"EPISODE COMPLETE\")\n",
    "    print(f\"min_reward: {min(epReward)} || max_reward: {max(epReward)} || total_reward: {sum(epReward)} || average_reward: {np.mean(epReward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b33793f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the models\n",
    "# need to save everything\n",
    "import pickle\n",
    "# saving the models\n",
    "torch.save(targetNet, \"models/TargetNetv4.pth\")\n",
    "torch.save(policyNet, \"models/PolicyNetv4.pth\")\n",
    "\n",
    "# saving the lists\n",
    "with open('models/rewardListv4.pkl', 'wb') as f:\n",
    "    pickle.dump(episodeRewards, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a84961",
   "metadata": {},
   "source": [
    "# Debugging Shenanigans\n",
    "\n",
    "NOTE TO SELF: Before re-running this tomorrow, check that all the shapes are consistent, and that the gather function is working correctly, only then, run it. I have fixed some tonight, such as:\n",
    "- current phase for the next state is set to be equal to the action, instead of previous phase (which I had done by mistake) [DONE]\n",
    "- Gather wasnt giving correct Q Values due to shape issues, that should have been fixed, if not, refer the cell below for the correct aBatch shape and the correct gather parameters. [DONE]\n",
    "- Make sure that for the loss function, the QVals and TargetQVals have the same shape, it could be 64,1 or 1,64 or whatever, its fine as long as their shape is the same. [DONE]\n",
    "- Try using SmoothL1Loss instead of MSE. Apparantly it is supposed to be more stable. [DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c248966",
   "metadata": {},
   "outputs": [],
   "source": [
    "stBatch, aBatch, rBatch, nstBatch, doneBatch = memory.sample(PARAM_batch_size)\n",
    "stBatch = torch.tensor(stBatch, device=PARAM_device, dtype=torch.float32)\n",
    "aBatch = torch.tensor(aBatch, device=PARAM_device).unsqueeze(1)\n",
    "rBatch = torch.tensor(rBatch, device=PARAM_device, dtype=torch.float32)\n",
    "nstBatch = torch.tensor(nstBatch, device=PARAM_device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd12d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aBatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68dd7597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [0],\n",
       "        [1],\n",
       "        [3],\n",
       "        [0],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74d0b5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = policyNet(stBatch)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "577824b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -772.4964],\n",
       "        [-2856.6091],\n",
       "        [ -612.6323],\n",
       "        [-3285.7737],\n",
       "        [ -479.3011],\n",
       "        [-3018.7864],\n",
       "        [-3210.8848],\n",
       "        [-1734.7593],\n",
       "        [ -377.2107],\n",
       "        [ -663.1656],\n",
       "        [ -482.6180],\n",
       "        [ -695.5036],\n",
       "        [-3102.1565],\n",
       "        [-3638.4099],\n",
       "        [-2906.9006],\n",
       "        [-3294.7976],\n",
       "        [-2865.6377],\n",
       "        [-3124.0583],\n",
       "        [ -717.2283],\n",
       "        [-2441.7175],\n",
       "        [ -467.0646],\n",
       "        [-2965.3010],\n",
       "        [-3162.9597],\n",
       "        [ -719.0084],\n",
       "        [-3320.7771],\n",
       "        [-2837.5127],\n",
       "        [-2687.4963],\n",
       "        [-3239.9607],\n",
       "        [-3052.2825],\n",
       "        [ -512.8585],\n",
       "        [-3113.2363],\n",
       "        [-3402.0400],\n",
       "        [-3154.1001],\n",
       "        [ -769.5070],\n",
       "        [-3139.3794],\n",
       "        [-3266.2278],\n",
       "        [-2491.9050],\n",
       "        [ -810.2480],\n",
       "        [-3139.6357],\n",
       "        [ -629.4410],\n",
       "        [-2956.2581],\n",
       "        [-3432.9868],\n",
       "        [ -551.6158],\n",
       "        [-2873.3030],\n",
       "        [-3286.9487],\n",
       "        [-2821.6582],\n",
       "        [-3111.5769],\n",
       "        [-3166.7612],\n",
       "        [-3130.9851],\n",
       "        [-2930.7598],\n",
       "        [-3205.4080],\n",
       "        [ -519.3462],\n",
       "        [-3158.7639],\n",
       "        [-1511.8700],\n",
       "        [ -532.6009],\n",
       "        [-2930.3855],\n",
       "        [-2920.9062],\n",
       "        [-3243.4697],\n",
       "        [-2955.8479],\n",
       "        [-3193.4163],\n",
       "        [-2937.0173],\n",
       "        [ -503.7598],\n",
       "        [-3136.3489],\n",
       "        [-3099.8252]], device='cuda:0', grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qv = q.gather(1, aBatch)\n",
    "# qv.shape\n",
    "qv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3754ff36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -739.4864,  -762.3033,  -769.9965,  -772.4964],\n",
       "        [-2856.6091, -2933.9246, -3021.2268, -3030.1731],\n",
       "        [ -595.3767,  -612.6323,  -621.8085,  -626.0457],\n",
       "        [-3197.9443, -3285.7737, -3380.2305, -3391.0081],\n",
       "        [ -464.1333,  -480.8954,  -479.3011,  -482.9185],\n",
       "        [-3018.7864, -3103.8247, -3185.9072, -3198.1831],\n",
       "        [-3125.9009, -3210.8848, -3305.6948, -3314.5444],\n",
       "        [-1656.0590, -1706.2531, -1734.7593, -1747.5022],\n",
       "        [ -377.2107,  -390.0788,  -392.0471,  -394.1255],\n",
       "        [ -638.7922,  -662.3054,  -657.1705,  -663.1656],\n",
       "        [ -463.7555,  -479.2653,  -482.6180,  -486.1859],\n",
       "        [ -665.9645,  -685.6186,  -695.5036,  -701.2598],\n",
       "        [-2935.6384, -3014.3857, -3102.1565, -3113.1453],\n",
       "        [-3430.6130, -3523.7234, -3628.7434, -3638.4099],\n",
       "        [-2747.3538, -2825.8904, -2894.1951, -2906.9006],\n",
       "        [-3206.4062, -3294.7976, -3387.5488, -3399.2417],\n",
       "        [-2865.6377, -2948.1169, -3015.4456, -3031.7034],\n",
       "        [-3041.5107, -3124.0583, -3212.8853, -3221.4011],\n",
       "        [ -686.3755,  -711.1260,  -708.8452,  -717.2283],\n",
       "        [-2316.4771, -2385.0813, -2441.7175, -2449.9480],\n",
       "        [ -451.1576,  -467.0646,  -467.4834,  -468.7238],\n",
       "        [-2885.3650, -2965.3010, -3051.8865, -3059.7959],\n",
       "        [-2996.8311, -3078.2190, -3162.9597, -3172.7168],\n",
       "        [ -682.6020,  -702.1351,  -714.4927,  -719.0084],\n",
       "        [-3133.2200, -3220.1975, -3308.5684, -3320.7771],\n",
       "        [-2837.5127, -2916.8208, -2999.1060, -3006.5415],\n",
       "        [-2611.7866, -2687.4963, -2759.6763, -2767.4187],\n",
       "        [-3054.3579, -3136.2432, -3230.4021, -3239.9607],\n",
       "        [-3052.2825, -3135.2883, -3229.1284, -3237.8186],\n",
       "        [ -492.0651,  -507.3953,  -512.8585,  -516.4587],\n",
       "        [-2944.3660, -3025.2913, -3113.2363, -3122.2673],\n",
       "        [-3311.6553, -3402.0400, -3501.7104, -3512.8162],\n",
       "        [-2990.2476, -3076.1682, -3154.1001, -3166.1794],\n",
       "        [ -769.5070,  -798.4885,  -790.3619,  -799.8730],\n",
       "        [-3056.4690, -3139.3794, -3232.6274, -3241.5718],\n",
       "        [-3081.2585, -3166.9724, -3256.3081, -3266.2278],\n",
       "        [-2366.3369, -2438.3965, -2491.9050, -2503.5667],\n",
       "        [ -783.5356,  -810.2480,  -812.2254,  -821.4691],\n",
       "        [-2970.1187, -3050.7263, -3139.6357, -3147.7329],\n",
       "        [ -600.1216,  -618.4197,  -625.4775,  -629.4410],\n",
       "        [-2875.5691, -2956.2581, -3030.0205, -3045.3826],\n",
       "        [-3246.9624, -3336.5093, -3432.9868, -3441.9924],\n",
       "        [ -551.6158,  -571.3687,  -570.2324,  -574.5465],\n",
       "        [-2873.3030, -2951.6638, -3033.3914, -3046.3391],\n",
       "        [-3199.9880, -3286.9487, -3384.6802, -3394.8977],\n",
       "        [-2821.6582, -2902.3196, -2972.3950, -2986.6912],\n",
       "        [-3028.2905, -3111.5769, -3202.1943, -3211.4011],\n",
       "        [-2995.8943, -3075.9551, -3166.7612, -3175.5669],\n",
       "        [-3047.7822, -3130.9851, -3221.0967, -3230.0986],\n",
       "        [-2930.7598, -3009.7297, -3099.6772, -3109.5369],\n",
       "        [-3028.1069, -3114.0535, -3189.7676, -3205.4080],\n",
       "        [ -501.3587,  -518.7578,  -519.3462,  -524.2563],\n",
       "        [-2978.7795, -3059.0764, -3148.4319, -3158.7639],\n",
       "        [-1468.6910, -1511.8700, -1547.6128, -1553.7456],\n",
       "        [ -516.6721,  -532.6009,  -538.1233,  -541.9800],\n",
       "        [-2765.7651, -2840.8240, -2922.3750, -2930.3855],\n",
       "        [-2840.8711, -2920.9062, -3000.9438, -3009.4641],\n",
       "        [-3060.6582, -3142.7122, -3234.6362, -3243.4697],\n",
       "        [-2878.1387, -2955.8479, -3044.5027, -3053.0457],\n",
       "        [-3013.2974, -3096.0935, -3185.4050, -3193.4163],\n",
       "        [-2771.8594, -2848.2368, -2929.0925, -2937.0173],\n",
       "        [ -487.8611,  -503.7598,  -505.9855,  -510.2795],\n",
       "        [-3048.5979, -3136.3489, -3204.7212, -3222.9927],\n",
       "        [-2932.1174, -3010.2095, -3099.8252, -3109.5479]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q\n",
    "# qv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51ecdf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1]), torch.Size([64, 1]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QVals.shape, targetQVals.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "044a123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QVals.shape, targetQVals.unsqueeze(1).shape\n",
    "QValsR = QVals.reshape(64,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(env._getState(0), dtype=torch.float32, device=PARAM_device)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16f73cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    q = policyNet(t)\n",
    "    a = torch.argmax(q).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "052185d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor([2542.5510, 2600.9631, 2471.1885, 2534.9917], device='cuda:0'))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3d17d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2542.5510, 2600.9631, 2471.1885, 2534.9917], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyNet(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d846093d",
   "metadata": {},
   "source": [
    "# Trying to make predictions\n",
    "\n",
    "After making the changes *hopefully (fingers crossed)* it works. Please GOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944f16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PressureEnv(maxSteps=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca6a61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (layer1): Linear(in_features=13, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT RUN\n",
    "# policyNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dd0ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "policyNet = torch.load('models/PolicyNetv6.pth', weights_only=False)\n",
    "targetNet = torch.load('models/TargetNetv6.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c264e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0\n",
      "QVals:  tensor([[-15.5755, -15.8600, -18.2971, -19.3286]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  1\n",
      "QVals:  tensor([[-19.9135, -21.5175, -21.2117, -19.0056]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  2\n",
      "QVals:  tensor([[-24.9992, -24.9981, -24.7322, -38.1167]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  3\n",
      "QVals:  tensor([[-27.3267, -26.1677, -28.5961, -29.1412]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  4\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  5\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  6\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  7\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  8\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  9\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  10\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  11\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  12\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  13\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  14\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  15\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  16\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  17\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  18\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  19\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  20\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  21\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  22\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  23\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  24\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  25\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  26\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  27\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  28\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  29\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  30\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  31\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  32\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  33\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  34\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  35\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  36\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  37\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  38\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  39\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  40\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  41\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  42\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  43\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  44\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  45\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  46\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  47\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  48\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  49\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  50\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  51\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  52\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  53\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  54\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  55\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  56\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  57\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  58\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  59\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  60\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  61\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  62\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  63\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  64\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  65\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  66\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  67\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  68\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  69\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  70\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  71\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  72\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  73\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  74\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  75\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  76\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  77\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  78\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  79\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  80\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  81\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  82\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  83\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  84\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  85\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  86\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  87\n",
      "QVals:  tensor([[-28.1956, -25.8882, -29.7186, -30.2695]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  88\n",
      "QVals:  tensor([[-27.5502, -32.4305, -29.5893, -27.7698]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  89\n",
      "QVals:  tensor([[-28.5513, -28.1850, -26.3112, -25.9624]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  90\n",
      "QVals:  tensor([[-28.6608, -26.8911, -25.6776, -32.4396]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  91\n",
      "QVals:  tensor([[-102.6175, -100.6018, -129.1811, -115.3640]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  92\n",
      "QVals:  tensor([[-118.3834, -132.1897, -156.9338, -143.0723]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  93\n",
      "QVals:  tensor([[-138.0428, -136.0172, -151.6028, -150.5701]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  94\n",
      "QVals:  tensor([[-151.4878, -162.1543, -180.3858, -175.1568]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  95\n",
      "QVals:  tensor([[-177.9450, -178.1703, -188.9614, -190.4738]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  96\n",
      "QVals:  tensor([[-202.5176, -177.2559, -198.2093, -186.1521]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  97\n",
      "QVals:  tensor([[-220.2344, -214.9074, -224.3995, -226.3662]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  98\n",
      "QVals:  tensor([[-213.1559, -219.8599, -223.0528, -224.2442]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  99\n",
      "QVals:  tensor([[-240.1357, -245.5916, -249.7007, -256.3856]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  100\n",
      "QVals:  tensor([[-254.5874, -246.3279, -254.9663, -255.3674]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  101\n",
      "QVals:  tensor([[-277.8184, -274.8143, -279.2525, -283.6736]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  102\n",
      "QVals:  tensor([[-262.0474, -270.7622, -271.9022, -270.8847]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  103\n",
      "QVals:  tensor([[-304.6885, -306.7164, -309.0962, -314.2502]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  104\n",
      "QVals:  tensor([[-310.4399, -299.5740, -307.8323, -311.4677]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  105\n",
      "QVals:  tensor([[-326.0395, -335.8294, -338.2473, -335.5228]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  106\n",
      "QVals:  tensor([[-338.9928, -329.1496, -337.4153, -341.2147]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  107\n",
      "QVals:  tensor([[-359.2981, -370.9486, -373.1478, -369.2543]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  108\n",
      "QVals:  tensor([[-368.0777, -360.1497, -366.8991, -370.1098]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  109\n",
      "QVals:  tensor([[-386.1881, -397.8395, -399.2946, -395.2430]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  110\n",
      "QVals:  tensor([[-389.1226, -384.0710, -389.8596, -390.6609]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  111\n",
      "QVals:  tensor([[-408.2970, -419.9051, -420.5842, -416.0765]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  112\n",
      "QVals:  tensor([[-417.7524, -414.2019, -418.9675, -418.7538]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  113\n",
      "QVals:  tensor([[-435.1472, -446.6955, -446.5940, -441.8014]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  114\n",
      "QVals:  tensor([[-440.7195, -438.2332, -441.1484, -440.5980]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  115\n",
      "QVals:  tensor([[-461.1689, -471.8227, -470.6334, -466.4023]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  116\n",
      "QVals:  tensor([[-466.7877, -464.4349, -465.5981, -465.5190]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  117\n",
      "QVals:  tensor([[-486.8968, -497.5277, -494.7346, -491.7839]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  118\n",
      "QVals:  tensor([[-493.0586, -491.6489, -490.9162, -492.2910]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  119\n",
      "QVals:  tensor([[-516.3016, -512.8713, -515.9187, -516.1570]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  120\n",
      "QVals:  tensor([[-503.7642, -515.1042, -514.3683, -509.0163]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  121\n",
      "QVals:  tensor([[-523.2883, -529.7695, -528.5739, -528.1572]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  122\n",
      "QVals:  tensor([[-531.9252, -526.4102, -529.2900, -527.8809]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  123\n",
      "QVals:  tensor([[-536.9929, -546.1993, -550.8135, -542.2645]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  124\n",
      "QVals:  tensor([[-537.9910, -533.1736, -537.4831, -534.5827]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  125\n",
      "QVals:  tensor([[-549.9543, -553.9929, -558.3797, -552.2130]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  126\n",
      "QVals:  tensor([[-556.8120, -549.4390, -552.7289, -552.2239]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  127\n",
      "QVals:  tensor([[-566.8333, -573.1299, -574.6354, -569.9331]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  128\n",
      "QVals:  tensor([[-574.5479, -568.8212, -569.9180, -570.1330]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  129\n",
      "QVals:  tensor([[-593.6734, -601.0283, -599.7252, -596.4517]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  130\n",
      "QVals:  tensor([[-602.4939, -597.3959, -597.1249, -598.0040]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  131\n",
      "QVals:  tensor([[-617.1154, -617.6897, -620.3503, -619.1304]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  132\n",
      "QVals:  tensor([[-625.2921, -616.4470, -618.9385, -619.0093]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  133\n",
      "QVals:  tensor([[-635.1187, -644.9896, -645.7058, -638.8347]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  134\n",
      "QVals:  tensor([[-646.5905, -640.3525, -641.6685, -642.9895]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  135\n",
      "QVals:  tensor([[-657.7474, -667.8599, -667.9285, -661.1011]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  136\n",
      "QVals:  tensor([[-655.5835, -651.6902, -652.0510, -653.1924]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  137\n",
      "QVals:  tensor([[-663.8712, -671.7499, -669.0370, -663.4164]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  138\n",
      "QVals:  tensor([[-651.8845, -677.0076, -667.5083, -661.5602]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  139\n",
      "QVals:  tensor([[-681.1049, -682.2656, -679.1300, -681.8516]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  140\n",
      "QVals:  tensor([[-681.0674, -686.2721, -687.6995, -682.1149]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  141\n",
      "QVals:  tensor([[-702.5430, -702.7552, -703.0284, -703.6949]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  142\n",
      "QVals:  tensor([[-721.1201, -706.2098, -712.2540, -710.6128]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  143\n",
      "QVals:  tensor([[-710.4648, -720.0286, -721.8022, -711.9324]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  144\n",
      "QVals:  tensor([[-711.8014, -696.5274, -701.9707, -700.7288]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  145\n",
      "QVals:  tensor([[-695.6245, -707.4794, -706.0814, -697.6340]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  146\n",
      "QVals:  tensor([[-684.1631, -681.1732, -681.9505, -682.0726]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  147\n",
      "QVals:  tensor([[-687.2571, -698.0696, -692.8918, -686.7579]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  148\n",
      "QVals:  tensor([[-680.4958, -704.6977, -693.5199, -688.0924]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  149\n",
      "QVals:  tensor([[-704.5636, -708.6094, -704.0303, -706.5502]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  150\n",
      "QVals:  tensor([[-708.0211, -713.7800, -713.7487, -708.4807]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  151\n",
      "QVals:  tensor([[-723.4257, -725.2104, -724.9289, -725.1730]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  152\n",
      "QVals:  tensor([[-736.0868, -722.8940, -728.3979, -727.0722]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  153\n",
      "QVals:  tensor([[-724.5323, -735.7814, -736.4799, -726.5814]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  154\n",
      "QVals:  tensor([[-717.7371, -711.5212, -713.6937, -714.7963]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  155\n",
      "QVals:  tensor([[-709.5430, -723.9442, -720.7000, -712.9926]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  156\n",
      "QVals:  tensor([[-695.8843, -696.7725, -695.6692, -694.7114]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  157\n",
      "QVals:  tensor([[-698.4395, -725.3616, -714.2319, -708.2922]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  158\n",
      "QVals:  tensor([[-708.7971, -710.3382, -707.7582, -709.8400]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  159\n",
      "QVals:  tensor([[-724.7617, -731.9020, -731.1349, -727.6185]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  160\n",
      "QVals:  tensor([[-734.2373, -731.8542, -734.1234, -733.9343]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  161\n",
      "QVals:  tensor([[-748.6985, -766.9565, -764.1812, -753.1954]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  162\n",
      "QVals:  tensor([[-742.2485, -743.9381, -744.2736, -743.6605]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  163\n",
      "QVals:  tensor([[-751.9099, -748.5340, -750.5210, -750.1602]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  164\n",
      "QVals:  tensor([[-726.8964, -741.8843, -737.2039, -729.2730]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  165\n",
      "QVals:  tensor([[-731.6819, -735.0518, -732.7795, -731.1233]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  166\n",
      "QVals:  tensor([[-733.0665, -763.7678, -753.5800, -745.0472]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  167\n",
      "QVals:  tensor([[-747.6780, -746.4627, -743.8950, -747.4293]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  168\n",
      "QVals:  tensor([[-733.8585, -742.5414, -740.4648, -736.3192]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  169\n",
      "QVals:  tensor([[-761.1942, -760.5718, -762.2421, -761.4799]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  170\n",
      "QVals:  tensor([[-751.0555, -769.7711, -765.9724, -755.5974]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  171\n",
      "QVals:  tensor([[-756.5016, -760.2859, -759.5344, -758.5159]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  172\n",
      "QVals:  tensor([[-743.1736, -740.8531, -741.7037, -741.8583]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  173\n",
      "QVals:  tensor([[-733.8189, -747.4122, -739.6877, -732.3373]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  174\n",
      "QVals:  tensor([[-711.5825, -737.1271, -725.1373, -719.7679]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  175\n",
      "QVals:  tensor([[-732.8696, -737.3139, -732.9282, -735.0809]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  176\n",
      "QVals:  tensor([[-741.6302, -738.6343, -736.5555, -739.9843]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  177\n",
      "QVals:  tensor([[-733.4738, -743.0361, -741.3483, -737.3472]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  178\n",
      "QVals:  tensor([[-756.2015, -752.0367, -755.1524, -754.5092]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  179\n",
      "QVals:  tensor([[-760.5266, -777.4011, -775.7331, -764.0620]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  180\n",
      "QVals:  tensor([[-755.6552, -753.7313, -755.6717, -754.5056]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  181\n",
      "QVals:  tensor([[-758.3133, -773.5489, -767.7884, -759.5428]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  182\n",
      "QVals:  tensor([[-748.8743, -754.3630, -750.8237, -750.1991]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  183\n",
      "QVals:  tensor([[-751.6792, -755.3320, -751.8829, -748.8348]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  184\n",
      "QVals:  tensor([[-748.0074, -777.5233, -766.7172, -759.4847]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  185\n",
      "QVals:  tensor([[-772.5160, -770.1228, -769.3686, -768.5782]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  186\n",
      "QVals:  tensor([[-765.4363, -798.7067, -788.8994, -780.4244]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  187\n",
      "QVals:  tensor([[-801.9534, -790.3661, -787.8586, -795.0005]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  188\n",
      "QVals:  tensor([[-781.5115, -784.2302, -784.9332, -783.8298]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  189\n",
      "QVals:  tensor([[-791.4257, -785.7507, -786.3485, -789.5170]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  190\n",
      "QVals:  tensor([[-765.5570, -783.5344, -779.6021, -770.1958]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  191\n",
      "QVals:  tensor([[-770.8421, -773.3148, -771.3860, -772.1348]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  192\n",
      "QVals:  tensor([[-754.8673, -752.2141, -751.7529, -752.2191]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  193\n",
      "QVals:  tensor([[-748.8713, -760.6281, -758.4295, -752.5225]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  194\n",
      "QVals:  tensor([[-744.4546, -742.5751, -745.0992, -742.3663]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  195\n",
      "QVals:  tensor([[-767.6929, -799.4301, -791.9149, -780.9839]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  196\n",
      "QVals:  tensor([[-788.0016, -776.5995, -780.8191, -781.2346]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  197\n",
      "QVals:  tensor([[-779.4459, -796.6944, -793.9042, -783.2480]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  198\n",
      "QVals:  tensor([[-774.0120, -774.1567, -774.2256, -774.0962]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  199\n",
      "QVals:  tensor([[-777.0140, -772.1817, -773.5294, -773.2534]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  200\n",
      "QVals:  tensor([[-749.5588, -758.4507, -752.6312, -746.0104]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  201\n",
      "QVals:  tensor([[-741.1981, -767.9218, -755.1344, -751.0508]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  202\n",
      "QVals:  tensor([[-744.5391, -745.5776, -742.0534, -745.2443]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  203\n",
      "QVals:  tensor([[-755.6432, -764.4014, -761.7896, -758.7111]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  204\n",
      "QVals:  tensor([[-742.4039, -744.3558, -743.4096, -743.7859]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  205\n",
      "QVals:  tensor([[-775.9161, -767.5013, -769.9203, -770.7822]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  206\n",
      "QVals:  tensor([[-755.1580, -767.5909, -766.6285, -757.3141]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  207\n",
      "QVals:  tensor([[-762.0433, -760.0397, -760.3710, -758.8973]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  208\n",
      "QVals:  tensor([[-746.1442, -778.6182, -768.7875, -759.8275]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  209\n",
      "QVals:  tensor([[-753.8123, -751.9184, -749.6877, -752.0785]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  210\n",
      "QVals:  tensor([[-739.3994, -748.2737, -746.9175, -743.1262]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  211\n",
      "QVals:  tensor([[-767.1201, -761.0803, -762.9995, -763.6912]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  212\n",
      "QVals:  tensor([[-755.9334, -771.9628, -770.0059, -759.8124]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  213\n",
      "QVals:  tensor([[-763.7658, -760.9532, -761.5465, -761.7063]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  214\n",
      "QVals:  tensor([[-746.4227, -761.9283, -757.0506, -748.8153]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  215\n",
      "QVals:  tensor([[-748.5850, -752.2151, -749.2065, -746.3840]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  216\n",
      "QVals:  tensor([[-740.8492, -768.4951, -756.3287, -750.7975]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  217\n",
      "QVals:  tensor([[-761.3518, -763.9018, -760.9109, -761.7538]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  218\n",
      "QVals:  tensor([[-757.9116, -769.6940, -767.2504, -761.8290]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  219\n",
      "QVals:  tensor([[-787.0627, -787.6151, -788.3739, -787.1736]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  220\n",
      "QVals:  tensor([[-797.0214, -791.2315, -793.9343, -794.0833]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  221\n",
      "QVals:  tensor([[-778.5970, -795.7842, -792.8940, -783.0724]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  222\n",
      "QVals:  tensor([[-770.3181, -770.5981, -770.4451, -769.2535]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  223\n",
      "QVals:  tensor([[-758.7032, -792.8985, -782.3003, -772.9067]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  224\n",
      "QVals:  tensor([[-752.2943, -752.1860, -750.8646, -751.9286]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  225\n",
      "QVals:  tensor([[-752.2651, -764.7834, -761.8094, -756.4097]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  226\n",
      "QVals:  tensor([[-769.9691, -769.8326, -771.6633, -769.8683]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  227\n",
      "QVals:  tensor([[-782.8243, -803.0782, -798.7100, -787.6021]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  228\n",
      "QVals:  tensor([[-771.5271, -778.4686, -777.4088, -774.1261]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  229\n",
      "QVals:  tensor([[-781.9677, -783.3800, -783.9337, -781.3522]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  230\n",
      "QVals:  tensor([[-751.9952, -785.8262, -775.5569, -766.3290]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  231\n",
      "QVals:  tensor([[-756.5936, -756.1873, -755.8207, -755.9789]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  232\n",
      "QVals:  tensor([[-760.4966, -765.7336, -767.1918, -763.2833]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  233\n",
      "QVals:  tensor([[-779.9014, -773.7360, -778.2814, -777.0847]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  234\n",
      "QVals:  tensor([[-750.5884, -769.0089, -766.5146, -754.4761]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  235\n",
      "QVals:  tensor([[-759.6797, -764.0178, -763.6096, -760.9670]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  236\n",
      "QVals:  tensor([[-747.5526, -746.6578, -747.3587, -745.9601]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  237\n",
      "QVals:  tensor([[-743.9820, -777.8383, -767.4860, -758.1269]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  238\n",
      "QVals:  tensor([[-739.2804, -739.7065, -738.8732, -738.7675]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  239\n",
      "QVals:  tensor([[-744.1425, -776.0212, -765.0309, -758.8825]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  240\n",
      "QVals:  tensor([[-760.7987, -756.4007, -756.1705, -760.0323]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  241\n",
      "QVals:  tensor([[-758.8105, -761.3884, -762.7785, -760.7230]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  242\n",
      "QVals:  tensor([[-755.8063, -757.9058, -758.4835, -757.6158]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  243\n",
      "QVals:  tensor([[-764.0944, -760.3228, -762.0287, -762.4624]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  244\n",
      "QVals:  tensor([[-723.8485, -741.9027, -740.3475, -727.5662]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  245\n",
      "QVals:  tensor([[-739.2335, -741.9282, -740.3864, -739.6760]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  246\n",
      "QVals:  tensor([[-725.6587, -723.4756, -725.0189, -722.2135]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  247\n",
      "QVals:  tensor([[-708.3687, -739.6779, -727.2331, -723.5511]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  248\n",
      "QVals:  tensor([[-706.5463, -705.1396, -702.6070, -706.2917]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  249\n",
      "QVals:  tensor([[-729.5106, -731.2589, -731.4197, -732.2670]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  250\n",
      "QVals:  tensor([[-736.1896, -727.4855, -731.6425, -732.5723]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  251\n",
      "QVals:  tensor([[-719.6786, -736.0341, -734.4990, -723.5206]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  252\n",
      "QVals:  tensor([[-719.7877, -721.6948, -719.8467, -720.3395]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  253\n",
      "QVals:  tensor([[-718.2574, -714.9245, -715.0369, -715.0430]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  254\n",
      "QVals:  tensor([[-689.3950, -705.4614, -699.2925, -693.0137]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  255\n",
      "QVals:  tensor([[-698.5596, -703.6328, -697.5036, -696.1006]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  256\n",
      "QVals:  tensor([[-703.6901, -728.0255, -715.3895, -718.5314]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  257\n",
      "QVals:  tensor([[-721.1552, -719.5412, -717.0912, -720.2759]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  258\n",
      "QVals:  tensor([[-710.1808, -711.2088, -709.8610, -712.9526]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  259\n",
      "QVals:  tensor([[-720.9043, -721.4376, -724.1972, -721.7999]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  260\n",
      "QVals:  tensor([[-725.0392, -722.8690, -726.9928, -725.3091]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  261\n",
      "QVals:  tensor([[-713.3447, -725.9124, -725.2578, -715.6096]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  262\n",
      "QVals:  tensor([[-706.0535, -713.7932, -711.7467, -709.8387]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  263\n",
      "QVals:  tensor([[-711.0144, -711.3443, -712.8981, -710.5460]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  264\n",
      "QVals:  tensor([[-688.5032, -706.9272, -699.1437, -699.3107]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  265\n",
      "QVals:  tensor([[-691.0392, -688.9576, -689.1296, -691.1721]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  266\n",
      "QVals:  tensor([[-691.2208, -697.3270, -695.9274, -694.6851]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  267\n",
      "QVals:  tensor([[-707.6006, -707.4432, -706.8445, -708.3928]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  268\n",
      "QVals:  tensor([[-695.8997, -693.3038, -695.5976, -696.9620]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  269\n",
      "QVals:  tensor([[-683.9873, -694.2838, -691.4349, -686.7775]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  270\n",
      "QVals:  tensor([[-672.6072, -681.2972, -677.0186, -677.2442]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  271\n",
      "QVals:  tensor([[-691.1099, -693.9498, -694.2167, -691.9121]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  272\n",
      "QVals:  tensor([[-688.3746, -683.5583, -688.2458, -686.1611]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  273\n",
      "QVals:  tensor([[-662.1891, -673.7976, -672.4827, -661.4901]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  274\n",
      "QVals:  tensor([[-651.1038, -673.1699, -666.2033, -664.0458]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  275\n",
      "QVals:  tensor([[-674.5816, -659.8873, -666.0418, -664.5515]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  276\n",
      "QVals:  tensor([[-633.7280, -645.3397, -647.8760, -642.4037]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  277\n",
      "QVals:  tensor([[-643.3947, -638.9445, -642.8545, -641.0963]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  278\n",
      "QVals:  tensor([[-625.4794, -643.0251, -647.9052, -640.0603]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  279\n",
      "QVals:  tensor([[-659.9739, -657.5125, -665.0530, -658.9587]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  280\n",
      "QVals:  tensor([[-627.1040, -652.3546, -656.1297, -650.9003]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  281\n",
      "QVals:  tensor([[-660.2493, -657.5018, -669.3471, -655.4691]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  282\n",
      "QVals:  tensor([[-667.2075, -667.1879, -670.0240, -673.1169]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  283\n",
      "QVals:  tensor([[-667.3823, -674.9623, -684.2593, -675.6277]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  284\n",
      "QVals:  tensor([[-687.5876, -668.8807, -686.3527, -671.5295]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  285\n",
      "QVals:  tensor([[-681.9285, -681.3021, -695.3954, -680.9052]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  286\n",
      "QVals:  tensor([[-665.4856, -665.7682, -665.2080, -670.6704]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  287\n",
      "QVals:  tensor([[-657.0400, -671.0644, -679.8873, -673.8535]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  288\n",
      "QVals:  tensor([[-685.6521, -668.1584, -684.4169, -675.3759]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  289\n",
      "QVals:  tensor([[-674.5225, -678.4960, -693.2297, -679.2701]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  290\n",
      "QVals:  tensor([[-701.3876, -671.5869, -694.2488, -677.8375]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  291\n",
      "QVals:  tensor([[-689.8590, -679.0150, -699.7701, -678.4594]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  292\n",
      "QVals:  tensor([[-656.8635, -651.3364, -655.5433, -657.2560]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  293\n",
      "QVals:  tensor([[-638.5868, -648.0176, -659.2598, -646.4124]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  294\n",
      "QVals:  tensor([[-672.0929, -644.8574, -666.6891, -647.3011]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  295\n",
      "QVals:  tensor([[-662.1071, -652.3974, -670.8991, -649.3065]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  296\n",
      "QVals:  tensor([[-641.7669, -640.5189, -638.8097, -640.0775]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  297\n",
      "QVals:  tensor([[-641.8076, -643.1103, -653.7268, -643.1357]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  298\n",
      "QVals:  tensor([[-684.8795, -638.5525, -666.1091, -647.5651]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  299\n",
      "QVals:  tensor([[-668.6877, -642.8555, -667.5725, -645.3108]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  300\n",
      "QVals:  tensor([[-640.9613, -628.7549, -649.0646, -628.5476]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  301\n",
      "QVals:  tensor([[-612.3601, -617.1000, -613.5140, -616.6956]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  302\n",
      "QVals:  tensor([[-639.2211, -604.4213, -624.3908, -610.2924]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  303\n",
      "QVals:  tensor([[-601.9148, -596.4249, -610.2020, -595.0920]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  304\n",
      "QVals:  tensor([[-597.3143, -600.1940, -582.4431, -602.0346]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  305\n",
      "QVals:  tensor([[-604.1018, -606.0971, -611.2618, -608.7777]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  306\n",
      "QVals:  tensor([[-642.9099, -604.9176, -624.3975, -616.5606]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  307\n",
      "QVals:  tensor([[-609.2519, -596.4794, -612.2870, -601.5590]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  308\n",
      "QVals:  tensor([[-594.8065, -588.5086, -603.6451, -592.7599]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  309\n",
      "QVals:  tensor([[-584.7247, -587.4671, -599.6243, -587.7191]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  310\n",
      "QVals:  tensor([[-581.5641, -561.9839, -577.8293, -567.4258]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  311\n",
      "QVals:  tensor([[-553.1487, -564.5662, -572.3300, -559.6594]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  312\n",
      "QVals:  tensor([[-582.2618, -567.3140, -582.4575, -568.9796]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  313\n",
      "QVals:  tensor([[-592.2450, -586.9523, -602.6619, -583.3120]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  314\n",
      "QVals:  tensor([[-582.8606, -574.3696, -568.7160, -575.0617]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  315\n",
      "QVals:  tensor([[-601.0765, -593.9567, -605.9592, -596.0869]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  316\n",
      "QVals:  tensor([[-605.1526, -597.8797, -615.4290, -602.9910]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  317\n",
      "QVals:  tensor([[-587.2681, -594.3687, -606.9239, -592.6128]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  318\n",
      "QVals:  tensor([[-623.7200, -595.1015, -615.4423, -605.0480]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  319\n",
      "QVals:  tensor([[-601.6605, -596.4386, -613.2136, -595.1006]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  320\n",
      "QVals:  tensor([[-561.8624, -568.6900, -555.4572, -572.6971]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  321\n",
      "QVals:  tensor([[-567.6088, -570.8450, -579.7952, -573.9586]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  322\n",
      "QVals:  tensor([[-613.7647, -577.4948, -600.1766, -590.2905]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  323\n",
      "QVals:  tensor([[-625.8175, -603.8450, -626.7881, -611.3059]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  324\n",
      "QVals:  tensor([[-605.6279, -594.0554, -614.5377, -600.2952]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  325\n",
      "QVals:  tensor([[-585.2552, -585.6585, -601.9184, -585.1546]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  326\n",
      "QVals:  tensor([[-557.0385, -567.6216, -553.7405, -575.9934]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  327\n",
      "QVals:  tensor([[-540.0361, -553.0478, -561.6831, -561.6252]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  328\n",
      "QVals:  tensor([[-563.6897, -532.4133, -553.4361, -551.9830]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  329\n",
      "QVals:  tensor([[-564.7937, -552.6259, -572.4216, -560.7994]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  330\n",
      "QVals:  tensor([[-551.6845, -543.0956, -562.2466, -547.8529]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  331\n",
      "QVals:  tensor([[-545.8053, -548.4379, -564.7593, -547.4547]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  332\n",
      "QVals:  tensor([[-553.5854, -524.0434, -546.8694, -536.8263]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  333\n",
      "QVals:  tensor([[-526.1661, -528.5988, -544.5154, -523.2377]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  334\n",
      "QVals:  tensor([[-535.2920, -542.9214, -530.9625, -549.6475]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  335\n",
      "QVals:  tensor([[-539.0329, -538.5533, -550.4062, -545.4135]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  336\n",
      "QVals:  tensor([[-526.7236, -516.6182, -537.9280, -524.9417]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  337\n",
      "QVals:  tensor([[-542.4684, -539.9737, -561.7906, -544.1019]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  338\n",
      "QVals:  tensor([[-539.2256, -542.5021, -568.3858, -548.2387]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  339\n",
      "QVals:  tensor([[-550.6748, -524.5421, -549.5069, -535.8746]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  340\n",
      "QVals:  tensor([[-505.4056, -504.1235, -524.7773, -500.5588]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  341\n",
      "QVals:  tensor([[-530.2734, -537.6169, -528.5139, -544.3857]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  342\n",
      "QVals:  tensor([[-521.7713, -516.5656, -539.9456, -525.9484]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  343\n",
      "QVals:  tensor([[-522.1286, -511.3542, -541.2417, -516.8641]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  344\n",
      "QVals:  tensor([[-526.0707, -517.3399, -550.7448, -524.1125]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  345\n",
      "QVals:  tensor([[-534.5905, -536.6702, -570.8196, -539.2778]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  346\n",
      "QVals:  tensor([[-532.2263, -501.4895, -534.1066, -514.0363]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  347\n",
      "QVals:  tensor([[-507.0544, -508.2127, -530.2094, -502.4914]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  348\n",
      "QVals:  tensor([[-503.4106, -511.8544, -511.8182, -520.8500]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  349\n",
      "QVals:  tensor([[-574.7601, -532.3692, -547.7675, -551.1839]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  350\n",
      "QVals:  tensor([[-546.0715, -523.7271, -536.9121, -533.0660]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  351\n",
      "QVals:  tensor([[-543.4114, -534.9001, -547.4706, -536.4625]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  352\n",
      "QVals:  tensor([[-547.6177, -541.9978, -555.5908, -542.1844]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  353\n",
      "QVals:  tensor([[-515.2423, -530.5211, -535.3651, -525.0872]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  354\n",
      "QVals:  tensor([[-524.8181, -514.4705, -523.5294, -519.8109]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  355\n",
      "QVals:  tensor([[-510.9050, -535.7015, -538.9400, -527.6530]], device='cuda:0')\n",
      "best Action:  0\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  356\n",
      "QVals:  tensor([[-555.4260, -538.3715, -555.6693, -544.7784]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  357\n",
      "QVals:  tensor([[-557.0042, -559.8716, -571.8877, -556.6281]], device='cuda:0')\n",
      "best Action:  3\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  358\n",
      "QVals:  tensor([[-573.2665, -561.3095, -548.8644, -566.8898]], device='cuda:0')\n",
      "best Action:  2\n",
      "Done: False\n",
      "------------------------------\n",
      "Step:  359\n",
      "QVals:  tensor([[-580.8832, -572.4000, -581.7473, -576.3127]], device='cuda:0')\n",
      "best Action:  1\n",
      "Done: True\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env._getState(0)\n",
    "# prevAct = 0\n",
    "rewards = []\n",
    "step = 0\n",
    "while not done:\n",
    "    print(\"Step: \", step)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=PARAM_device).unsqueeze(0)\n",
    "    # get the prediction from the model\n",
    "    with torch.no_grad():\n",
    "        QVals = policyNet(state_tensor)\n",
    "        print(\"QVals: \", QVals)\n",
    "        # get the best action from QVals\n",
    "        bestAction = torch.argmax(QVals).item()\n",
    "        print(\"best Action: \", bestAction)\n",
    "    # perform the action\n",
    "    nextState, r, done = env.take_action(bestAction)\n",
    "    print(\"Done:\", done)\n",
    "    # print('State: ', state.cpu().numpy())\n",
    "    print('------------------------------')\n",
    "\n",
    "    # update the current state\n",
    "    state = nextState\n",
    "    rewards.append(r)\n",
    "    step+=1\n",
    "    # if step==5:\n",
    "    #     break\n",
    "    # break\n",
    "    # prevAct = bestAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dea15bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.numSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7747611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(0)\n",
    "done = False\n",
    "while not done:\n",
    "    ns, r, done = env.take_action(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735b626",
   "metadata": {},
   "source": [
    "#### Things to note: V5.\n",
    "Here are a bunch of things that I understood from the V5. See OneNote for that.\n",
    "___\n",
    "## Trying to analyze the model's performance, basically things such as average travel time, visually looking at the performance of the model in the frontend etc.\n",
    "\n",
    "- Looking at average travel time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62f2702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "944.0625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.engine.get_average_travel_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14f56b",
   "metadata": {},
   "source": [
    "Creating an env without rl with 10 seconds durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ce6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonRLEngine = cityflow.Engine('generated/configManual.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22dd536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running for 3600 seconds\n",
    "for i in range(3600):\n",
    "    nonRLEngine.next_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce077a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999.2485313315927"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonRLEngine.get_average_travel_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d9da74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-579.2397, -551.5745, -571.1451, -565.6125], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policyNet(torch.tensor(env._getState(0), dtype=torch.float32, device=PARAM_device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
